{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Role       | Description                                                                                                                                                   |\n",
    "|------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| system     | Used to tell the chat model how to behave and provide additional context. Not supported by all chat model providers.                                          |\n",
    "| user       | Represents input from a user interacting with the model, usually in the form of text or other interactive input.                                              |\n",
    "| assistant  | Represents a response from the model, which can include text or a request to invoke tools.                                                                    |\n",
    "| tool       | A message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved. Used with chat models that support tool calling. |\n",
    "| function (legacy) | This is a legacy role, corresponding to OpenAI's legacy function-calling API. tool role should be used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain Message Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    Message Type        |    Corresponding Role       |    Description                                                                                                          |\n",
    "|------------------------|-----------------------------|--------------------------------------------------------------------------------------------------------------------------|\n",
    "|    SystemMessage       |    system                   |    Corresponds to the system role.                                                                                       |\n",
    "|    HumanMessage        |    user                     |    Corresponds to the user role.                                                                                         |\n",
    "|    AIMessage           |    assistant                |    Corresponds to the assistant role.                                                                                    |\n",
    "|    AIMessageChunk      |    assistant                |    Corresponds to the assistant role, used for streaming responses.                                                      |\n",
    "|    ToolMessage         |    tool                     |    Corresponds to the tool role.                                                                                         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('./../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://localhost:11434\"\n",
    "\n",
    "model = 'llama3.2:1b'\n",
    "llm = ChatOllama(base_url=base_url, model = model)\n",
    "\n",
    "response = llm.invoke('tell me about the earth in 3 points')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'sherlock'\n",
    "llm = ChatOllama(base_url=base_url, model = model)\n",
    "\n",
    "response = llm.invoke('tell me about the earth in 3 points')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'sheldon'\n",
    "llm = ChatOllama(base_url=base_url, model = model)\n",
    "\n",
    "response = llm.invoke('tell me about the earth in 3 points')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Role Assumption for LLM\n",
    "model = 'llama3.2:1b'\n",
    "llm = ChatOllama(base_url=base_url, model = model)\n",
    "\n",
    "system = SystemMessage(\"You are elementary school teacher. You answer in short and brief sentences.\")\n",
    "question = HumanMessage(\"tell me about the earth in 3 points\")\n",
    "\n",
    "messages = [system, question]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain Prompt Templates\n",
    "\n",
    "|    Prompt Template Class             |    Description                                                                                                     |\n",
    "|--------------------------------------|---------------------------------------------------------------------------------------------------------------------|\n",
    "|    SystemMessagePromptTemplate       |    Template for generating system messages that provide model context or instructions.                            |\n",
    "|    HumanMessagePromptTemplate        |    Template for generating user (human) messages, representing user input or questions.                          |\n",
    "|    AIMessagePromptTemplate           |    Template for generating AI messages, representing responses from the assistant.                                |\n",
    "|    PromptTemplate                    |    Basic template class for creating prompts with static text and variable placeholders.                          |\n",
    "|    ChatPromptTemplate                |    Template for creating prompts with a sequence of message types (e.g., system, user, assistant) in a chat format. |\n",
    "|    ChatPromptTemplateList            |    Template for managing a list of multiple chat prompt templates in a structured chat workflow.                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "                                    SystemMessagePromptTemplate, \n",
    "                                    HumanMessagePromptTemplate, \n",
    "                                    PromptTemplate, \n",
    "                                    ChatPromptTemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'llama3.2:1b'\n",
    "llm = ChatOllama(base_url=base_url, model = model)\n",
    "\n",
    "template = PromptTemplate.from_template(\"tell me about the earth in 3 points\")\n",
    "template\n",
    "\n",
    "template = PromptTemplate.from_template(\"tell me about the {topic} in {points} points\")\n",
    "template\n",
    "template.invoke({'topic': 'sun', 'points': 3})\n",
    "\n",
    "question = template.invoke({'topic': 'sun', 'points': 3})\n",
    "\n",
    "# Must be a PromptValue, str, or list of BaseMessages.\n",
    "# messages = [system, question] this can not be done\n",
    "\n",
    "response = llm.invoke(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatprompt template\n",
    "\n",
    "template = ChatPromptTemplate.from_template(\"tell me about the {topic} in {points} points\")\n",
    "template\n",
    "question = template.invoke({'topic': 'sun', 'points': 3})\n",
    "\n",
    "response = llm.invoke(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine system and human messages\n",
    "template = ChatPromptTemplate(\n",
    "    [\n",
    "        system,\n",
    "        HumanMessage(\"Tell me about the {topic} in {points} points\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "template\n",
    "\n",
    "template = ChatPromptTemplate(\n",
    "    [\n",
    "        system,\n",
    "        HumanMessagePromptTemplate.from_template(\"Tell me about the {topic} in {points} points\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "template\n",
    "question = template.invoke({'topic': 'sun', 'points': 3})\n",
    "\n",
    "response = llm.invoke(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Everything Together\n",
    "\n",
    "- SystemMessage, HumanMessage : Can't be invoked\n",
    "- SystemMessagePromptTemplate, HumanMessagePromptTemplate : Can be invoked if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'llama3.2:1b'\n",
    "llm = ChatOllama(base_url=base_url, model = model)\n",
    "\n",
    "system = SystemMessagePromptTemplate.from_template(\"You are {school} teacher. You answer in short and brief sentences\")\n",
    "question = HumanMessagePromptTemplate.from_template(\"tell me about the {topic} in {points} points\")\n",
    "\n",
    "template = ChatPromptTemplate([system, question])\n",
    "\n",
    "template\n",
    "\n",
    "template.invoke({'school': 'elementary', 'topic': 'sun', 'points': 3})\n",
    "\n",
    "question = template.invoke({'school': 'Ph.D', 'topic': 'sun', 'points': 3})\n",
    "\n",
    "response = llm.invoke(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
