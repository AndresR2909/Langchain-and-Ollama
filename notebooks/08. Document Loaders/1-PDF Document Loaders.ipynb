{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Document Loaders\n",
    "- Load various kind of documents from the web and local files.\n",
    "- Apply LLM to the documents for summarization and question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 1: Question Answering from PDF Document\n",
    "- We will load the document from the local file and apply LLM to answer the questions.\n",
    "- Lets use research paper published on the missuse of the health supplements for workout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /Users/andrestrepo/Documents/repos_personal/Langchain-and-Ollama/.venv/lib/python3.12/site-packages (1.24.12)\n",
      "Requirement already satisfied: tiktoken in /Users/andrestrepo/Documents/repos_personal/Langchain-and-Ollama/.venv/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/andrestrepo/Documents/repos_personal/Langchain-and-Ollama/.venv/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/andrestrepo/Documents/repos_personal/Langchain-and-Ollama/.venv/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/andrestrepo/Documents/repos_personal/Langchain-and-Ollama/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andrestrepo/Documents/repos_personal/Langchain-and-Ollama/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andrestrepo/Documents/repos_personal/Langchain-and-Ollama/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrestrepo/Documents/repos_personal/Langchain-and-Ollama/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"rag-dataset/Summarize Large Documents or Text Using LLMs and LangChain _ by Ranjeet Tiwari _ Senior Architect - AI _ IITJ _ Medium.pdf\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Member-only story\n",
      "Summarize Large Documents or\n",
      "Text Using LLMs and LangChain\n",
      "Ranjeet Tiwari | Senior Architect - AI | IITJ · Follow\n",
      "4 min read · Jul 17, 2024\n",
      "100\n",
      "Summarizing long texts can be quite a challenge, but with LangChain and\n",
      "Language Learning Model (LLM), it’s made simple. Imagine you’re reading a\n",
      "lengthy book or a detailed report, and you need to condense it into a short,\n",
      "easy-to-read summary.\n",
      "LangChain(with LLM) provides several strategies to help you do just that.\n",
      "Let’s dive into these strategies using real-world examples to make things\n",
      "clearer.\n",
      "LangChain\n",
      "The “Stuff” Strategy\n",
      "The simplest method is called the “stuff” strategy. If the entire text fits within the\n",
      "LLM’s context window, you can directly input the raw text and get a summary.\n",
      "For example, suppose you have a short article about climate change:\n",
      "Input Text:\n",
      "“Climate change refers to long-term shifts and alterations in temperature\n",
      "and weather patterns, primarily due to human activities like burning fossil\n",
      "fuels, deforestation, and industrial processes. These activities increase levels\n",
      "of greenhouse gases in the atmosphere, leading to global warming and its\n",
      "This member-only story is on us. Upgrade to access all of Medium.\n",
      "Open in app\n",
      "Search\n",
      "Write\n",
      "28/1/25, 1:43 p.m.\n",
      "Summarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium\n",
      "https://medium.com/@AI-Simplified/summarize-large-documents-or-text-using-llms-and-langchain-3116543df4ba\n",
      "1/6\n"
     ]
    }
   ],
   "source": [
    "docs[0].metadata\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the list of PDFs in the dir\n",
    "import os\n",
    "\n",
    "pdfs = []\n",
    "for root, dirs, files in os.walk(\"rag-dataset\"):\n",
    "    # print(root, dirs, files)\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            pdfs.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 0}, page_content='1\\nAn Empirical Survey on Long Document Summarization:\\nDatasets, Models and Metrics\\nHUAN YEE KOH, Monash University, Australia\\nJIAXIN JU, Monash University, Australia\\nMING LIU∗, Deakin University, Australia\\nSHIRUI PAN∗†, Monash University, Australia\\nLong documents such as academic articles and business reports have been the standard format to detail out\\nimportant issues and complicated subjects that require extra attention. An automatic summarization system\\nthat can effectively condense long documents into short and concise texts to encapsulate the most important\\ninformation would thus be significant in aiding the reader’s comprehension. Recently, with the advent of neural\\narchitectures, significant research efforts have been made to advance automatic text summarization systems,\\nand numerous studies on the challenges of extending these systems to the long document domain have emerged.\\nIn this survey, we provide a comprehensive overview of the research on long document summarization and\\na systematic evaluation across the three principal components of its research setting: benchmark datasets,\\nsummarization models, and evaluation metrics. For each component, we organize the literature within the\\ncontext of long document summarization and conduct an empirical analysis to broaden the perspective on\\ncurrent research progress. The empirical analysis includes a study on the intrinsic characteristics of benchmark\\ndatasets, a multi-dimensional analysis of summarization models, and a review of the summarization evaluation\\nmetrics. Based on the overall findings, we conclude by proposing possible directions for future exploration in\\nthis rapidly growing field.\\nCCS Concepts: • Information systems →Summarization; • Computing methodologies →Information\\nextraction.\\nAdditional Key Words and Phrases: document summarization, datasets, neural networks, language models,\\nTransformer\\n1\\nINTRODUCTION\\nSummarization of textual information is an exacting task for humans and the rate of information\\ngrowth in the era of big data has made summarizing most information manually to be impractical and\\nimpossible. This phenomenon is exacerbated when it comes to long form textual documents as the\\nknowledge and human labour effort required to process and summarize it increases exponentially\\nwith the length of documents. Inevitably, a significant amount of invaluable information and\\nknowledge have gone unnoticed, presenting an important bottleneck in the progress of social and\\n∗Corresponding Authors: Ming Liu and Shirui Pan.\\n†This work is done while Shirui Pan is with Monash University. From August 2022, he is with the School of Information\\nand Communication Technology, Griffith University, Southport, QLD 4222, Australia.\\nAuthors’ addresses: Huan Yee Koh, huan.koh@monash.edu, Monash University, Australia; Jiaxin Ju, jjuu0002@student.\\nmonash.edu, Monash University, Australia; Ming Liu, Deakin University, Australia, m.liu@deakin.edu.au; Shirui Pan,\\nMonash University, Australia, shiruipan@ieee.org.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\\n© 2022 Association for Computing Machinery.\\n0360-0300/2022/1-ART1 $15.00\\nhttps://doi.org/10.1145/3545176\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\\narXiv:2207.00939v1  [cs.CL]  3 Jul 2022'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 1}, page_content='1:2\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\neconomic development. In response, there has been a strong demand for exhaustive research in the\\nfield of automatic long document summarization [2, 20, 25, 77, 104].\\nAutomatic text summarization involves a process of shortening a source text efficiently while\\nkeeping the main idea intact, which aids in reducing the amount of time required to process\\ninformation, helps with faster search for information, and makes learning one topic easier [71, 73].\\nWhile the potentiality of developing an effective automatic text summarization system has attracted\\nsignificant interest and attention from the research community, automatic text summarization\\nremains a challenging task and is not ready for wide practical use in day-to-day lives, particularly\\nwhen it comes to summarizing long documents [8, 61, 62, 81]. Intuitively, long document sum-\\nmarization is harder than short document summarization due to the significant difference in the\\namount of lexical tokens and breadth of content between short and long documents. As the length\\nincreases, the content that would be considered important will also increase, resulting in a more\\nchallenging task for an automatic summarization model to capture all salient information in the\\nlimited output length [37]. Further, short documents are often generic text such as news articles\\n[43, 84, 87, 102], while long documents are commonly domain-specific articles such as scientific\\npapers that contain more complex formulas and terminologies [20, 49, 59]. Together with other\\nreasons that will be explored in this survey, long document summarization poses a significantly\\nmore challenging task than short document summarization.\\nIn general, automatic text summarization can be conceptualized as having three approaches:\\nextractive, abstractive, and hybrid approach [62]. The extractive approach directly copies salient\\nsentences from the source document and combine them as the output [15, 38], whereas the abstrac-\\ntive approach imitates human that comprehends a source document and writes a summary output\\nbased on the salient concepts of the source document [101, 103]. The hybrid approach attempts to\\ncombine the best of both approaches by rewriting a summary based on a subset of salient content\\nextracted from the source document [36, 47, 73]. Each approach has its advantages and limitations\\nthat may suit certain summarization tasks better. For example, extractive summarization may\\nbe sufficient in summarizing certain news articles [15, 128] but inadequate to summarize a long\\ndialogue where salient content are sparsely distributed [131]. This is because while the extractive\\nsummarization approach is always factually consistent with the source document, it does not\\nmodify the original text and thus lacks the ability to generate fluent and concise summary [120].\\nHistorically, to measure the performance of different summarization architectures, ROUGE score\\n[70] has been the modus operandi for researchers in the summarization research field to compare\\nand study the quality of different candidate summaries. The core idea of ROUGE score is to measure\\nthe lexical overlaps such as words and phrases between candidate summary and ground truth\\nsummary. While it is efficient, recent findings have shown that ROUGE score does not correlate\\nwell with how humans assess the quality of a candidate summary [3, 11, 44, 61]. As a result, there is\\na significant amount of effort in improving the way we measure the quality of candidate summaries\\nand performance of summarization architectures [62, 80, 81, 126, 130]. Unfortunately, these efforts\\nhave entirely been focusing on the short document domains and the progress in measuring the\\nquality of long document summarization approach has been lacking [3, 41, 48, 89, 93].\\nNevertheless, there has been a considerable amount of advancement made in the long document\\nsummarization research field and the area lacks a comprehensive survey [5, 28, 32, 105]. Our\\npaper fills this gap by providing a comprehensive overview of the research on long document\\nsummarization and a systematic evaluation across the three principal components of its research\\nsetting: benchmark datasets, summarization models, and evaluation metrics.\\nThe contribution of our paper is as follows:\\nComprehensive Review. A comprehensive survey of the long document summarization research\\nliterature.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 2}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:3\\nFull-view of summarization research. Text summarization literature mainly explores the three key\\naspects of research setting: developing advanced models, releasing new datasets, and proposing\\nalternative evaluation metrics. We empirically provide a detailed review of all three key components\\nwithin the context of long document summarization.\\nEmpirical Studies and Thorough Analysis. To ensure wide coverage of emerging trends, we\\nempirically analyze each component of the long document summarization research setting through\\nfine-grained human analysis and ad-hoc experiments.\\nFuture Direction. We discuss the current progress of long document summarization, analyze the\\nlimitation of existing methods, and suggest promising future research directions in terms of model\\ndesigns, quality and diversity of datasets, the practicality of evaluation metrics and, finally, the\\nfeasibility of implementing summarization techniques to real-life applications.\\nThe survey is organized as follows: firstly, an overview of the fundamentals of long document\\nsummarization in section 2. Secondly, a detailed study of ten summarization benchmark datasets\\nis in section 3. A comprehensive survey on summarization models that are designed specifically\\nor have to ability to summarize long documents in section 4. Then, in section 5, we analyze the\\nperformances of models that are representative of the different types of architectures commonly\\nused by researchers through ad-hoc experiments. In section 6, we summarize the advancement in\\nevaluation metrics and their applicability in the long document summarization domain. Section\\n7 goes into the applications of long document summarization models and Section 8 discusses\\npromising future research direction in this field. Finally, section 9 concludes this survey.\\n2\\nFUNDAMENTALS OF LONG DOCUMENT SUMMARIZATION\\nTo make clear the distinction between short and long documents, we conceptualize the summariza-\\ntion task problem from three different fundamental aspects: 1) length of document, 2) breath of\\ncontent, and 3) degree of coherence.\\n2.1\\nLength of Document\\nDocuments are commonly classified as \"long\" because the number of lexical tokens in the source\\ndocument is enormous and it requires a considerable amount of time for an average human to\\nconsume the full text. While this definition makes intuitive sense, in the context of machine learning,\\na document is considered long when current state-of-the-art models for a normal document cannot\\nbe implemented similarly in an effective manner due to hardware and model limitations. For\\nexample, previous research [10] considers CNN/DM and NYT benchmark datasets in the news\\ndomain as long documents when in the present research context they are now considered to be\\nshort document datasets. Currently, a benchmark dataset with an average source document length\\nthat exceeds 3,000 lexical tokens could be well-considered as \"long documents\" [77, 127] due to\\nthe fact that most existing state-of-the-art summarization systems (e.g., pre-trained models) are\\nlimited to 512 to 1024 lexical tokens only [23, 129]. These limitations cannot be easily solved\\nwithout novel techniques that help in assisting current architectures to reason over a long range of\\ntextual inputs [77, 82, 127]. Accordingly, this survey adopts a similar definition where a document\\nis only considered as long if current state-of-the-art systems used in the short document cannot\\nbe extended and applied to a document with significantly longer text. Despite the potentially\\nconfusing definition, this enduring definition ensures that the model architectures implemented by\\nresearchers require novel techniques to overcome hardware limitations rather than just a mere\\nreplica of previous works.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 3}, page_content='1:4\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\n2.2\\nBreadth of Content\\nOn average, informative content that is non-redundant will increase together with the length of\\na document. However, despite the fact that reference summary length often increases together\\nwith the source document length, the length of a summary is usually constrained by what an\\naverage user considered as reasonable [20, 104]. Thus, while it may be sufficient for summaries of\\na reasonable length to cover the most or even all of the informative aspects for short documents,\\nthis is not necessarily true for summaries of long documents. In section 3, we empirically show\\nthat the relative length of summary against the source document becomes exponentially shorter as\\nthe source document length increases. Due to this elevated constraint, the ground truth summary\\nof a long document will inevitably lose information that is not key to the central narrative of the\\noriginal author or summary writer [37]. Furthermore, recent work [61] has also identified that\\nhuman users could not agree on what should be considered important for a given document in\\nthe short document news domain due to the heterogeneity of user preferences and expectations.\\nThis issue is exacerbated when it comes to long document summarization as (a) the relative length\\nof summary against the source document is shorter and (b) the chance of users having different\\npreferences and expectations would increase as the breadth of content increases, making the long\\ndocument summarization task significantly harder than short document.\\n2.3\\nDegree of Coherence\\nAs compared to short documents, long documents are often structured into sections for the ease\\nof user comprehension [20, 59]. The content within each section also differ to a certain extent\\ndespite revolving around a key narrative of the long documents. This makes the long document\\nsummarization task more burdensome as summarization models cannot concatenate salient texts\\nfrom different sections without considering its impact on the fluency, redundancy, and semantic\\ncoherence of the final summary outputs.\\nBased on the fundamental aspects, the rest of this paper provides an empirical survey on long\\ndocument summarization, covering the benchmark datasets, summarization models, and metrics.\\n3\\nDATASETS\\nPublicly available benchmark datasets have been introduced to evaluate the performance of summa-\\nrization models. Nonetheless, the benchmark datasets have different intrinsic characteristics that\\nhave been found to be crucial in the understanding of model performances [81, 111], summarization\\napproach suitability (i.e., extractive or abstractive approach) [104, 128] and evaluation metrics effec-\\ntiveness [31, 89]. Hence, only through a comprehensive understanding of the benchmark datasets,\\none can assess the underlying performance and applicability of a summarization model in the real-\\nworld settings [61]. Further, insights drawn from benchmark datasets have led to the introduction\\nof state-of-the-art models across a wide range of natural language processing (NLP) tasks [13, 123],\\nincluding the text summarization task [25, 37, 77]. In response, intrinsic dataset evaluation through\\nlarge-scale automatic evaluation [4] or more fine-grained human evaluation at a smaller scale [111]\\nhas also been performed to enhance the understanding of various benchmarks. Nevertheless, none\\nof the aforementioned works performed a large-scale automatic evaluation analysis nor a thorough\\nhuman evaluation of benchmark datasets in the long document text summarization domain. To\\naddress this gap, this section explores the basic statistics and intrinsic characteristics of popular\\nbenchmarks in short and long document domain through the usage of large-scale automatic evalu-\\nation metrics and performs fine-grained human analysis on the arXiv benchmark to encourage a\\nbetter appreciation of the most widely used long document summarization dataset [49, 77, 127].\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 4}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:5\\n3.1\\nCorpora\\nShort document. Short-document datasets studied in this survey are CNN-DM, NWS, XSUM,\\nReddit-TIFU, and WikiHow. The first three news datasets are chosen due to their popularity while\\nReddit-TIFU and WikiHow are studied to ensure short documents from other domains are also\\nincluded. The document-summary pairs from CNN-DM, NWS, and XSUM are typical of that\\nin the news domain, where the source document represents news article while the summary\\nrepresents either human-curated summary [43, 87] or summary created by concatenating bullet-\\npoint sentences in the original source document [84]. On the other hand, Reddit-TIFU is a dataset\\ncollected from the subreddit r/TIFU [56], while the WikiHow benchmark is created using the first\\nsentence of each WikiHow web page’s paragraph as the summary and the rest as source text [60].\\nLong document. For long document summarization research, arXiv, PubMed, BIGPATENT, Bill-\\nSum, and GovReport have been used in prior research to test and compare novel long document\\nsummarization models. arXiv and PubMed [20] are scientific long document summarization datasets\\ncollected from arXiv.org and PubMed.com scientific repository. Both datasets represent the earliest\\nwork on large-scale long document summarization datasets. BIGPATENT [104] is an enormous\\ndataset with over 1.3 million document-summary records of U.S. patent documents along with\\nhuman written abstractive summary. BillSum [59] is a dataset on summarizing Congressional and\\nCalifornia state bills where the content structures and stylistic features of writing are considerably\\ndifferent from documents in other domains. GovReport [49], assembled from reports published by\\nU.S. Government Accountability Office, is markedly longer than the other long document datasets.\\nOther long document benchmark datasets that are worth mentioning but are no longer widely used\\ndue to the limited amount of document-summary pairs are CL-SciSumm and SciSummNet [50, 121].\\nSome other benchmark datasets that are released more recently in the podcast [19] and dialogue\\ndomains [9, 51, 97, 138] may also be classified as long document summarization benchmark [77]\\nbut are not explored in this survey as dialogue summarization has been recognized as another\\nsub-domain due to its distinctive features as compared to other document types.\\n3.2\\nData Metrics\\nGiven a document, 𝐷, and a corresponding reference summary, 𝑆, each document will have a\\nsequence of tokens 𝐷𝑡𝑜𝑘𝑒𝑛= {𝑡1,𝑡2, ...,𝑡𝑛} and each summary will also have a sequence of tokens\\n𝑆𝑡𝑜𝑘𝑒𝑛= {𝑡∗\\n1,𝑡∗\\n2, ...,𝑡∗\\n𝑚}. Similarly, each document and summary have𝑙and𝑜sentences as represented\\nby 𝐷𝑠𝑒𝑛𝑡= {𝑠1,𝑠2, ...,𝑠𝑙} and 𝑆𝑠𝑒𝑛𝑡= {𝑠∗\\n1,𝑠∗\\n2, ...,𝑠∗\\n𝑜} respectively. Length of document and summary\\nmeasured in number of tokens are represented as |𝐷| and |𝑆| while length measured in number\\nof sentences are represented as ||𝐷|| and ||𝑆||. Extending on the works in the short document\\nsummarization domain [4, 43], the following discusses each of the five metrics used to evaluate the\\nbenchmark datasets shown in Table 1: compression ratio, extractive coverage, extractive density,\\nredundancy and uniformity.\\nCompression Ratio measures the ratio of a source document length against its reference summary\\nlength. A higher compression ratio indicates larger information loss in the original document after\\nbeing summarized. Compression ratios are measured based on tokens and sentences:\\n𝐶𝑂𝑀𝑃𝑅𝐸𝑆𝑆𝐼𝑂𝑁𝑡𝑜𝑘𝑒𝑛= |𝐷|\\n|𝑆|\\nand\\n𝐶𝑂𝑀𝑃𝑅𝐸𝑆𝑆𝐼𝑂𝑁𝑠𝑒𝑛𝑡= ||𝐷||\\n||𝑆||\\nExtractive Coverage and Extractive Density are introduced by Grusky et al. [43] based on the\\nnotion of matching fragments. Fragments are obtained by greedily matching the longest shared\\ntoken sequence where F (𝐷,𝑆) reflects a set of fragments with each fragment having a length\\nrepresented by |𝑓|. Extractive coverage calculates the percentage of tokens in summary that is a\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 5}, page_content='1:6\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\nderivation of the original source text, whereas, extractive density relates to the average squared\\nlength of the extractive fragments in the summary. The former indicates the need for a model to\\ncoin novel tokens that are not in the original source text while the latter measures whether a model\\ncan match the ground truth summary merely by extracting from the original source text without\\nrearranging or paraphrasing text.\\n𝐶𝑂𝑉𝐸𝑅𝐴𝐺𝐸(𝐷,𝑆) = 1\\n|𝑆|\\n∑︁\\n𝑓∈F(𝐷,𝑆)\\n|𝑓|\\n𝐷𝐸𝑁𝑆𝐼𝑇𝑌(𝐷,𝑆) = 1\\n|𝑆|\\n∑︁\\n𝑓∈F(𝐷,𝑆)\\n|𝑓|2\\nRedundancy relates to the redundancy of ground truth summary by measuring the average\\nROUGE-L F1-score of all distinct pairs of summary sentences [4]. As ROUGE-L measures the\\nlongest common sub-sequence overlap between two texts [70], a higher redundancy score would\\nsuggest that a candidate summary is more redundant as the sentence pairs in the ground truth\\nsummary contain more similar content in each sentence pair. For each summary consisting of m\\nsentences, S, we have a set of distinct pairs of sentences, S × S, where the redundancy score is\\ncalculated as:\\n𝑅𝐸𝐷𝑈𝑁𝐷𝐴𝑁𝐶𝑌(𝑆) =\\n𝑎𝑣𝑒𝑟𝑎𝑔𝑒\\n(𝑥𝑖,𝑥𝑗) ∈S×S,𝑥𝑖≠𝑥𝑗\\n𝑅𝑂𝑈𝐺𝐸(𝑥𝑖,𝑥𝑗)\\nUniformity measures whether content that are considered important by the reference summary\\nare uniformly scattered across the entire source document. A higher score indicates that important\\ncontent are scattered across the entire document with no obvious layout bias to take advantage of.\\nThis is calculated based on the normalized entropy of the decile positions of salient unigrams in\\nthe source text, where salient unigrams are the top 20 keywords extracted1, excluding stopwords,\\nfrom the reference summary.\\n𝑈𝑁𝐹(𝑢𝑛𝑖𝑔𝑟𝑎𝑚𝑝𝑜𝑠) = 𝐻𝑛𝑜𝑟𝑚(𝑢𝑛𝑖𝑔𝑟𝑎𝑚𝑝𝑜𝑠)\\n3.3\\nIntrinsic Characteristics of Datasets\\n3.3.1\\nShort vs Long Document Benchmark Dataset.\\nBased on Table 1 below, the following discusses the findings of intrinsic characteristics of long\\ndocument benchmark datasets in comparison to short document benchmark datasets.\\nFinding 1. Length of Long Documents: A basic yet important finding is that, except for\\nBillSum, all the other long document datasets have an average source document length of at least\\n3,000 tokens. In contrast, the longest short document dataset, CNN-DM, has an average document\\nlength of 774 tokens. This indicates that a vanilla pre-trained Transformer-based models [66, 96, 129]\\nwhich commonly have an input length limit of 1,024 tokens would need to truncate at least half\\nof the source document in the long document benchmark datasets. Thus, if pre-trained models\\nthat have proven to work well under short document settings are implemented without any long\\ndocument adaptations in their architectural settings and mechanisms, they are unlikely to generate\\nhigh-quality summaries for long documents [45, 81, 100].\\nFinding 2. High Compression Ratio and its Implications: On average, the token-level and\\nsentence-level compression ratio of the long document summarization datasets is greater than the\\nshort document datasets by 1.4 and 2.2 times respectively. For long documents, this suggests that\\neither a) there is a greater information loss in the summaries, b) the salient content is more sparsely\\ndistributed across the source documents, and/or c) the source document contains significantly\\n1We use NLTK-RAKE for keywords extraction.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 6}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:7\\nShort Document Datasets\\nLong Document Datasets\\nLong vs. Short\\nCNN-DM\\nNWS\\nXSum\\nWikiHow\\nReddit\\nArXiv\\nPubMed\\nBigPatent\\nBillSum\\nGovReport\\nAvg. Ratio\\n# doc-summ.\\n278K\\n955K\\n203K\\n231K\\n120K\\n215K\\n133K\\n1.34M\\n21.3K\\n19.5K\\n-\\nsumm tokens\\n55\\n31\\n24\\n70\\n23\\n242\\n208\\n117\\n243\\n607\\n6.9x\\ndoc tokens\\n774\\n767\\n438\\n501\\n444\\n6446\\n3143\\n3573\\n1686\\n9409\\n8.3x\\nsumm sents\\n3.8\\n1.5\\n1\\n5.3\\n1.4\\n6.3\\n7.1\\n3.6\\n7.1\\n21.4\\n3.7x\\ndoc sents\\n29\\n31\\n19\\n27\\n22\\n251\\n102\\n143\\n42\\n300\\n6.5x\\nCompressiontoken\\n14.8\\n31.7\\n19.7\\n7.2\\n18.4\\n41.2\\n16.6\\n36.3\\n12.2\\n18.7\\n1.4x\\nCompressionsent\\n8.3\\n22.4\\n18.9\\n3.3\\n14.5\\n44.3\\n15.6\\n58.7\\n9.7\\n18.1\\n2.2x\\nCoverage\\n0.890\\n0.855\\n0.675\\n0.610\\n0.728\\n0.920\\n0.893\\n0.861\\n0.913\\n0.942\\n1.2x\\nDensity\\n3.6\\n9.8\\n1.1\\n1.1\\n1.4\\n3.7\\n5.6\\n2.1\\n6.6\\n7.7\\n1.5x\\nRedundancy\\n0.157\\n0.088\\n-\\n0.324\\n0.078\\n0.144\\n0.146\\n0.223\\n0.163\\n0.124\\n1.0x\\nUniformity\\n0.856\\n0.781\\n0.841\\n0.813\\n0.777\\n0.894\\n0.896\\n0.922\\n0.903\\n0.932\\n1.2x\\nTable 1. Comparison of Short and Long Document Summarization Datasets. Intrinsic characteristics are\\ncomputed based on the average result of test samples. Average Ratios are computed based on the average\\nlong over short document statistics.\\nmore redundant information. As the high compression ratio of long document benchmark is more\\nlikely to be the results of the two former factors, this increases the relative difficulty of the long\\ndocument summarization task as a model would have to clearly identify the key narrative from the\\nsource while excluding the content that are expected to be less important by the summary readers.\\nMoreover, if there is a greater information loss in the summary of a long document, the generated\\nsummary will inevitably miss an even greater amount of information that is considered important\\nby some readers, diminishing the effectiveness of a generalized summarization approach to satisfy\\nthe needs of summary readers. This finding supports the efforts in controllable summarization,\\nwhere the final generated summaries will be based on the reader’s needs and expectations [45, 116].\\nFinding 3. Abstractiveness and Diversity of Datasets: With the exception of BIGPATENT,\\nall long document datasets have greater coverage and density values than the short document\\ndatasets. This is likely due to the genres of benchmark datasets where long documents are often\\nrelated to domain-specific articles such as scientific papers that contain more complex formulas\\nand terminologies. Nonetheless, this indicates that a model that merely extracts lexical fragments\\nfrom the original source text of a long document can still generate a summary that more closely\\nresembles the reference summary. As abstractive summarization models have recently been found to\\ncontain factual inconsistencies in up to 30% of the summary outputs in the short document domain\\n[8, 62] while extractive summarization model will faithfully preserve the original content, this\\nfinding is encouraging for the development of long document extractive models in the real-world\\nproduction level settings. Finally, as the abstractiveness of datasets have been found to greatly\\naffect the summarization strategies of a supervised model [115, 119, 128], efforts to introduce\\nbenchmark datasets with greater abstractiveness (low extractive coverage and density value) should\\nbe encouraged to improve the diversity of long document benchmark datasets.\\nFinding 4. Lesser Layout Bias in Long Document: Kryściński et al. [61] found substantial\\nlayout bias in the source text where nearly 60% of important sentences are contained in the first\\n30% of the source articles and argued that such layout bias does not apply to the other domains.\\nOur findings on the uniformity of salient content in Table 1 validates their arguments where the\\nsalient content of long documents are scattered across the entire source text more uniformly than\\nthe short documents. This suggests that unlike practices in the short document summarization\\ndomain where models are often benefited by taking advantage of layout biases [36, 90, 103], long\\ndocument models that implement a truncation strategy to process only a small subset of the leading\\ncontent of the long documents will likely suffer from significant performance degradation.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 7}, page_content='1:8\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\nLong\\nShort\\na) Pairwise Correlations Between Metrics\\nb) Facets Covered by ArXiv Ground Truth Summary\\nFig. 1. Pairwise Correlations between Metrics (figure 1a) and Facets Covered by ArXiv Ground Truth Summary\\n(figure 1b). For figure 1a, the upper diagonal reflects Pearson correlation coefficient for long document\\nbenchmarks while the lower diagonal reflects values for short document benchmarks. Figure 1b’s barplot is\\nconstructed based on human annotated testset data as described in section 3.5.\\nFinding 5. Relationship between Intrinsic Characteristics: Other than the intrinsic char-\\nacteristic measured in Table 1, the statistical relationship between these metrics could yield insights\\nregarding the underlying properties of a benchmark dataset. More importantly, whether the rela-\\ntionship between these metrics differs significantly under short and long document summarization\\nsettings should also be of great interest to practitioners. To quantify this, we report the pairwise\\ncorrelations between each metric pair for both short document (lower diagonal) and long document\\n(upper diagonal) benchmark datasets in figure 1a. The values reported are calculated using the\\nPearson correlation coefficient, 𝜌. As represented by darker blue color in figure 1a, 𝜌= 1 reflects\\na perfectly positive correlation between the metric pair and 𝜌= −1 when it is perfectly negative\\n(shown in darker red color).\\nFor positive controls, we see a strong positive relationship between the two compression ratios\\nand the two extractive metrics (coverage and density) under short and long document settings.\\nWe also see a lack of statistical correlation when uniformity is measured against other metrics as\\nuniformity relates more to the genres of documents rather than the other characteristics. We further\\nobserve redundancy to be inversely related to coverage and density, where a more abstractive\\nreference summary often contains more redundant information. This finding is consistent with\\na human evaluation study by Kryscinski et al. [62] where writers are found to be more verbose\\nand write summary content that do not add information when they are writing unconstrained,\\nabstractive summaries. Intriguingly, we see a weakly positive correlation between the extractive\\nmetrics and the compression metrics under the short document setting but a strongly negative\\ncorrelation under the long document setting. It is hypothesize that when authors have to write\\na concise summary, they are forced to paraphrase the original content more to ensure that the\\nsummary can cover the salient content within the constrained summary length.\\n3.3.2\\nComparison between Long Document Dataset Benchmarks.\\nLooking at the intrinsic characteristics between long document benchmark datasets, arXiv and\\nBIGPATENT have significantly higher compression ratios but lower extractive density values than\\nthe others, indicating that two of these datasets require a summarization model to generate a\\nsignificantly shorter summary that is not written in the same way as the source text. As discussed\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 8}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:9\\nabove, this is likely because for a summary to cover more content within a constrained summary\\nlength, one has to paraphrase the original content more. This is also evidenced by the diverging\\nvalues between extractive coverage and density metric for arXiv benchmark dataset that suggest\\nsummaries of arXiv scientific papers have high matching tokens and terminologies with the source\\ndocument (high coverage) but low matching phrases (low density). Overall, the BIGPATENT dataset\\nis the most suitable benchmark for long document supervised abstractive summarization due to its\\nlow coverage and density, substantial training sample pairs to serve as supervisory signals, and\\nhigh uniformity in salient content. However, only a handful of fully-supervised abstractive long\\ndocument summarization works [94, 127, 129] evaluated their models on BIGPATENT, limiting the\\nvisibility of current progress on long document summarizers in general applications. This is despite\\nthe fact that BIGPATENT was introduced not long after arXiv and PubMed. Encouragingly, with\\nthe recent introduction of long document datasets in domains other than scientific papers including\\nfinancial reports [75] and books [63], the research progress of long document summarization\\nmodels towards general application should become clearer in the near future.\\n3.4\\nFine-grained Analysis on ArXiv\\nTo perform fine-grained human analysis on the arXiv benchmark, this survey implements a stratified\\nrandom sampling strategy based on the 6 different categories of scientific domains contained\\nin the arXiv.org scientific repository: physics (ph), computer-science (cs), mathematics (math),\\nquantitative-biology (q-bio), quantitative-finance (q-fin) and statistics (stat). In total, we obtain over\\n700 annotated ground truth summaries with physics having the most samples (369) followed by\\ncomputer science (140). Based on fine-grained human analysis of 743 ground truth summaries in\\nthe arXiv test set, this subsection reports the disturbing data quality results and studies the degree\\nof diversity in formatting style of reference summary.\\nNoise in arXiv benchmark dataset: With the advent of data-hungry neural architectures, there\\nhas been an enormous demand for benchmark datasets with document-summary pairs that are\\nat least in the tens of thousands created through heuristic means such as scraping it directly\\nfrom the web. As a result, depending on the means of extracting these datasets, the quality of\\nbenchmark datasets may vary significantly from one another. To this end, Kryściński et al. [61]\\nhave quantified the percentage of samples with noise for CNN-DM and Newsroom from the short\\ndocument summarization datasets to be 4.19% and 3.17% using simple heuristic methods. The\\nnoises found in the datasets through heuristic means can only suggest a lower bound of what\\nthe true amount of noises are as heuristic approaches can only detect obvious structural flaws in\\nthe samples. This suggests that the true underlying noises are extremely widespread and often\\nunderstated. Glaringly, in our experiment, the problem of noisy data affects more than 60% of the\\nannotated ground truth summaries in the randomly sampled arXiv test set. This is greater than 54%\\ndetected in XSUM dataset [111]. While many of the errors and noises are minor, more than 15% of\\nthe reference summaries have significant errors where at least half of the summary contains errors,\\nrendering the summaries to be unreadable. Reassuringly, the rest of the test sets with identified\\nnoises are not overly significant and often only affect one or two sentences in a benchmark dataset\\nwith an average of 10 sentences in the reference summary. To further understand why the noises\\nand errors occurred, we trace the original data based on the arXiv id provided by the benchmark\\ndatasets. It was found that many errors occurred such as missing content or sentence breaking after\\na newline could be due to large-scale scraping of the original data using pandoc [20]. As neural\\nsummarization models may overfit to these problematic noises and contribute to less interpretable\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 9}, page_content='1:10\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\nbenchmarking results, we release the annotated data to allow a better understanding of the common\\nnoises and to encourage quality improvement in future benchmark datasets2.\\nContent Coverage of Reference Summary: Other than analyzing the noises in the arXiv benchmark\\ndataset, we also explore to what extent the ground truth summary covers various sections or\\nfacets of the source article. Figure 1b shows the average distribution of sections covered by ground\\ntruth summaries for different domains. The distribution is plotted based on the assumption that\\neach sentence in the reference summary covers a single facet of the original article and human\\nannotators are asked to identify the section covered by the summary sentences. The facets studied\\nare Introduction, Methodology, Result, Conclusion and Limitation of Research. Interestingly, while\\nthe reference summaries in all domains have covered introduction and methodology sections\\nwith similar emphasis, we see a negative correlation between contribution and conclusion (i.e.,\\npapers that emphasize contributions will write less on conclusions, and vice-versa). Notably, we see\\nscientific papers in the mathematics domain emphasize more on the contribution while papers in\\nthe physics domain emphasize more on conclusions. These results make intuitive sense as findings\\nin mathematics often do not lead to a strong substantive conclusion. The trade-off between various\\nsections also illustrates the inevitable information loss when summarizing a long document as the\\nsummary can only describe certain aspects of the source document but not all.\\nStyle of Writing and External Knowledge: Importantly, except for quantitative-biology, all scientific\\npapers do not discuss the limitations of their research. This is consistent with common practices\\nof writing abstracts to attract readers in reading the original paper by emphasizing on the result\\nfindings and contributions of the authors. Nevertheless, most researchers would find a discussion\\non the limitations of research works to be informative and significant. Whether the abstract itself\\nrepresents the best possible summary for a summarization architecture to imitate from and learn\\nhow to appropriately summarize all the salient content including the limitation discussed in the\\noriginal paper remains an important question to be answered. Recent progress on summarization\\napproaches that generate user-specific summaries based on the need of readers are also important\\ndirections towards general applicability of summarization models in commercial settings [45, 116].\\nLastly, to summarize the limitations of a paper often requires more external knowledge outside of\\nthe content related to the source document and whether current summarization models are able to\\ninfer such knowledge from the benchmark dataset is an interesting study left for future works.\\n4\\nMODELS\\n4.1\\nOverview\\nThe following describes the differences between the extractive, abstractive and hybrid summariza-\\ntion approaches and the general taxonomy of a summarization system.\\nA. Extractive, Abstractive and Hybrid Approach.\\nThe works in automatic text summarization research are traditionally classified into three different\\nsummarization approaches: (i) the extractive approach that involves direct extraction of salient\\nfragments such as sentences of the original documents into a summary [15, 38], (ii) the abstractive\\napproach imitates human behavior of paraphrasing important parts of a document into a summary\\n[101, 103] and (iii) the hybrid approach that attempts to combine the best of both approaches\\n[37, 77]. Intuitively, the extractive summarization method is an easier machine learning task and\\ncan be thought of as a classification and/or ranking problem of extracting lexical fragment units\\n(e.g., sentences) into a summary. Contrastively, abstractive summarization requires paraphrasing\\n2The annotated dataset are released: https://github.com/huankoh/long-doc-summarization\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 10}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:11\\nImportant sentence\\nLess Important sentence\\nUnimportant sentence\\nWord/Sentence/Section \\nAttention Mechanism\\nR \\nN \\nN\\nR \\nN \\nN\\nR \\nN \\nN\\nX1\\nX2\\nXn\\nEncoder\\n...\\nR \\nN \\nN\\nR \\nN \\nN\\nR \\nN \\nN\\nY1\\nY2\\nYn\\nDecoder\\nC1\\nCn-1\\n...\\nR \\nN \\nN\\nR \\nN \\nN\\nR \\nN \\nN\\nY1\\nY2\\nYn\\nDecoder\\n...\\nExtractive\\nAbstractive\\nContext vector\\nC\\nC0\\nClassifier\\nPredict/\\ngenerate\\nA - Classic Graph (Extractive)\\nB - RNN-based Model Architecture\\nC - Transformers\\nSoftmax\\nLinear\\nAdd & Norm\\nFeed \\nForward\\nAdd & Norm\\nMulti-Head\\nAttention\\nAdd & Norm\\nMasked \\nMulti-Head \\nAttention\\nOutput\\nEmbedding\\nOutputs \\n(Shifted right)\\nAdd & Norm\\nFeed \\nForward\\nAdd & Norm\\nMulti-Head\\nAttention\\nInput \\nEmbedding\\nInputs\\nN x\\nN x\\nOutput \\nProbabilities\\nPositional \\nEncoding\\nPositional \\nEncoding\\nFig. 2. Overview of Model Architectures.\\nimportant ideas of a document into a summary either by rearranging words and phrases from\\noriginal text or contriving novel wordings while maintaining the factual consistency of the generated\\nsummary with the original document.\\nSince the extractive summarization approach only extracts and arranges the original text that\\nit believes to be salient and does not alter the original text, it enjoys the benefit of generating\\nsummaries that are factually consistent with the source document [21]. Nevertheless, as human-\\nbased summarization often involves paraphrasing ideas and concepts into shorter, concise sentences,\\nthe extracted sentences of this approach often contain redundant and uninformative phrases [42].\\nWhile there exist extractive summarization models that break a source document into lower lexical\\nunits than sentences (e.g., elementary discourse units) [120], they are often not applied in the long\\ndocument summarization domain due to the extreme length of the input document.\\nOn the other hand, mimicking how humans write summaries, the abstractive summarization\\napproach presents a blue-sky potential of generating summaries that are fluent, concise and relevant\\nto the source document [103]. It can also incorporate external knowledge to the summary depending\\non the needs of a user [81]. However, at the current stage of development, summaries generated\\nby the state-of-the-art abstractive models often contain a significant amount of content that is\\nfactually inconsistent with the source document, limiting its application in commercial settings\\n[8, 62].\\nFinally, in response to the limitation of current model architectures and designs, the hybrid\\nsummarization approach only differs from the abstractive summarization approach in that it takes\\nin a carefully chosen subset of the original input document rather than the entire input document\\nin its original form [37, 94]. This extra step reduces the burden on the abstractive summarization\\nmodels that have to generate an abstract summary and select important content at the same time.\\nThis approach is used more often in the long document summarization domain because current\\nmodels still fail either (a) at reasoning over extremely long texts [77, 82] and/or (b) suffers from\\nmemory complexity issues and hardware limitations that prevent it from processing over a long\\ninput text [49, 127].\\nB. General Taxonomy.\\nIn each long document summarization model, this paper breaks down a model into two different\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 11}, page_content='1:12\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\nconstituents: (i) Main Architecture and (ii) its Mechanisms. The main architecture refers to the\\ncore framework structure that a model uses and the mechanisms are the different settings or\\nmodifications implemented by a model to the main architecture. Two differing models may use\\nthe same main architectures but are implemented with different mechanisms, and vice-versa. For\\nexample, models that use graph-based main architecture may use different encoding mechanisms\\nin vectorizing the sentences of an input document. The following describes the various main archi-\\ntectures of the summarization models together with how previous works differ in the mechanisms\\nemployed to generate long document summaries.\\n4.2\\nMain Architecture and its Mechanisms\\nIn the search for optimal architectural settings of summarization systems, the research field started\\nout with many different novel designs of main architectures and mechanisms but often converge\\ntowards a few ideas that are often most effective until another ground-breaking idea that leap-frogs\\nthe performance of previous systems, and the cycle repeats.\\n1. Graph Architecture:\\nFor the extractive summarization approach, the classic graph architecture involves a two-stage\\nprocess of mapping a document into a graph network, where the vertices are sentences and the\\nedges are the similarity between these sentences, and extracting the top-𝐾sentences. The sentences\\nare ranked based on the graph centrality scoring of each sentence [29, 83]. As there are many\\ndifferent ways to (a) encode or vectorize a sentence before calculating the similarity between them\\nand (b) calculate the centrality score of each sentence, research involving this architecture often\\ndiffers only in these two mechanisms. For example, with respect to the former mechanism, graph\\narchitecture in the past [29, 83] encodes sentences based on word-occurrence or term frequency-\\ninverse document frequency (Tf-Idf) while graph architecture today [69, 135] encodes sentences\\nwith state-of-the-art pre-trained models. On the other hand, to improve the centrality scoring\\nmechanism, PacSum [135] and FAR [69] adjust the centrality score of a sentence based on whether\\nthe other sentences come before or after it, while HipoRank [25] exploits the discourse structure\\ncontained in by adjusting the centrality score with positional and sectional bias. In general form,\\ngiven a set of sentences in the original source document, 𝐷= {𝑠1,𝑠2, ...,𝑠𝑚} with the inter-sentential\\nsimilarity relations represented as 𝑒𝑖𝑗= (𝑠𝑖,𝑠𝑗) ∈𝐸where 𝑖≠𝑗, the following illustrates the\\naforementioned architecture in computing the scoring for each sentence:\\n𝑐𝑒𝑛𝑡𝑟𝑎𝑙𝑖𝑡𝑦(𝑠𝑖) =\\n∑︁\\n𝑗∈{1,...,𝑖−1,𝑖+1,...,𝑚}\\n𝑒𝑖𝑗∗𝐵𝑖𝑎𝑠(𝑒𝑖𝑗)\\nThe similarity between each sentence is computed using similarity measures such as dot product\\nor cosine similarity, and the sentences are vectorized using Tf-Idf or BERT representation values.\\nThe final summary is generated by extracting the top-k sentences ranked by 𝑐𝑒𝑛𝑡𝑟𝑎𝑙𝑖𝑡𝑦(𝑠𝑖). Im-\\nportantly, while there are other classical architectures [38, 112], the graph architecture is worth\\na separate mentioning here due to the fact that (a) it remains as a strong baseline against other\\nadvanced architectures, (b) it can effectively incorporate external knowledge as an inductive bias to\\nthe calculation of the importance of a sentence and (c) it achieves state-of-the-art result in long doc-\\nument unsupervised extractive summarization setting when integrated with current state-of-the-art\\npre-trained models [25, 69]. Lastly, other than the multi-sentence compression approach [6, 54, 132]\\nthat may be extended to long document summarization tasks, there has been no applicable work\\non classical graph-based architecture for long document abstractive summarization.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 12}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:13\\n2. Other Classical Architectures:\\nIn the early work of automated, non-neural text summarization models, past research mostly\\nfocused on the extractive summarization approach due to the difficulty of the abstractive sum-\\nmarization task. The main architectures that were tested ranged from support vector machines\\n[12, 106], Bayesian classifiers [64], decision trees [57, 78] to citation network-based summarization\\n[95]. The ones that remained relevant when comparing model performance across various bench-\\nmarks in long document summarization settings are LSA [38], which is based on Singular Value\\nDecomposition (SVD), and SumBasic [112] that ranks sentences by simple average word-occurrence\\nprobability [112].\\n3. Recurrent Neural Networks:\\nExtractive summarization that employed neural networks with continuous representations rather\\nthan pre-trained word embeddings on traditional techniques [58, 124] was proposed by Cheng\\nand Lapata [15]. The model implemented an RNN encoder-decoder architecture with attention\\nmechanism to locate the region of focus during sentence extraction process. Nevertheless, due to\\nthe lack of a large-scale long document dataset and the RNN’s inability in capturing long-range\\ntemporal dependencies across a long input text, it wasn’t until Xiao and Carenini [118] that tried\\nimplementing LSTM-minus (a variant of RNN) on solving long document summarization task.\\nTypical of a long document summarization system, it incorporates discourse-information (i.e. section\\nstructure) of the source document by encoding the section-level and document-level representation\\ninto each sentence to significantly boost the model performance. Pilault et al. [94] also suggested\\ntwo different variants of RNNs on extractive summarization for long document summarization.\\nRather than utilizing pre-trained word embeddings, they implemented a hierarchical LSTM to\\nencode words and sentences separately.\\nWhen it comes to the abstractive summarization approach, Celikyilmaz et al. [10] proposed\\nmultiple communicating agents to address the task of long document summarization. However,\\nas compared to other simpler architecture, this approach did not gain significant traction after\\nthe introduction of the first large-scale dataset on long scientific documents. Together with the\\ncontribution of two most commonly used long scientific document datasets, arXiv and PubMed,\\nCohan et al. [20] presented an LSTM encoder-decoder architecture where the decoder attends to\\neach section of the source document to determine section-level attention weights before attending\\nto each word. While similar architectures have been widely used in prior works, this work ef-\\nfectively incorporates discourse information that suits the long document summarization task well.\\n4. Transformers:\\nThe Transformer model proposed in 2017 together with the pre-trained Bidirectional Encoder\\nRepresentation from Transformers (BERT) model that was based on the Transformer model itself\\nhave taken the NLP area by storm [23, 113]. Like other NLP tasks, subsequent summarization\\nmodel architectures have changed significantly to take advantage of these two momentous ideas.\\nImportantly, BERTSum [74] showed that by modifying the BERT-segmentation embeddings, it\\ncan capture not only sentence-pair inputs but multi-sentential inputs. The BERTSum model could\\neffectively solve both extractive and abstractive summarization tasks. The extractive summarization\\nmodel proposed by BERTSum involves stacking a Transformer-based classifier on top of the fine-\\ntuned BERT to select and extract salient sentences while the abstractive summarization model\\ninvolves a classic encoder-decoder Transformer framework where the encoder is a fine-tuned\\nBERT and the decoder is a randomly-initialized Transformer that is jointly trained together in\\nan end-to-end manner. While effective, this architecture cannot be implemented to solve long\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 13}, page_content='1:14\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\ndocument summarization tasks due to the BERT’s input length limit of 512 tokens. In this survey,\\nwe define Transformer as the main architecture and settings related to the Transformer model as\\nthe mechanisms including the use of different types of pre-trained models. As Transformer has\\neffectively replaced most main architectures in both summarization approaches as state-of-the-art\\nmodels, the various mechanisms applied in the long document summarization context will be\\nthoroughly discussed in section 4.3.\\n4.3\\nMechanisms of Transformer-based Architectures\\nTransformer-based model is ubiquitously state-of-the-art across a wide range of tasks in the\\nNLP domain. In line with this development, recent works in the long document summarization\\nmodels often involve using the same Transformer base architecture but with different proposing\\nmechanisms. These Transformer-based models involve implementing novel mechanisms with long\\ndocument adaptations to ensure the task of summarizing a document with significantly longer input\\nsequence texts can be effectively addressed. The mechanisms used by extractive, abstractive and\\nhybrid Transformer-based summarization models are described in the following with an overview\\nof mechanisms used by abstractive and hybrid summarization models shown in Figure 3.\\nLong Doc \\nAdaptation\\nDiscourse \\nStructure\\nTruncation\\nContent \\nSelection\\nTransformer-based \\nAbstractive \\nSummarization \\nSystem\\nEﬀicient \\nAttention\\nPre-training \\nTasks\\nSignal \\nGuidance\\nPrompt \\nEngineering\\nSource Doc\\nFig. 3. Overview of Transformer-based Abstractive & Hybrid Summarization Models.\\nA. Extractive Transformer.\\nAs Transformer and its pre-trained models are optimized for short document settings, they may not\\nreason well over long text sequences if not properly fine-tuned. To this end, Cui et al. [22] proposed\\ncombining neural topic modeling together with BERT in learning a topic-enhanced, inter-sentence\\nrelationship across the entire document. Nonetheless, the issues of memory complexity and input\\ntoken length limits were not resolved and significant source text is truncated under this research\\nsetting. Recently, Cui and Hu [21] proposed a memory network that incorporates graph attention\\nnetworks and gated recurrent units to dynamically select important sentences through sliding a\\nwindow along the entire source document. This approach can effectively integrate the pre-trained\\nBERT model for long document summarization task by limiting its usage within each window,\\nwhere the window size is set to be lower than or equal to 512 tokens.\\nB. Abstractive Transformer.\\n1) General Sequence-to-Sequence Pre-training Task\\nSince the advent of BERT [23], various large-scale models with different pre-training tasks have been\\nintroduced. As summarization with the abstractive approach is naturally a sequence-to-sequence\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 14}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:15\\ntask, a pre-trained model with a sequence-to-sequence objective task would suit it better rather than\\nan encoder-only (e.g., BERT/RoBERTa) or a decoder-only (e.g., GPT-2/GPT-3) pre-trained model.\\nIn the summarization domain, Bidirectional and Auto-Regressive Transformers (BART) [66] and\\nText-to-Text Transfer Transformer (T5) [96] are the two most widely used sequence-to-sequence\\npre-trained models. BART is pre-trained on a self-supervised task of reconstructing arbitrarily\\ncorrupted text while T5 is pre-trained on both unsupervised and supervised objectives, such as\\ntoken masking, as well as translation and summarization. Interestingly, none of the supervised\\nTransformer models in the long document summarization domain has implemented a summarizer\\nwith T5 pre-training task despite its success in the short document domain [100].\\n2) Gap-Sentence Generation (GSG) Pre-training Task\\nOther than the generalized pre-training task like BART and T5, PEGASUS [129] attempted to signif-\\nicantly advance the progress in the abstractive summarization field through large-scale pre-training\\nwith objectives that are specific to the summarization task. The proposed model is self-trained on\\ntwo large scale datasets (C4 and HugeNews) with the gap-sentence generation pretraining task.\\nGap-sentence generation pretraining task draws a close resemblance with the general summariza-\\ntion task by self-supervising the model to generate sentences that are masked entirely in a given\\ndocument. At the time of PEGASUS model release, the model effectively achieves state-of-the-art\\nresults across 12 different benchmark datasets, including long document arXiv and PubMed dataset.\\n3) Efficient Attentions\\nThe vanilla Transformer models that utilize full attention have a memory complexity 𝑂(𝑛2). This\\nattribute limits its wider usage across many domains, including long document summarization.\\nFor example, to circumvent the input tokens limits of PEGASUS, DANCER [37] summarizes each\\nsection of the long document separately and concatenates each of them to form the final summary.\\nAs not all benchmark datasets contain discourse information such as section structures, this limits\\nthe model usage in many long document summarization settings. To this end, researchers have\\nproposed various ingenious ideas to reduce the memory and time complexity of Transformer\\nmodels. The variants of Transformer models that require less memory are often known as efficient\\nTransformers [109, 110] and the mechanism is referred to as efficient attentions [49].\\nLongformer [2] combines local attention, stride patterns and global memory for fine-tuning\\npre-trained BART to effectively summarize long documents with a maximum input length of\\n16,384 tokens as opposed to the 1,024 token limit of the original BART model. The model achieved\\nstate-of-the-art results in the long document summarization along with other NLP tasks when\\nthe model was introduced. BigBird [127] also implemented the efficient attention mechanism\\non Transformer-based abstractive summarizer by utilizing the same attention modifications as\\nLongformer with an additional random pattern to achieve matching performance results in terms\\nof ROUGE score. An important work by Huang et al. [49] explores and compares the performance\\nof different variants of efficient transformers in the context of long document summarization.\\n4) Prompt-Engineering\\nThe GPT-3 model [7] has strongly demonstrated that large-scale pre-trained language models\\ncan achieve impressive results on numerous downstream language tasks in zero- and few-shots\\nexperimental settings. Rather than fine-tuning the language models for specific tasks in a conven-\\ntional way, natural language prompts and task demonstrations were created for GPT-3 to infer and\\ncomplete the tasks. Importantly, this is different from the conventional tagging method such as\\n<bos> token for conditional generations or taking [CLS] tag from BERT for classification tasks.\\nPrompt engineering refers to taking the extra step to design a natural language prompt or template\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 15}, page_content='1:16\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\nClassification Task\\nSummarization Task\\nConventional \\nPrompt Engineering \\nConventional \\nPrompt Engineering\\nCLS\\nthis is a long doc  . [SEP]\\nCLS Head\\nlabel: 0\\nlabel: 1\\n[CLS] this is a long doc . [SEP]   this is                      .\\n[MASK]\\nMLM head\\ncorrect (label: 1)\\nincorrect (label: 0)\\nINPUT\\nPROMPT\\nINPUT\\nthis is a doc relates to survey on summarization . \\n<bos> a summarization survey . <eos>\\nENCODER\\nDECODER\\nthis is a doc relates to survey on summarization . \\n<bos> survey: a summarization survey . <eos>\\nPROMPT\\nENCODER\\nDECODER\\nFig. 4. How prompt engineered language tasks differ. Classification task example from [34] and summarization\\ntask using keywords from original document as language prompt from [45].\\nthat can optimize the pre-trained model for a specific task. Figure 4 illustrates this important\\ndifference.\\nMany works have explored the ways of uncovering the right prompt for various downstream\\nlanguage tasks to significantly boost the performance of pre-trained models [34, 108]. In the long\\ndocument summarization research area, CRTLSum [45] achieves significant improvement on vanilla\\nfine-tuned BART model on arXiv dataset through prompt-engineering. The model attempts to\\nmore effectively use the pre-trained BART model with the help of extracted keyword prompts, as\\nshown in Figure 4. Further, the work also showed that, given an optimized language prompt, the\\nimplemented BART summarization model can achieve ROUGE score that matches ROUGE score of\\noracle summaries in the test dataset.\\n5) Signal Guidance\\nUnlike a prompt-engineering mechanism that requires an engineered language prompt or template\\nfor a given task, the signal guidance mechanism relates to utilizing signals as inputs to lead models\\nin better identifying and summarizing important content of source texts. Using this approach,\\nthe GSum [26] model implemented a fine-tuned BART model with dual encoders, one for input\\ndocument and another for extracted signals, and a decoder that attends to both encoded represen-\\ntations. Similar signal based approach is also used by Ju et al. [55] to implement an unsupervised\\npipeline-based long document summarization model.\\n6) Discourse Bias\\nSimilar to the signal guidance mechanism, discourse bias involves the inclusion of the discourse\\nstructure of a source document such as the section of a sentence as signals for summarization\\nsystems to better identify and summarize important content in the original source text [17, 122].\\nThis mechanism can be classified under signal guidance but is mentioned separately due to its\\neffectiveness and popularity in Transformer [37, 94] and non-Transformer [20, 25] based long\\ndocument summarization models. Unlike short documents, long documents often contain discourse\\nstructure information such as table of content, section structures, references and others to guide a\\nhuman reader in comprehending the original document and previous works have exploited this\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 16}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:17\\ninformation to achieve state-of-the-art results. Nonetheless, previous works in the long document\\nsummarization domain only utilized discourse information that is made available by the benchmark\\ndatasets and did not implement automatic discourse parsing using RST trees [79] and coreference\\nmentions due to the difficulties of building an effective representation for a document with extreme\\ninput length [52, 120].\\nC. Hybrid Transformer - Content Selection + Abstractive Summarization.\\nThe hybrid summarization approach only differs from the abstractive approach in that it takes in a\\ncarefully chosen subset of the input document rather than the entire input document. This extra\\nstep reduces the burden on the neural summarizers that have to generate an abstract summary and\\nselect important content at the same time. Some also refer models that utilize the hybrid approach\\nas retrieve-the-summarize model because it involves retrieving a subset of long document text\\nbefore summarizing it [131]. TLM+Ext [94] first implemented this method by limiting inputs of\\nthe scientific articles in arXiv datasets as the introduction of the document, a subset of carefully\\nselected sentences of the original article using extractive summarization approach, and, finally,\\ninclude the remaining text if there remains extra space for Transformer-based decoder. However,\\ngiven the effectiveness of sequence-to-sequence neural models, one limitation of this work is that\\nit only utilizes a decoder framework to generate the final summary rather than an encoder-decoder\\nframework that most subsequent works on abstractive and hybrid summarization approach do.\\nConsequently, LoBART [77] proposes a hybrid summarization system that completes a summary\\ngeneration in two separate steps, (i) content selection: using a multi-task RNN, select salient content\\nfrom the original source document until the total text output reaches the limit of the sequence-\\nto-sequence pre-trained BART model and (ii) abstractive summarization: summarize the carefully\\nselected subset using a pre-trained BART model with efficient transformer mechanism. SEAL\\n[134] presents a generalized encoder-decoder framework for transformer-based long document\\nsummarization and proposed an abstractive summarization system that selects salient content and\\ndynamically choose segments of the selected content for the decoder to attend and summarize in\\nan end-to-end manner. The architecture, however, did not attempt in exploiting the large-scale\\npre-trained models that were used in most summarization research works. Lastly, facing a similar\\nissue, development in the open-domain question-answering and knowledge-intensive language\\ntasks reflect an interesting parallel with the progress in the long document summarization domain\\n[67, 91].\\n4.4\\nSummary of Trends in Long Document Summarization Systems\\nTable 2 summarizes the trends and developments in long document summarization models as\\ndiscussed above. The two standout base architectures that are used in the long document sum-\\nmarization domains are graph-based ranking algorithm for unsupervised extractive models and\\npre-trained Transformer for supervised abstractive models. While both architectures were ini-\\ntially proposed and tested on short documents, they can be effectively adapted to summarize long\\ndocuments after incorporating novel mechanisms.3\\nFinding 1. Graph-based Extractive Models with Discourse Bias: Classical graph-based\\nunsupervised extractive models have been found to suffer from picking similar sentences that\\nresults in a summary with redundant sentences [69]. To this end, HipoRank [25] implements the\\ngraph-based architecture for unsupervised extractive summarization by including the sectional\\ninformation of ArXiv/PubMed as inductive bias when calculating centrality scoring to achieve\\nstate-of-the-art results. The discourse bias mechanism is commonly incorporated by other proposed\\nsummarization models, including models with RNN and Transformer base architectures.\\n3For a brief description of each model, please refer to the Supplementary Materials.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 17}, page_content='1:18\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\nModel\\nArchitecture\\nPre-Train\\nLong Document Mechanism\\nMax Token\\nUnsupervised\\nExtractive\\nPacSum [135]\\nGraph\\nBERT\\nDiscourse Bias\\n-\\nHipoRank [25]\\nGraph\\nBERT\\nDiscourse Bias\\n-\\nFAR [69]\\nGraph\\nBERT\\nFacet-Aware Scoring\\n-\\nIBSumm [55]\\nPipeline\\nSciBERT\\nSignal Guidance\\n-\\nSupervised\\nExtractive\\nGlobalLocal [118]\\nRNN\\n-\\nDiscourse Bias\\nSent-CLF/PTR [94]\\nRNN\\n-\\n-\\n-\\nTopic-GraphSum [22]\\nGAT\\nBERT\\nNeural Topic Modelling\\n-\\nSSM-DM [21]\\nDMN\\nBERT\\n-\\n-\\nSupervised\\nAbstractive\\nDiscourse-Aware [20]\\nRNN\\n-\\nDiscourse Bias\\n-\\nLongformer [2]\\nTransformer\\nBART\\nEfficient Attention\\n16,384\\nBigBird [127]\\nTransformer\\nPEGASUS\\nEfficient Attention\\n4,096\\nGSUM [26]\\nTransformer\\nBART\\nSignal Guidance\\n4,096\\nCRTLSum [45]\\nTransformer\\nBART\\nPrompt Engineering\\n1,024\\nHAT-BART [99]\\nTransformer\\nBART\\nHierarchical Attention\\n1,024\\nHEPOS [49]\\nTransformer\\nBART\\nEfficient Attention\\n10,240\\nSupervised\\nHybrid\\nTLM+Ext [94]\\nTransformer\\n-\\nContent Selection + Discourse Bias\\n-\\nDANCER [37]\\nTransformer\\nPEGASUS\\nContent Selection + Discourse Bias\\n-\\nSEAL [134]\\nTransformer\\n-\\nContent Selection w/ Segment-wise Scorer\\n-\\nLoBART [77]\\nTransformer\\nBART\\nContent Selection + Efficient Attention\\n-\\nTable 2. Long Document Summarization Models in Chronological Order. Max token represents the maximum\\ninput sequence length that the model can process and any text that exceeds this cutoff point will be truncated.\\nFinding 2. Pre-training task for Abstractive Summarization Models: Interestingly, despite\\nhaving other pre-trained sequence-to-sequence models such as T5 [96], BART and PEGASUS are the\\nonly two Transformer-based pre-trained models that were used for long document summarization\\n[66, 129]. Nevertheless, as both pre-trained models are trained on short documents, they have an\\ninput limit of 1,024 tokens. To process long documents that are longer than this limit, the pre-trained\\nTransformers will have to incorporate long document mechanisms to extend the input limits.\\nFinding 3. Long Document Mechanisms for Transformer: As pre-trained models were of-\\nten trained on large-scale datasets with input limit length between 512 to 1,024 [23, 66], these\\nTransformer-based pre-trained models were optimized for short document language tasks rather\\nthan long documents. Without any long document mechanisms to adapt these models for the long\\ndocument summarization task, Meng et al. [82] has shown that BART cannot summarize a long\\ndocument effectively. Other than the discourse bias mechanism, we observe that (a) efficient atten-\\ntion and (b) content selection mechanisms are the two most notable long document mechanisms.\\nAs the content selection mechanism requires a separate retriever to extract salient content from\\nthe source (i.e., the hybrid approach), we distinguish Transformer models with content selection\\nmechanism as the retrieve-then-summarize model [131] and the pure encoder-decoder Transformer\\nwithout this mechanism as an end-to-end model for the rest of this work. Lastly, it is also important\\nto note that both mechanisms can be jointly implemented within a single architecture, where the\\ncontent selection mechanism will extract a longer subset of input to be processed by a Transformer\\nwith efficient attention [77].\\n5\\nMULTI-DIMENSIONAL ANALYSIS OF LONG DOCUMENT SUMMARIZERS\\nGiven the important findings in the graph-based unsupervised extractive model and the Transformer-\\nbased supervised abstractive model in the previous section, we design an experiment with the aim\\nof thoroughly understanding the reasons behind the popularity of these architectures and its mech-\\nanisms. Our experiment tests out the graph-based extractive and Transformer-based abstractive\\nsummarization architectures and its mechanisms on the arXiv benchmark dataset. The documents\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 18}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:19\\nin the arXiv dataset have an average length of 6,446 tokens. Mechanisms of the supervised extractive\\napproach are not examined as the architectures used between the proposed models vary greatly.\\n5.1\\nImplementation\\n5.1.1\\nGraph - Unsupervised Extractive.\\nTo investigate the effect of incorporating long document discourse structure information, we\\nexperiment with four unsupervised graph models by varying the two following mechanisms:\\n• Sentence Encoder Mechanism: Sentences of source text are encoded either using Term\\nfrequency–inverse document frequency (Tf-Idf) [53] or BERT SentenceTransformer [98].\\n• Discourse Bias Mechanism: For both models that implement Tf-Idf or BERT sentence\\nencoder, we experiment a model with the long document discourse bias mechanism and\\none without the bias. For models without a discourse bias mechanism, the centrality score\\nof each sentence is computed based on the summation of cosine similarity between other\\nsentences. For models with a discourse bias mechanism, the mechanism implemented follows\\nthe work of Dong et al. [25]. For each sentence, we adjust centrality score based on the\\nsentence position within each section, 𝑐𝑖𝑛𝑡𝑟𝑎(𝑠𝑖), and the sentence’s section position within\\nthe document, 𝑐𝑖𝑛𝑡𝑒𝑟(𝑠𝑖). Sentences that are closer to the section and document boundaries\\nwill be given higher importance. The \"discourse-aware\" centrality score for each sentence is:\\n𝑐(𝑠𝑖) = 𝜇1 · 𝑐𝑖𝑛𝑡𝑒𝑟(𝑠𝑖) + 𝑐𝑖𝑛𝑡𝑟𝑎(𝑠𝑖)\\nwhere 𝜇1 is a weighting factor for inter-section centrality. Following the original author, we\\nfine-tune the weighting factor based on validation set. To ensure comparability, the maximum\\nlength of summary for all unsupervised extractive models is set to be 242 tokens4.\\n5.1.2\\nTransformer - Supervised Abstractive.\\nTo study the current state-of-the-art abstractive neural summarizers, we experiment with two\\ndifferent pre-trained Transformers, BART and PEGASUS. For BART, we analyze the long document\\nmechanisms from the perspective of two common approaches:\\n• End-to-End: We experiment with three end-to-end BART models. Firstly, a vanilla BART\\nmodel with full self-attention that will truncate any input text that exceed 1,024 tokens.\\nThen, two BART models with efficient longformer attention [2] that can extend up to 4,096\\nand 16,384 input tokens respectively. The main goal of assessing end-to-end BART with\\nand without efficient attention is to assess how the quality of the generated summary is\\naffected when BART is adapted from a short document summarizer into long document\\nsummarizer by allowing it to process long input sequences at the cost of full self-attention.\\nFor implementation, due to lack of computational resources, we only fine-tuned original\\nBART on arXiv and obtain the weights of longformer that was trained on arXiv from the\\noriginal author5.\\n• Retrieve-then-summarize For BART retrieve-then-summarize model, our experiment fol-\\nlows entirely the implementation of LoBART by [77]6. LoBART has two variants: (a) BART\\nwith full self-attention that takes in a selected subset of input text with a maximum length of\\n1,024 tokens and (b) BART with efficient local attention where maximum length of subset\\ninput is 4,096 tokens. The only difference between the two variants is the amount of content\\nto be retrieved from the original source text before feeding it into the Transformer BART\\nmodel. The two main objectives of experimenting with the retrieve-then-summarize BART\\n4Details of implementation are reported in the Supplementary Materials.\\n5https://huggingface.co/allenai/led-large-16384-arxiv\\n6https://github.com/potsawee/longsum0\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 19}, page_content='1:20\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\nmodels are to assess: (1) the effectiveness of content selection mechanism in adapting short\\ndocument BART to summarize a long document and (2) whether the performance is improved\\nwhen content selection mechanism is combined with efficient attention mechanism.\\nAs there is no existing framework that applies content selection mechanism on PEGASUS, we\\nonly experiment two end-to-end PEGASUS model: original PEGASUS7 and PEGASUS with BigBird\\nefficient attention8. The models have input token limit of 1,024 and 4,096 respectively. Pre-trained\\nweights for both model variants on arXiv benchmark are obtained directly from the original author.\\n5.1.3\\nAssessing Model Outputs.\\nRather than relying entirely on ROUGE score like most summarization research settings, we use\\nfour different metrics to analyse model summary outputs from three important dimensions (D#):\\nrelevance, informativeness and semantic coherence.\\n• D1 - Relevance of a summary is the extent to which a summary contains the main ideas\\nof a source. We use ROUGE [70] and BERTScore [130] metrics to measure relevancy of the\\ncandidate summary.\\n• D2 - Informativeness is the amount of new information and knowledge a summary brings\\nto the reader [92]. This information may not necessarily be key to the narrative of the source\\nbut should add value to readers. For example, limitations of an academic article are not central\\nto the narrative but do add value to readers. This metric tests a model architecture’s ability\\nto effectively generate summary that can cover different aspects of the original source text.\\nThis is approximated by the percentage of sections that are covered by a candidate summary,\\nwhere we assume each sentence of the candidate summary covers a particular section and\\nthe sentence belongs to the section where it achieves the highest ROUGE-L score.\\n• D3 - Semantic Coherence measures whether a summary is fluent and semantically coher-\\nent. Following Bommasani and Cardie [4]’s implementation, this is approximated as:\\n𝑆𝐶(𝑆) =\\nÍ||𝑆||\\n𝑖=2 𝑁𝑆𝑃(𝑠𝑖|𝑠𝑖−1)\\n||𝑆||\\nwhere 𝑁𝑆𝑃(.) is BERT NSP function, and 𝑠𝑖denotes the position of a sentence in the candidate\\nsummary. However, Bommasani and Cardie [4] did not fine-tune the general pre-trained\\nBERT model while our BERT NSP model is fine-tuned on arXiv using positive and negative\\nsentence-pairs with a final F1-score of 0.92.\\n5.1.4\\nOther Implementation Details.\\nFor all the model variants implemented, the train, validation and test sample split on the arXiv\\nbenchmark dataset are 203,037/6,436/6,440, which is the same for all prior works as they follow\\nthe same configuration by the original author [20]. To ensure consistent preprocessing pipeline,\\nwe follow pre-processing of LoBART in all model implementations [77]. All models that require\\nfine-tuning are trained on the same RTX 3090 GPU with 24 GiB of GPU memory. We use pyrouge\\npackage for ROUGE metric.\\n5.2\\nResults and Analysis\\nThe following discusses the experimental findings based on the results shown in Table 3.\\n7https://github.com/google-research/pegasus\\n8https://github.com/google-research/bigbird\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 20}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:21\\nArchitecture and its Mechanisms\\nRelevance\\nInformativeness\\nSemantic Coherence\\nR-1\\nR-2\\nR-L\\nBERTScore\\nUnsupervised Extractive - Graph\\nTf-Idf\\n0.344\\n0.095\\n0.285\\n0.822\\n0.355\\n0.779\\nBERT\\n0.351\\n0.098\\n0.290\\n0.819\\n0.398\\n0.792\\nTf-Idf + Discourse Bias\\n0.357\\n0.113\\n0.311\\n0.820\\n0.447\\n0.677\\nBERT + Discourse Bias\\n0.361\\n0.112\\n0.322\\n0.822\\n0.513\\n0.667\\nSupervised Abstractive - BART Variants\\nEnd-to-End (Max Token)\\nBART-only (1,024)\\n0.413\\n0.153\\n0.368\\n0.846\\n0.331\\n0.851\\nBART+LongformerAttn (4,096)\\n0.463\\n0.187\\n0.412\\n0.852\\n0.361\\n0.855\\nBART+LongformerAttn (16,384)\\n0.467\\n0.196\\n0.418\\n0.865\\n0.433\\n0.877\\nRetrieve-then-summarize\\nBART+CS\\n0.472\\n0.193\\n0.419\\n0.837\\n0.403\\n0.812\\nBART+LocalAttn+CS\\n0.486\\n0.201\\n0.422\\n0.845\\n0.427\\n0.835\\nSupervised Abstractive - PEGASUS Variants\\nEnd-to-End (Max Token)\\nPEGASUS (1,024)\\n0.439\\n0.171\\n0.381\\n0.857\\n0.333\\n0.863\\nPEGASUS+BigBirdAttn (4,096)\\n0.462\\n0.190\\n0.415\\n0.855\\n0.359\\n0.869\\nTable 3. Experimental Results of Graph-based Unsupervised Extractive and Transformer-based Supervised\\nAbstractive. The best results are in boldface, and the second highest scores are underlined. Max token\\nrepresents the maximum input length where texts that exceed this cutoff point are truncated. LocalAttn\\nrepresents local attention where each token only attends to its neighbouring 1,024 tokens. LongformerAttn\\nand BigBirdAttn represents efficient attention variants proposed by Longformer [2] and BigBird [127].\\n5.2.1\\nGraph - Unsupervised Extractive.\\nFinding 1. Global Word Representation versus Contextual Embedding: Using BERT as\\nthe sentence encoder mechanism boosts the unsupervised summarization model performance in\\nthe relevancy and informativeness dimension. We hypothesize this is due to the semantic reasoning\\ncapability of BERT in encoding important sentences that are informative but are not worded in the\\nsame way as the other important sentences when similarity and centrality scoring are computed.\\nThis is particularly important for long document as it has higher compression ratio and a higher\\nchance of having sentences with the exact same information being repeated multiple times in the\\nsource text. Thus, not encoding the sentences with semantically rich encoding may result in a\\nsummary with more redundancy.\\nFinding 2. Discourse Bias Mechanism boosts Relevance and Informativeness: The inclu-\\nsion of positional and sectional bias when computing sentence centrality score greatly improves\\nthe architecture’s ability in capturing more relevant sentences in the long document and generating\\na more informative summary, validating our hypothesis in section 4. However, since extractive\\nmodels merely combine the extracted sentences, these sentences that come from various sections\\nlikely caused a drop in the semantic coherence of the final summary outputs.\\n5.2.2\\nTransformer - Supervised Abstractive.\\nFinding 1. Diminishing Return of PEGASUS Pre-training: As compared to the BART-only\\nmodel, the PEGASUS-only model achieved greater performance across all dimensions, indicating\\nthat PEGASUS pre-training mechanism helps a Transformer-based model in writing more relevant,\\ninformative and semantically coherent summaries. As PEGASUS is pre-trained on a different corpus\\nas compared to BART [66, 129], it is not conclusive whether the GSG pre-training task and/or\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 21}, page_content='1:22\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\nthe difference in the pre-training corpus contributed to the superior performance of PEGASUS.\\nInterestingly, the performance gain is not as obvious when the input sequence length is allowed to\\nbe extended to 4,096 with efficient attention. This could be due to the difference in the efficient\\nattention mechanism used or the need for predicting salient content outside the truncated text has\\ndiminished.\\nFinding 2. Mixed Results on Retrieve-then-summarize Models: For both retrieve-then-\\nsummarize BART models, we see state-of-the-art result achieved in terms of the standard ROUGE\\nscore metric. The result showed improvement across all dimensions when the Transformer model\\nprocessed a longer subset extracted by the retriever, demonstrating the effectiveness of Transformer\\nin computing pairwise relations between tokens to identify salient content. Except for the BART\\nwith longformer attention (16,384), retrieve-then-summarize models also performed better in\\ninformativeness as the models are allowed to process the entire source documents in arXiv dataset.\\nHowever, the retrieve-then-summarize models performed the worst in semantic coherence when\\ncompared to the other abstractive summarization models. As the content selection mechanism is\\nnot trained in an end-to-end manner, we hypothesize that this is due to the inevitable disconnect\\nbetween the content selection mechanism and the encoder-decoder Transformer model at the\\ninference stage. Further, it is possible when the retrieved subset extracted by the content selection\\nmechanism are not ordered in its original form, the incoherence of the subset cascade downwards\\nto the final summary output, causing a drop in semantic coherence. This finding also illustrates\\nthe importance of measuring model performance in a multi-dimensional way rather than relying\\nentirely on ROUGE score that has found to have important limitations [3, 61].\\nFinding 3. Transformer’s Reasoning Capability over Long Sequences: Holding the pre-\\ntrained BART model constant, extending the total input token limits for the pre-trained Transformer\\nimproves the summarizer’s ability in generating a summary that is more relevant, informative\\nand semantically coherent. This finding is consistent with a human evaluation experiment by\\n[49], providing confidence to the automatic evaluation metrics used in this work. The impact of\\nprocessing only 1,024 tokens is particularly obvious when it comes to the informativeness of the\\nsummary output where BART (1,024) informativeness score is 10 points lower than BART (16,384).\\nImportantly, ROUGE score again did not fully capture this performance difference, highlighting the\\nlimitation of traditional summarization research setting of measuring model performance using\\nonly ROUGE score. Lastly, this finding suggests that unlike the result of Meng et al. [82], our\\nexperiment demonstrates that Transformer can reason over long sequences given that the right\\nconfiguration is made to fine-tuned the model for specific downstream task.\\nThrough ad-hoc experiment, we systematically analyze the common approaches in long document\\nsummarization domain. The experimental result demonstrated that exploiting explicit discourse\\nstructures of long documents in unsupervised models and processing longer inputs with long\\ndocument adaptation on pre-trained Transformer models can yield promising outcomes for the\\nlong document summarization task. The result also showed that retrieve-then-summarize model can\\nachieve state-of-the-art results in terms of ROUGE score but may generate less coherent summaries.\\n5.3\\nLimitation of Experiment\\nRecent studies have found that summary outputs of state-of-the-art abstractive summarization\\nmodels contain factual inconsistency in up to 30% of summary output [8, 62, 81]. To address the\\naforementioned issues, various models and metrics have been proposed to measure the factual\\nconsistency of candidate summaries conditioned on the source documents [27, 40]. Nonetheless,\\ndue to the limitations of the proposed metrics including the input length limit of pre-trained\\nmodels, difficulty of implementation and performance variation across benchmarks [89], we did not\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 22}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:23\\nmeasure the factual consistency of summary outputs and represents an important limitation of the\\nmulti-dimensional analysis experiment above. This is despite after trying out various adaptations on\\nthe textual entailment approach proposed by Maynez et al. [81], our tested models have almost no\\ndiscriminative ability and was thus not used. The robustness of the metrics used across the relevance,\\ninformativeness and semantic coherence dimensions should also be interpreted with care. Lastly,\\nthe experiment was only conducted using the arXiv benchmark dataset as it is the only dataset\\nwhere all pre-trained weights are publicly available. To encourage similar analysis to be conducted\\nacross a wide range of benchmark dataset and model implementation, our evaluation metric toolkit\\nfor dataset and model is available at https://github.com/huankoh/long-doc-summarization.\\n6\\nMETRICS\\nAs evaluating generated summary outputs using manual efforts are costly and impractical, the\\nefficient ROUGE metric [70] has long been the standard way of comparing summarization model\\nperformance. It measures the lexical overlap between reference and candidate summary and the\\ncommon n-gram measures are unigram (ROUGE-1), bigram (ROUGE-2) and longest common\\nsub-sequence (ROUGE-L). However, as it is based on exact token matches and overlap between\\nsynonymous tokens or phrases will be ignored, the limitation of ROUGE score metrics have been\\nwidely explored [3, 11, 44, 61] and many have also attempted to propose more comprehensive\\ncontent overlap metrics using soft semantic overlap [33, 130]. Further, while content overlap is\\nthe fundamental objective of summarization, the quality of a summary, as Gehrmann et al. [36]\\nand Peyrard [92] suggested, should be measured in a multi-dimensional way including relevance,\\nfactual consistency, conciseness and semantic coherence. Relevance refers to whether the candidate\\nsummary contains the main ideas. Factual consistency metric measures whether a candidate\\nsummary is factually consistent with the source document. Conciseness measure whether important\\ninformation is encapsulated in a short and brief manner. Semantic coherence relates to the collective\\nquality and fluency of summary sentences. Based on these quality aspects, the following discusses\\nthe research efforts in the wider summarization domain with a focus on the long document\\nsummarization research settings at the end of this section.\\n6.1\\nRelevance\\nA) Hard Lexical Overlap\\nAs mentioned above, ROUGE score is an efficient way to consider content overlap through hard\\nlexical matching between candidate summary and the ground truth summary. However, as ROUGE\\nonly considers exact matching between reference summary and model output, it (a) will penalize\\nmodels that coin novel wordings and phrases that do not match the wordings in the reference\\nsummary, (b) does not consider factual consistency between the model output and the source\\ndocument and (c) does not directly consider fluency and conciseness of a summary. Finally, ROUGE\\nscore also goes against the human approach of clever paraphrasing and summarizing.\\nB) Soft Content Overlap\\nTo solve the problem of exact matching of lexical units, Zhang et al. [130] proposes a model that\\nmeasures soft overlap between the reference and candidate summary by comparing the contextual\\nBERT embeddings of both summaries. Other variants of this idea include MoverScore [133], Word\\nMover Similarity and an extension of it, Sentence Mover Similarity [18, 65]. The soft content\\noverlap metrics often rely substantially on the encoder used to vectorized the candidate and ground\\ntruth summary. BERTScore, for example, utilizes BERT as the fundamental pre-trained model to\\nencode its representations. While BERT has been proven to perform amazingly well under many\\ndifferent benchmark settings, its performance under certain domains such as legal or scientific\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 23}, page_content='1:24\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\nresearch has not been thoroughly explored. For example, Beltagy et al. [1] fine-tuned BERT on large-\\nscale scientific paper datasets and have found its performance to improve in scientific domains as\\ncompared to the BERT-base model. Evidenced by Tejaswin et al. [111]’s experimental result where\\nBERTScore is found not to discriminate summaries with and without errors well, this questions the\\nuse of BERT-base model as the \"independent evaluator\" of candidate summaries across all domains.\\nC) Reference-free Approach\\nRather than measuring the quality of candidate summary based on a ground truth summary,\\nreference-free metrics for relevance measure the quality of candidate summary based on pseudo-\\nreference summaries that are generated from source documents. Wu et al. [117]’s proposed metric\\nrequires training samples of high-quality summaries for model supervision while other reference-\\nfree metrics can generate metric scores without the use of high-quality summaries as supervisory\\nsignals [14, 35]. In section 3 of benchmark datasets, we see that the information covered by a\\nreference summary depends on the data annotation approach as well as the intent of the original\\nauthors. We further observe that reference summaries of certain benchmark datasets contain\\nsignificant noises. If supervised summarization models were to train on datasets with similar issues,\\nthey may fit on target summaries that are inconsistent with the expectation and needs of summary\\nreaders. A reference-free approach that can bypass the requirement of ground truth summaries\\nwould thus be beneficial to the development of models in cases where there is high heterogeneity\\nin summary reader expectation and/or lack of ground truth summary labels. Nevertheless, the use\\ncase of reference-free metrics is often limited by the fact that they still require pseudo-reference\\nsummaries to be generated by an \"independent model\". Last but not least, the reference-free\\napproach can also be used to augment the reference-based metrics [46].\\n6.2\\nFactual Consistency\\nWidespread factual inconsistency in abstractive summarization model outputs greatly limit the\\npotentiality of these abstractive models to be applied in most commercial settings. To this end,\\nautomated metrics on factual consistency have been proposed by others [27, 62, 81, 114], which can\\nbe categorized into two different approaches: Entailment Classification and Question Answering.\\nA) Entailment Classification Approach\\nThe entailment classification approach evaluates the factual inconsistency of a candidate summary\\nby breaking down the summary into smaller units (e.g., phrases/sentences) to be verified against\\nthe original document. For example, FactCC [62] implements a BERT-based factual consistency\\nclassifier that is trained on synthetic data, where the positive data labels are non-paraphrased and\\nparaphrased sentences from the original source document, and the negative labels are artificially\\ncorrupted sentences from the source document. At the inference stage, the faithfulness score\\nfor a candidate summary is the number of consistent sentences divided by the total number of\\nsummary sentences. Similarly, other proposed models implement factual consistency classifiers by\\nincorporating structured knowledge such as OpenIE triples [39] or dependency arc [40]. For the\\nclassifiers to be effective in discriminating the factual consistency of a candidate summary, they\\noften require supervisory signals from factually consistent and inconsistent data [89].\\nB) Question-Answering Approach\\nThe Question-Answering (QA) approach employs a question-generation model to generate questions\\nfrom a given summary output [27, 114]. The generated questions are then answered in two different\\nways: i) answering the question conditioning on the source text and ii) answering the question\\nconditioning on the summary output. If the answers match between the source text and candidate\\nsummary, the answer is then considered consistent, otherwise, it is inconsistent. The final score will\\nbe based on discrepancies between the answers generated conditional on the candidate summary\\nand the answers generated conditional on the souce document. Recently, QAGen [85] proposes to\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 24}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:25\\ngenerate questions and answers from a given text concurrently within a single model to evaluate\\nfactual consistency to improve the efficiency of this approach.\\nOther Important Studies:\\nIt is important to note that the aforementioned works consider factual consistency as a binary\\noutcome. In contrast, FRANK [89] advocates for a multi-dimensional approach to evaluate factual\\nconsistency based on semantic error, discourse error and content verifiability error. Through\\nsubstantial human annotation, the study further found that the effectiveness of metrics is found to\\nbe extremely dependent on the types of architecture measured and the benchmark dataset used.\\nSimilarly, human evaluation experiments from previous works have shown conflicting and varying\\nresults in the desired approach of developing factuality metrics [81, 85]. In response, Gabriel et al.\\n[31] proposed five conditions for the development of an effective factuality metric to encourage\\nbetter standardization in the factual consistency metric research.\\n6.3\\nConciseness and Semantic Coherence\\nMetrics to measure other aspects of summarization such as conciseness and semantic coherence\\nwere also introduced. As they are not as crucial as relevance and factual consistency, these metrics\\noften complement the others to allow a metric or a model to be more holistic and practical. For\\nexample, Bommasani and Cardie [4] considers semantic coherence of reference summaries when\\nevaluating single document benchmark datasets while Ju et al. [55]’s unsupervised model generates\\nfluent summary by utilizing the next sentence prediction task in BERT. Metrics for conciseness are\\nalso introduced to measure the quality of summaries [4, 14].\\n6.4\\nResearch Efforts on Metrics in the Long Document Domain\\nMany recently proposed metrics incorporate pre-trained architectures to achieve better perfor-\\nmances. However, as argued in the model discussion above, these pre-trained architectures cannot\\nbe easily extended to long documents. As an illustration, our experiment has attempted various\\nadaptations9 on a BERT textual entailment model to evaluate arXiv candidate summaries but has\\nfound it not effective in discriminating a summary’s factual consistency with the source. This is\\ndespite Maynez et al. [81]’s finding that this model best correlates with the human judgment of\\nfactual consistency on the XSUM short document dataset. Furthermore, other than the difficulty of\\nadapting these models on long documents, Nan et al. [85] has also identified the issue of resource\\nefficiency, where a competing model would take approximately 4 days to evaluate a CNN-DM\\ntest set with an NVIDIA V100 Tensor Core GPU and would likely take significantly longer under\\nany long document benchmark datasets. Consequently, the need of re-designing the proposed\\nevaluation models and the requirement for costly computation resources have likely discouraged\\nthe adoption of factual consistency assessment models in the long document summarization domain.\\nLooking at the broader research on evaluation metrics of summarization as a whole, for 17 different\\nresearch papers related to evaluation metrics published in ACL main conferences10 from 2015 to\\nSeptember 2021, there were no discussion on the evaluation metrics in the context of long document\\nsummarization datasets. This is important as Pagnoni et al. [89] has found that the effectiveness of\\nproposed metrics to vary based on the dataset characteristics. In sum, unlike the quick adoption\\nof short document practices in the model architectures space, research in exploring evaluation\\nmetrics within the context of long document summarization is lacking and may potentially hold\\nback the future progression of long document summarization.\\n9Result details are in the Supplementary Materials.\\n10ACL main conferences are ACL, NAACL, EACL, EMNLP, CoNLL, and AACL. Papers are listed in the Supplementary\\nMaterials.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 25}, page_content='1:26\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\n7\\nAPPLICATIONS\\nAs the quality of long document summaries generated by state-of-the-art models continues to\\nimprove, past works have explored their feasibility in the research and industrial domains. A natural\\nextension for models that were implemented on the scientific paper benchmark, arXiv/PubMed, is to\\nemploy it for research purposes. These include writing section-structured [82], user-specific [45] or\\npresentation-based [107] summaries for scientific papers, automating scientific reviewing [126], and\\neven generating literature survey based on multiple biomedical long scientific papers [24]. When\\nit comes to the general industrial applications of long document summarization, the knowledge\\nand techniques learned from the research domain can address numerous commercial tasks. On the\\nsurface level, any information that would be expressed in a textual format would benefit from the\\nadvancement in this field, which encompasses summarizing any forms of long textual documents\\n[75, 104], extracting content as feature snippets for search engines11, writing reviews for long media\\ncontent [63] and summarizing long dialogues [16, 71, 136, 138] and multi-modal content [125].\\nWith the development becomes increasingly mature in the real-world settings, summarization\\nmodels are now commercialized as a Software-as-a-Service (Saas) product in the news12, business13\\nand consulting14 domains. Furthermore, as the long document summarization task can be generally\\nunderstood as identification of important aspects from long sequences, the positive spillover from\\nsuccessful model implementation in this domain can affect a wide range of domains. Long document\\nsummarization models, for example, can be utilized for auxiliary tasks such as video captioning [72],\\nlong document question-answering [76] or multi-modal tasks [68, 86]. Liu et al. [73] also identified\\nthe \"unexpected side-effect\" of language model reliably learned how to transliterate names between\\nlanguages, despite the fact that the model was trained to summarize long Wikipedia articles, while\\nBigBird [127] applies Transformer-based models designed for long sequences not only to long\\ndocument summarization but also to DNA promoter region and chromatin profile prediction tasks\\nin the genomics research domain.\\n8\\nGENERAL CHALLENGES AND FUTURE DIRECTIONS\\nThis section discusses the general challenges of long document summarization that have yet to\\nbe solved and pinpoints potential future research directions to attract practitioners’ attention\\nand improve our understanding and techniques in the long document summarization domain.\\nAdvancement in the long document summarization domain should also give rise to beneficial\\nspillover to closely-related NLP sub-domains such as multi-hop QA, information retrieval and\\nreading comprehension.\\n8.1\\nNeural Models and Long Sequence Reasoning\\nWhile there have been significant efforts in solving the time and memory complexity of a neural\\narchitecture such as Transformers to enhance model efficiencies, the understanding of a model’s\\neffectiveness in solving different NLP tasks or domains is limited. As shown by our model experiment\\nand result findings of others [2, 49, 127], fine-tuning pre-trained models using efficient Transformers\\nthat can attend to larger input size of tokens can improve the model performances across a wide\\nrange of NLP tasks. However, the underlying reasons of the performance improvement is not well\\nunderstood. For example, while Transformer models have found to outperform RNN models as\\nRNN lacks the ability to reason over long sequences, Pagnoni et al. [89] have found that pre-trained\\n11https://developers.google.com/search/docs/advanced/appearance/featured-snippets\\n12https://ai.baidu.com/tech/nlp_apply/news_summary\\n13https://quillbot.com/\\n14https://www.datagrand.com/about-us/\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 26}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:27\\nTransformer summarization models still make a similar amount of discourse-related errors as the\\nRNN models. Furthermore, research in the effectiveness of various efficient attention mechanisms\\nused by a Transformer to summarize long documents also showed varying results. On the one\\nhand, Huang et al. [49] showed that efficient attention with learnable patterns to significantly\\noutperform the the efficient attention with fixed patterns such as local-only attention mechanism.\\nOn the other hand, Manakul and Gales [77] have found that extending window size of efficient\\nTransformers to increase number of attended tokens per token do not affect the average distance of\\nattended neighbor, suggesting that local attention to neighboring tokens will be sufficient for the\\nlong document summarization task. Altogether, these results highlight the limited understanding\\non the strategies employed by current neural models to summarize long document and the need of\\nfurther research to enhance our understanding on this issue.\\n8.2\\nSummarizer with Automatic Discourse Parsers/Annotator\\nOur experimental result has demonstrated that the simple unsupervised graph architecture outper-\\nforms the other unsupervised models when discourse bias of arXiv section information is included.\\nNonetheless, information regarding sections of a document may not always be of high quality\\nor available for a summarization model. This limits the implementation of many long doument\\nsummarization models that require explicit section-based discourse information. Similar issue has\\nbeen faced by researchers in the dialogue summarization domain where discourse level information\\nis not provided and past work in this domain have achieved state-of-the-art results by incorpo-\\nrating effective automatic discourse annotators [30, 71, 116]. An architecture that can effectively\\nincorporate automated discourse parsers or annotators would thus be a fruitful direction for long\\ndocument summarization researchers to explore.\\n8.3\\nEnd-to-end Neural Summarizer with Content Selection Mechanism\\nIn the medium term, in spite of the expected progress in computing efficiencies, there exist a\\nsignificant amount of long documents such as business reports and books that have tokens that\\nexceed hundreds of thousand [63, 75]. Thus, it is not possible to summarize the entire document\\nusing a powerful state-of-the-art model without any long document adaptation, as it will truncates\\nmost of the long document source text given the current input length limit. A more practical\\ndirection is to explore architectures with a content selection mechanism that has shown to be\\neffective in long document summarization [77, 134]. Zhao et al. [134] has proposed an end-to-end\\nlong document summarization framework using transformers but did not incorporate powerful\\npre-trained models and performed slightly worse than other state-of-the-art models. LoBART [77],\\non the other hand, did not design the content selection and abstractive summarizer in an end-to-end\\nmanner. Experimental result in this survey has shown that LoBART’s disconnection between the\\nretriever and summarizer resulted in less semantically coherent summaries. In the open-domain\\nQA domain, RAG [67] achieves state-of-the-art by successfully incorporating content selection\\nmechanism and pre-trained models in an end-to-end manner [91], pointing a promising direction\\nfor practitioners in the long document summarization domain to explore.\\n8.4\\nQuality and Diversity of Benchmark Dataset\\nIn section 3 of benchmark datasets, human annotation efforts have been done to measure the\\nquality of the most commonly used long document summarization dataset, arXiv. It was found\\nthat 60% of reference summaries contain some form of errors and 15% of them have significant\\nerrors where at least half of the summary contains errors. This calls for a benchmark dataset with\\nsignificantly better quality with fewer errors through robust heuristic rules and scraping strategies.\\nMoreover, the long document summarization benchmark datasets are often in the legislative and\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 27}, page_content='1:28\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\nscientific domain. While these domains are extremely important, many other domains such as\\nfinancial reports with significant numerical complexity or long-form dialogues of daily conversation\\nin business settings are equally important. Development across different domains could attract\\neven greater attention from a wide range of partners to incentivize greater research efforts in the\\nsummarization field. Last but not least, to achieve the original objectives of benchmark datasets,\\nproposed model architectures for long document summarization should also be tested across a\\ndiverse set of long document benchmark datasets rather than focusing merely on arXiv/PubMed.\\n8.5\\nPracticality of Summarization Metrics\\nThe limitation of ROUGE metric that has been widely explored [33, 61, 88, 137] and significant efforts\\nhave been made to improve the way we measure candidate summaries from various different aspects.\\nNonetheless, the proposed methods lack practicality in terms of wide availability for all parties\\nin the research communities. For example, Nan et al. [85] found that using a single NVIDIA V100\\nTensor Core GPU, a factual consistency metric proposed requires longer than four days to evaluate a\\nsingle set of candidate summaries in the CNN-DM test dataset. Many metrics proposed also require\\nsubstantial computing resources to re-train across different benchmark settings [27, 62, 114]. These\\nissues will be exacerbated when it comes to the long document summarization domain. Moreover,\\nmost summarization metrics are only tested in the CNN-DM and XSum datasets but not others.\\nThis significantly limits its applicability as Pagnoni et al. [89] have found most metrics to lack\\nrobustness across different benchmark settings. To ensure effective metrics have wider application,\\nefficiencies and practicality of metrics should be paid with great attention to ensure that sufficient\\nincentive is provided for practitioners to explore the practicality of metrics rather than a mere\\nfocus on state-of-the-art metric performances.\\n9\\nCONCLUSION\\nIn this survey, we conduct a comprehensive overview of long document summarization and\\nsystematically analyze the three key components of its research settings: benchmark datasets,\\nsummarization models and evaluation metrics. We first highlight the intrinsic differences of short\\nand long document datasets and show that summarizing long documents requires extra compression\\nof the source text through the identification of key narratives that are more uniformly scattered\\nacross the source documents. Nevertheless, long documents are often more extractive in nature and\\noften have explicit discourse structures to take advantage of. For summarization models, we provide\\na thorough review, comparison and summarization of the model architectures and mechanisms\\nused to generate long document summaries. Through ad-hoc experiment, we also systematically\\ninvestigate the architectures and mechanisms that are widely applied across various works. We\\nfurther discuss the current research in evaluation metrics and call attention to the lack of research\\non metrics that can be easily applied to the long document summarization domain. Finally, we\\nexplore the applications of long document summarization models and suggest five future directions\\nfor long document summarization research.\\nREFERENCES\\n[1] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language Model for Scientific Text. In Proceedings\\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference\\non Natural Language Processing (EMNLP-IJCNLP). 3615–3620.\\n[2] Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint\\narXiv:2004.05150 (2020).\\n[3] Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Re-evaluating evaluation in\\ntext summarization. arXiv preprint arXiv:2010.07100 (2020).\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 28}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:29\\n[4] Rishi Bommasani and Claire Cardie. 2020. Intrinsic Evaluation of Summarization Datasets. In Proceedings of the 2020\\nConference on Empirical Methods in Natural Language Processing (EMNLP). 8075–8096.\\n[5] Ravali Boorugu and G Ramesh. 2020. A Survey on NLP based Text Summarization for Summarizing Product Reviews.\\nIn 2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA). IEEE, 352–356.\\n[6] Florian Boudin and Emmanuel Morin. 2013. Keyphrase Extraction for N-best Reranking in Multi-Sentence Com-\\npression. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies. Association for Computational Linguistics, Atlanta, Georgia, 298–305.\\n[7] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint\\narXiv:2005.14165 (2020).\\n[8] Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive\\nsummarization. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.\\n[9] Jean Carletta, Simone Ashby, Sebastien Bourban, Mike Flynn, Mael Guillemot, Thomas Hain, Jaroslav Kadlec, Vasilis\\nKaraiskos, Wessel Kraaij, Melissa Kronenthal, et al. 2005. The AMI meeting corpus: A pre-announcement. In\\nInternational workshop on machine learning for multimodal interaction. Springer, 28–39.\\n[10] Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and Yejin Choi. 2018. Deep Communicating Agents for Abstractive\\nSummarization. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1 (Long Papers). 1662–1675.\\n[11] Arun Chaganty, Stephen Mussmann, and Percy Liang. 2018. The price of debiasing automatic metrics in natural\\nlanguage evalaution. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume\\n1: Long Papers). Association for Computational Linguistics, Melbourne, Australia, 643–653.\\n[12] Yllias Chali, Sadid A Hasan, and Shafiq R Joty. 2009. A SVM-based ensemble approach to multi-document summariza-\\ntion. In Canadian Conference on Artificial Intelligence. Springer, 199–202.\\n[13] Danqi Chen, Jason Bolton, and Christopher D Manning. 2016. A Thorough Examination of the CNN/Daily Mail\\nReading Comprehension Task. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers). 2358–2367.\\n[14] Wang Chen, Piji Li, and Irwin King. 2021. A Training-free and Reference-free Summarization Evaluation Metric via\\nCentrality-weighted Relevance and Self-referenced Redundancy. In Proceedings of the 59th Annual Meeting of the\\nAssociation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing\\n(Volume 1: Long Papers). Association for Computational Linguistics, Online, 404–414.\\n[15] Jianpeng Cheng and Mirella Lapata. 2016. Neural summarization by extracting sentences and words. arXiv preprint\\narXiv:1603.07252 (2016).\\n[16] Bharath Chintagunta, Namit Katariya, Xavier Amatriain, and Anitha Kannan. 2021. Medically Aware GPT-3 as a\\nData Generator for Medical Dialogue Summarization. In Proceedings of the Second Workshop on Natural Language\\nProcessing for Medical Conversations. 66–76.\\n[17] Janara Christensen, Stephen Soderland, Oren Etzioni, et al. 2013. Towards coherent multi-document summarization.\\nIn Proceedings of the 2013 conference of the North American chapter of the association for computational linguistics:\\nHuman language technologies. 1163–1173.\\n[18] Elizabeth Clark, Asli Celikyilmaz, and Noah A Smith. 2019. Sentence mover’s similarity: Automatic evaluation\\nfor multi-sentence texts. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.\\n2748–2760.\\n[19] Ann Clifton, Sravana Reddy, Yongze Yu, Aasish Pappu, Rezvaneh Rezapour, Hamed Bonab, Maria Eskevich, Gareth\\nJones, Jussi Karlgren, Ben Carterette, and Rosie Jones. 2020. 100,000 Podcasts: A Spoken English Document Cor-\\npus. In Proceedings of the 28th International Conference on Computational Linguistics. International Committee on\\nComputational Linguistics, Barcelona, Spain (Online), 5903–5917.\\n[20] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian.\\n2018. A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents. In Proceedings of the\\n2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 2 (Short Papers). 615–621.\\n[21] Peng Cui and Le Hu. 2021. Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long\\nDocuments. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies. 5881–5891.\\n[22] Peng Cui, Le Hu, and Yuanchao Liu. 2020. Enhancing Extractive Text Summarization with Topic-Aware Graph Neural\\nNetworks. In Proceedings of the 28th International Conference on Computational Linguistics. 5360–5371.\\n[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\\nTransformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 29}, page_content='1:30\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\n4171–4186.\\n[24] Jay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, and Lucy Wang. 2021. MSˆ2: Multi-Document\\nSummarization of Medical Studies. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\\nProcessing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 7494–7513.\\n[25] Yue Dong, Andrei Mircea Romascanu, and Jackie Chi Kit Cheung. 2021. Discourse-Aware Unsupervised Summarization\\nfor Long Scientific Documents. In Proceedings of the 16th Conference of the European Chapter of the Association for\\nComputational Linguistics: Main Volume. 1089–1102.\\n[26] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. 2021. GSum: A General Framework\\nfor Guided Neural Abstractive Summarization. In Proceedings of the 2021 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Human Language Technologies. 4830–4842.\\n[27] Esin Durmus, He He, and Mona Diab. 2020. FEQA: A Question Answering Evaluation Framework for Faithfulness As-\\nsessment in Abstractive Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational\\nLinguistics. 5055–5070.\\n[28] Wafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea, and Hoda K Mohamed. 2021. Automatic text summarization: A\\ncomprehensive survey. Expert Systems with Applications 165 (2021), 113679.\\n[29] Günes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization.\\nJournal of artificial intelligence research 22 (2004), 457–479.\\n[30] Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, and Ting Liu. 2021. Language Model as an Annotator: Exploring\\nDialoGPT for Dialogue Summarization. In Proceedings of the 59th Annual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).\\nAssociation for Computational Linguistics, Online, 1479–1491.\\n[31] Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, and Jianfeng Gao. 2021. GO FIGURE: A Meta Evaluation of\\nFactuality in Summarization. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association\\nfor Computational Linguistics, Online, 478–487.\\n[32] Mahak Gambhir and Vishal Gupta. 2017. Recent automatic text summarization techniques: a survey. Artificial\\nIntelligence Review 47, 1 (2017), 1–66.\\n[33] Kavita Ganesan. 2018. ROUGE 2.0: Updated and Improved Measures for Evaluation of Summarization Tasks. arXiv\\npreprint arXiv:1803.01937 (2018).\\n[34] Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making Pre-trained Language Models Better Few-shot Learners.\\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics,\\nOnline, 3816–3830.\\n[35] Yang Gao, Wei Zhao, and Steffen Eger. 2020. SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics\\nfor Multi-Document Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational\\nLinguistics. Association for Computational Linguistics, Online, 1347–1354.\\n[36] Sebastian Gehrmann, Yuntian Deng, and Alexander M Rush. 2018. Bottom-Up Abstractive Summarization. In\\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 4098–4109.\\n[37] Alexios Gidiotis and Grigorios Tsoumakas. 2020. A divide-and-conquer approach to the summarization of long\\ndocuments. IEEE/ACM Transactions on Audio, Speech, and Language Processing 28 (2020), 3029–3040.\\n[38] Yihong Gong and Xin Liu. 2001. Generic text summarization using relevance measure and latent semantic analysis.\\nIn Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information\\nretrieval. 19–25.\\n[39] Ben Goodrich, Vinay Rao, Peter J Liu, and Mohammad Saleh. 2019. Assessing the factual accuracy of generated text.\\nIn Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 166–175.\\n[40] Tanya Goyal and Greg Durrett. 2020. Evaluating Factuality in Generation with Dependency-level Entailment. In\\nFindings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics,\\nOnline, 3592–3603.\\n[41] Yvette Graham. 2015. Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE. In Proceedings\\nof the 2015 conference on empirical methods in natural language processing. 128–137.\\n[42] Matt Grenander, Yue Dong, Jackie Chi Kit Cheung, and Annie Louis. 2019. Countering the Effects of Lead Bias in\\nNews Summarization via Multi-Stage Training and Auxiliary Losses. In Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP). 6019–6024.\\n[43] Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse\\nextractive strategies. arXiv preprint arXiv:1804.11283 (2018).\\n[44] Tatsunori B. Hashimoto, Hugh Zhang, and Percy Liang. 2019. Unifying Human and Statistical Evaluation for\\nNatural Language Generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 30}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:31\\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for\\nComputational Linguistics, Minneapolis, Minnesota, 1689–1701.\\n[45] Junxian He, Wojciech Kryściński, Bryan McCann, Nazneen Rajani, and Caiming Xiong. 2020. Ctrlsum: Towards\\ngeneric controllable text summarization. arXiv preprint arXiv:2012.04281 (2020).\\n[46] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: A Reference-free\\nEvaluation Metric for Image Captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural\\nLanguage Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic,\\n7514–7528.\\n[47] Wan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui Min, Jing Tang, and Min Sun. 2018. A Unified Model for\\nExtractive and Abstractive Summarization using Inconsistency Loss. In Proceedings of the 56th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers). 132–141.\\n[48] Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, and Yue Zhang. 2020. What Have We\\nAchieved on Text Summarization?. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\\nProcessing (EMNLP). 446–469.\\n[49] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient Attentions for Long Document\\nSummarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies. 1419–1436.\\n[50] Kokil Jaidka, Muthu Kumar Chandrasekaran, Sajal Rustagi, and Min-Yen Kan. 2016. Overview of the CL-SciSumm\\n2016 Shared Task. In Proceedings of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural\\nLanguage Processing for Digital Libraries (BIRNDL). 93–102.\\n[51] Adam Janin, Don Baron, Jane Edwards, Dan Ellis, David Gelbart, Nelson Morgan, Barbara Peskin, Thilo Pfau, Elizabeth\\nShriberg, Andreas Stolcke, et al. 2003. The ICSI meeting corpus. In 2003 IEEE International Conference on Acoustics,\\nSpeech, and Signal Processing, 2003. Proceedings.(ICASSP’03)., Vol. 1. IEEE, I–I.\\n[52] Yangfeng Ji and Jacob Eisenstein. 2014. Representation learning for text-level discourse parsing. In Proceedings of the\\n52nd annual meeting of the association for computational linguistics (volume 1: Long papers). 13–24.\\n[53] Karen Sparck Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of\\ndocumentation (1972).\\n[54] Jiaxin Ju, Ming Liu, Longxiang Gao, and Shirui Pan. 2020. Monash-Summ@ LongSumm 20 SciSummPip: An\\nUnsupervised Scientific Paper Summarization Pipeline. In Proceedings of the First Workshop on Scholarly Document\\nProcessing. 318–327.\\n[55] Jiaxin Ju, Ming Liu, Huan Yee Koh, Yuan Jin, Lan Du, and Shirui Pan. 2021. Leveraging Information Bottleneck\\nfor Scientific Document Summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021.\\n4091–4098.\\n[56] Byeongchang Kim, Hyunwoo Kim, and Gunhee Kim. 2019. Abstractive Summarization of Reddit Posts with Multi-\\nlevel Memory Networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2519–2531.\\n[57] Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to\\nsentence compression. Artificial Intelligence 139, 1 (2002), 91–107.\\n[58] Hayato Kobayashi, Masaki Noguchi, and Taichi Yatsuka. 2015. Summarization based on embedding distributions. In\\nProceedings of the 2015 conference on empirical methods in natural language processing. 1984–1989.\\n[59] Anastassia Kornilova and Vlad Eidelman. 2019. BillSum: A Corpus for Automatic Summarization of US Legislation.\\nEMNLP-IJCNLP 2019 (2019), 48.\\n[60] Mahnaz Koupaee and William Yang Wang. 2018. WikiHow: A Large Scale Text Summarization Dataset. ArXiv\\nabs/1810.09305 (2018).\\n[61] Wojciech Kryściński, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural Text\\nSummarization: A Critical Evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 540–551.\\n[62] Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the Factual Consistency\\nof Abstractive Text Summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\\nProcessing (EMNLP). Association for Computational Linguistics, Online, 9332–9346.\\n[63] Wojciech Kryściński, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2021. BookSum: A\\nCollection of Datasets for Long-form Narrative Summarization. arXiv preprint arXiv:2105.08209 (2021).\\n[64] Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of the 18th\\nannual international ACM SIGIR conference on Research and development in information retrieval. 68–73.\\n[65] Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word embeddings to document distances.\\nIn International conference on machine learning. PMLR, 957–966.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 31}, page_content='1:32\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\n[66] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov,\\nand Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,\\nTranslation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational\\nLinguistics. 7871–7880.\\n[67] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler,\\nMike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp\\ntasks. In NeurIPS.\\n[68] Jiazheng Li, Linyi Yang, Barry Smyth, and Ruihai Dong. 2020. MAEC: A multimodal aligned earnings conference\\ncall dataset for financial risk prediction. In Proceedings of the 29th ACM International Conference on Information &\\nKnowledge Management. 3063–3070.\\n[69] Xinnian Liang, Shuangzhi Wu, Mu Li, and Zhoujun Li. 2021. Improving unsupervised extractive summarization with\\nfacet-aware modeling. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 1685–1697.\\n[70] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out.\\n74–81.\\n[71] Chunyi Liu, Peng Wang, Jiang Xu, Zang Li, and Jieping Ye. 2019. Automatic dialogue summary generation for\\ncustomer service. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data\\nMining. 1957–1965.\\n[72] Hui Liu and Xiaojun Wan. 2021. Video Paragraph Captioning as a Text Summarization Task. In Proceedings of the\\n59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on\\nNatural Language Processing (Volume 2: Short Papers). 55–60.\\n[73] Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018.\\nGenerating Wikipedia by Summarizing Long Sequences. In International Conference on Learning Representations.\\n[74] Yang Liu and Mirella Lapata. 2019. Text Summarization with Pretrained Encoders. In Proceedings of the 2019 Conference\\non Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language\\nProcessing (EMNLP-IJCNLP). 3730–3740.\\n[75] Lefteris Loukas, Manos Fergadiotis, Ion Androutsopoulos, and Prodromos Malakasiotis. 2021. EDGAR-CORPUS:\\nBillions of Tokens Make The World Go Round. arXiv:2109.14394 [cs.CL]\\n[76] Chenyang Lyu, Lifeng Shang, Yvette Graham, Jennifer Foster, Xin Jiang, and Qun Liu. 2021. Improving Unsupervised\\nQuestion Answering via Summarization-Informed Question Generation. In Proceedings of the 2021 Conference on\\nEmpirical Methods in Natural Language Processing. 4134–4148.\\n[77] Potsawee Manakul and Mark Gales. 2021. Long-span summarization via local attention and content selection. In\\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint\\nConference on Natural Language Processing (Volume 1: Long Papers). 6026–6041.\\n[78] Inderjeet Mani and Eric Bloedorn. 1998. Machine learning of generic and user-focused summarization. In AAAI/IAAI.\\n821–826.\\n[79] William C Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text\\norganization. Text-interdisciplinary Journal for the Study of Discourse 8, 3 (1988), 243–281.\\n[80] Yuning Mao, Liyuan Liu, Qi Zhu, Xiang Ren, and Jiawei Han. 2020. Facet-Aware Evaluation for Extractive Summa-\\nrization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 4941–4957.\\n[81] Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On Faithfulness and Factuality in\\nAbstractive Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\\n1906–1919.\\n[82] Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, and Daqing He. 2021. Bringing\\nStructure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents. In Proceedings of the 59th\\nAnnual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural\\nLanguage Processing (Volume 2: Short Papers). Association for Computational Linguistics, Online, 1080–1089.\\n[83] Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into text. In Proceedings of the 2004 conference on\\nempirical methods in natural language processing. 404–411.\\n[84] Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Ça glar Gulçehre, and Bing Xiang. 2016. Abstractive Text\\nSummarization using Sequence-to-sequence RNNs and Beyond. In Proceedings of The 20th SIGNLL Conference on\\nComputational Natural Language Learning. 280–290.\\n[85] Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen McKeown, Ramesh Nallapati, Dejiao\\nZhang, Zhiguo Wang, Andrew O. Arnold, and Bing Xiang. 2021. Improving Factual Consistency of Abstractive\\nSummarization via Question Answering. In Proceedings of the 2021 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics,\\nOnline.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 32}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:33\\n[86] Medhini Narasimhan, Anna Rohrbach, and Trevor Darrell. 2021. CLIP-It! language-guided video summarization. In\\nThirty-Fifth Conference on Neural Information Processing Systems.\\n[87] Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018. Don’t Give Me the Details, Just the Summary! Topic-Aware\\nConvolutional Neural Networks for Extreme Summarization. In Proceedings of the 2018 Conference on Empirical\\nMethods in Natural Language Processing. 1797–1807.\\n[88] Jun Ping Ng and Viktoria Abrecht. 2015. Better Summarization Evaluation with Word Embeddings for ROUGE. In\\nProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 1925–1930.\\n[89] Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding Factuality in Abstractive\\nSummarization with FRANK: A Benchmark for Factuality Metrics. In Proceedings of the 2021 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for\\nComputational Linguistics, Online, 4812–4829.\\n[90] Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A Deep Reinforced Model for Abstractive Summarization.\\nIn International Conference on Learning Representations.\\n[91] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine\\nJernite, Vladimir Karpukhin, Jean Maillard, et al. 2021. KILT: a Benchmark for Knowledge Intensive Language Tasks.\\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies. 2523–2544.\\n[92] Maxime Peyrard. 2019. A Simple Theoretical Model of Importance for Summarization. In Proceedings of the 57th\\nAnnual Meeting of the Association for Computational Linguistics. 1059–1073.\\n[93] Maxime Peyrard. 2019. Studying summarization evaluation metrics in the appropriate scoring range. In Proceedings\\nof the 57th Annual Meeting of the Association for Computational Linguistics. 5093–5100.\\n[94] Jonathan Pilault, Raymond Li, Sandeep Subramanian, and Christopher Pal. 2020. On extractive and abstractive neural\\ndocument summarization with transformer language models. In Proceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP). 9308–9319.\\n[95] Vahed Qazvinian and Dragomir Radev. 2008. Scientific Paper Summarization Using Citation Summary Networks. In\\nProceedings of the 22nd International Conference on Computational Linguistics (Coling 2008). 689–696.\\n[96] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\\nPeter J Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of\\nMachine Learning Research 21 (2020), 1–67.\\n[97] Revanth Rameshkumar and Peter Bailey. 2020. Storytelling with Dialogue: A Critical Role Dungeons and Dragons\\nDataset. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for\\nComputational Linguistics, Online, 5121–5134.\\n[98] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In\\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint\\nConference on Natural Language Processing (EMNLP-IJCNLP). 3982–3992.\\n[99] Tobias Rohde, Xiaoxia Wu, and Yinhan Liu. 2021. Hierarchical learning for generation with long source sequences.\\narXiv preprint arXiv:2104.07545 (2021).\\n[100] Sascha Rothe, Joshua Maynez, and Shashi Narayan. 2021. A Thorough Evaluation of Task-Specific Pretraining for\\nSummarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association\\nfor Computational Linguistics, Online and Punta Cana, Dominican Republic, 140–145.\\n[101] Alexander M Rush, Sumit Chopra, and Jason Weston. 2015. A Neural Attention Model for Abstractive Sentence\\nSummarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 379–389.\\n[102] Evan Sandhaus. 2008. The new york times annotated corpus. Linguistic Data Consortium, Philadelphia 6, 12 (2008),\\ne26752.\\n[103] Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get To The Point: Summarization with Pointer-Generator\\nNetworks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers). 1073–1083.\\n[104] Eva Sharma, Chen Li, and Lu Wang. 2019. BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent\\nSummarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2204–2213.\\n[105] Tian Shi, Yaser Keneshloo, Naren Ramakrishnan, and Chandan K Reddy. 2021. Neural abstractive text summarization\\nwith sequence-to-sequence models. ACM Transactions on Data Science 2, 1 (2021), 1–37.\\n[106] K Shivakumar and Rab Soumya. 2015. Text summarization using clustering technique and SVM technique. International\\nJournal of Applied Engineering Research 10, 12 (2015), 28873–28881.\\n[107] Edward Sun, Yufang Hou, Dakuo Wang, Yunfeng Zhang, and Nancy XR Wang. 2021. D2S: Document-to-Slide\\nGeneration Via Query-Based Text Summarization. In Proceedings of the 2021 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies. 1405–1418.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 33}, page_content='1:34\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\n[108] Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. oLMpics-On What Language Model Pre-training\\nCaptures. Transactions of the Association for Computational Linguistics 8 (2020), 743–758.\\n[109] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian\\nRuder, and Donald Metzler. 2020. Long Range Arena: A Benchmark for Efficient Transformers. In International\\nConference on Learning Representations.\\n[110] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2022. Efficient Transformers: A Survey. ACM Comput.\\nSurv. (apr 2022). Just Accepted.\\n[111] Priyam Tejaswin, Dhruv Naik, and Pengfei Liu. 2021. How well do you know your summarization datasets? arXiv\\npreprint arXiv:2106.11388 (2021).\\n[112] Lucy Vanderwende, Hisami Suzuki, Chris Brockett, and Ani Nenkova. 2007. Beyond SumBasic: Task-focused\\nsummarization with sentence simplification and lexical expansion. Information Processing & Management 43, 6 (2007),\\n1606–1618.\\n[113] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information\\nProcessing Systems. 6000–6010.\\n[114] Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and Answering Questions to Evaluate the Factual\\nConsistency of Summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\\n5008–5020.\\n[115] Matt Wilber, William Timkey, and Marten van Schijndel. 2021. To Point or Not to Point: Understanding How\\nAbstractive Summarizers Paraphrase Text. In Findings of the Association for Computational Linguistics: ACL-IJCNLP\\n2021. Association for Computational Linguistics, Online, 3362–3376.\\n[116] Chien-Sheng Wu, Linqing Liu, Wenhao Liu, Pontus Stenetorp, and Caiming Xiong. 2021. Controllable Abstractive\\nDialogue Summarization with Sketch Supervision. In Findings of the Association for Computational Linguistics:\\nACL-IJCNLP 2021. Association for Computational Linguistics, Online, 5108–5122.\\n[117] Hanlu Wu, Tengfei Ma, Lingfei Wu, Tariro Manyumwa, and Shouling Ji. 2020. Unsupervised Reference-Free Summary\\nQuality Evaluation via Contrastive Learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP). Association for Computational Linguistics, Online, 3612–3621.\\n[118] Wen Xiao and Giuseppe Carenini. 2019. Extractive Summarization of Long Documents by Combining Global and\\nLocal Context. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 3011–3021.\\n[119] Jiacheng Xu, Shrey Desai, and Greg Durrett. 2020. Understanding Neural Abstractive Summarization Models via\\nUncertainty. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).\\n6275–6281.\\n[120] Jiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Discourse-Aware Neural Extractive Text Summarization. In\\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 5021–5031.\\n[121] Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R Fabbri, Irene Li, Dan Friedman, and Dragomir R Radev.\\n2019. Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with\\ncitation networks. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 7386–7393.\\n[122] Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek, Krishnan Srinivasan, and Dragomir Radev. 2017.\\nGraph-based Neural Multi-Document Summarization. In Proceedings of the 21st Conference on Computational Natural\\nLanguage Learning (CoNLL 2017). 452–462.\\n[123] Mark Yatskar. 2019. A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume\\n1 (Long and Short Papers). 2318–2323.\\n[124] Dani Yogatama, Fei Liu, and Noah A Smith. 2015. Extractive summarization by maximizing semantic volume. In\\nProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 1961–1966.\\n[125] Tiezheng Yu, Wenliang Dai, Zihan Liu, and Pascale Fung. 2021. Vision Guided Generative Pre-trained Language\\nModels for Multimodal Abstractive Summarization. In Proceedings of the 2021 Conference on Empirical Methods in\\nNatural Language Processing. 3995–4007.\\n[126] Weizhe Yuan, Pengfei Liu, and Graham Neubig. 2021. Can We Automate Scientific Reviewing? arXiv preprint\\narXiv:2102.00176 (2021).\\n[127] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip\\nPham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big Bird: Transformers for Longer Sequences.. In NeurIPS.\\n[128] Fang-Fang Zhang, Jin-ge Yao, and Rui Yan. 2018. On the abstractiveness of neural document summarization. In\\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 785–790.\\n[129] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. Pegasus: Pre-training with extracted gap-sentences\\nfor abstractive summarization. In International Conference on Machine Learning. PMLR, 11328–11339.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 34}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:35\\n[130] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. BERTScore: Evaluating Text\\nGeneration with BERT. In International Conference on Learning Representations.\\n[131] Yusen Zhang, Ansong Ni, Tao Yu, Rui Zhang, Chenguang Zhu, Budhaditya Deb, Asli Celikyilmaz, Ahmed Hassan\\nAwadallah, and Dragomir Radev. 2021. An Exploratory Study on Long Dialogue Summarization: What Works and\\nWhat’s Next. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Findings.\\n(2021).\\n[132] Jinming Zhao, Ming Liu, Longxiang Gao, Yuan Jin, Lan Du, He Zhao, He Zhang, and Gholamreza Haffari. 2020.\\nSummPip: Unsupervised Multi-Document Summarization with Sentence Graph Compression. In Proceedings of the\\n43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. 1949–1952.\\n[133] Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Steffen Eger. 2019. MoverScore: Text Generation\\nEvaluating with Contextualized Embeddings and Earth Mover Distance. In Proceedings of the 2019 Conference on\\nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language\\nProcessing (EMNLP-IJCNLP). 563–578.\\n[134] Yao Zhao, Mohammad Saleh, and Peter J Liu. 2020.\\nSeal: Segment-wise extractive-abstractive long-form text\\nsummarization. arXiv preprint arXiv:2006.10213 (2020).\\n[135] Hao Zheng and Mirella Lapata. 2019. Sentence Centrality Revisited for Unsupervised Summarization. In Proceedings\\nof the 57th Annual Meeting of the Association for Computational Linguistics. 6236–6247.\\n[136] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz,\\nYang Liu, Xipeng Qiu, and Dragomir Radev. 2021. QMSum: A New Benchmark for Query-based Multi-domain\\nMeeting Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Online,\\n5905–5921.\\n[137] Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu, and Eduard Hovy. 2006. ParaEval: Using Paraphrases to\\nEvaluate Summaries Automatically. In Proceedings of the Human Language Technology Conference of the NAACL, Main\\nConference. Association for Computational Linguistics, New York City, USA, 447–454.\\n[138] Chenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng. 2021. MediaSum: A Large-scale Media Interview Dataset for\\nDialogue Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies. 5927–5934.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 35}, page_content='1:36\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\n10\\nSUPPLEMENTARY MATERIALS\\n10.1\\nLong Document Summarization Systems\\nTable 4 details the summary of long document baseline and state-of-the-art summarization systems\\nproposed by previous works in this domain. The \"Prior\" column illustrates whether inductive bias\\nsuch as discourse structure information of the original long document is used and the \"Trunc\"\\ncolumn reflects the total percentage of significant truncation across the five long document bench-\\nmark.\\nModel\\nDescription\\nPrior\\nTrunc\\nUnsupervised\\nBaseline\\nExtractive\\nLSA [38]\\nSingular Value Decomposition + Term Frequency of Sentence Matrix\\n-\\n-\\nTextRank [83]\\nSentence Graph Centrality (PageRank) + Similarity(common words)\\n-\\n-\\nLexRank [29]\\nSentence Graph Centrality (PageRank) + Similarity(Tf-Idf)\\n-\\n-\\nSumBasic [112]\\nAverage Word-Occurrence Probability of Sentence\\n-\\n-\\nUnsupervised\\nNeural\\nExtractive\\nPacSum [135]\\nSentence Graph Centrality + Similarity(BERT)\\n-\\n-\\nHipoRank [25]\\nHierarchical Section-Sentence Graph Centrality + Similarity(BERT)\\nSection\\n-\\nFAR [69]\\nSentence Graph Centrality + Similarity(BERT) + Facet Aware Candidate Search\\n-\\n-\\nSupervised\\nNeural\\nExtractive\\nSent-CLF/PTR [94]\\nBi-LSTM (word & sentence level) w/o pretraining\\n-\\n-\\nGlobalLocal [118]\\nGloVe (word/sentence level) + Bi-LSTM (section & document level)\\nSection\\n-\\nTopic-GraphSum [22]\\nBERT (sentence level) + Bipartite GAT (NTM topic-sentence)\\n-\\nUNK\\nSSM-DM [21]\\nBERT (sentence level) + Sliding Selector Network (Memory Network & GAT)\\n-\\n-\\nSupervised\\nNeural\\nAbstractive\\nDiscourse-Aware [20]\\nBiLSTM (word- & section- level) w/o pretraining\\nSection\\n-\\nPegasus [129]\\nPretraining Summarization Task (GSG) on C4/HugeNews + Data-specific fine-tuning\\n-\\n85%\\nCRTLSum [45]\\nFine-tuned BART with keywords prompt-engineering\\n-\\n85%\\nBigBird [127]\\nPretrained Pegasus with Sparse Attention (Local + Global + Random)\\n-\\n20%\\nLongformer [2]\\nFine-tuned BART with SparseAttention (Local + Global)\\n-\\n-\\nHEPOS [49]\\nFine-tuned BART with Efficient Attentions (LSH/Sinkhorn + Hepos)\\n-\\n2%\\nSupervised\\nNeural\\nHybrid\\nTLM+Ext [94]\\nBi-LSTM for ContentSelection + Transformer-based Decoder w/o pretraining\\n-\\n-\\nDANCER [37]\\nPretrained Pegasus Section-by-Section Summarization\\nSection\\n-\\nSEAL [134]\\nEnd-to-end Transformer-based Content Selector + Decoder w/o pretraining\\n-\\n-\\nLoBART [77]\\nMulti-task RNN for ContentSelection + Fine-tuned BART with SparseAttention (Local)\\n-\\n-\\nTable 4. Summary of Long Document Baseline and State-of-the-Art Summarization Systems.\\n10.2\\nGraph-based Ranking Algorithm in Experimental Section\\nIn general form, given a set of sentences in the original source document, 𝐷= {𝑠1,𝑠2, ...,𝑠𝑚}\\nwith the inter-sentential similarity relations represented as 𝑒𝑖𝑗= (𝑠𝑖,𝑠𝑗) ∈𝐸where 𝑖≠𝑗, the\\nfollowing equation illustrates the graph-based ranking architecture in computing the scoring for\\neach sentence:\\n𝑐𝑒𝑛𝑡𝑟𝑎𝑙𝑖𝑡𝑦(𝑠𝑖) =\\n∑︁\\n𝑗∈{1,...,𝑚},𝑖≠𝑗\\n𝑒𝑖𝑗∗𝐵𝑖𝑎𝑠(𝑒𝑖𝑗)\\nThe similarity between each sentence is computed using similarity measures such as cosine\\nsimilarity after being encoded using a sentence encoder. The graph architecture we implemented\\nis a basic directed graph where the centrality score of each sentence is computed based on the\\nsummation of bias-adjusted cosine similarity between other sentences and/or sections. The section\\nnode will still be represented by sentences where it is the average of the representations for\\nsentences within the section of interest. In other words, a sentence that has the highest sum of\\nsimilarity against all the other sentences after adjusting for bias will be ranked as the top sentence.\\nTf-Idf Only. For Tf-Idf encoding, we use scikitlearn Tf-Idf vectorizer to train the encoder using\\nsource documents in the arXiv test set (note: this does not include the reference summaries). The\\npreprocessing follows [77] to ensure consistency and the minimum document frequency is set to\\nbe 0.01. As the vector dimension based on original Tf-Idf will be huge, we reduce the dimension to\\n768 using TruncatedSVD.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 36}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:37\\nBERT-only. For BERT encoding, we utilize SentenceTransformer package (https://www.sbert.net/)\\nwhere the model used is \"bert-base-nli-mean-tokens\".\\nTf-Idf + Long Document Discourse Bias. Following the implementation of [25], the bias is calculated\\nusing intra-section bias (or position-level bias within each section) and inter-section bias (or section-\\nlevel bias). For sentences within the same section, the cosine-similarity between sentences adjusted\\nby intra-section bias, 𝑒𝑖𝑛𝑡𝑟𝑎𝐵\\n𝑖𝑗\\n, is computed by adjusting the lambda biases, 𝜆1 and 𝜆2 based on\\nsentence boundary function, 𝑑𝑏:\\n𝑒𝑖𝑛𝑡𝑟𝑎𝐵\\n𝑖𝑗\\n=\\n\\x1a 𝑠𝑖𝑚(𝑠𝐼\\n𝑗,𝑠𝐼\\n𝑖) ∗𝜆1,𝑖𝑓𝑑𝑏(𝑠𝐼\\n𝑖) ≥𝑑𝑏(𝑠𝐼\\n𝑗)\\n𝑠𝑖𝑚(𝑠𝐼\\n𝑗,𝑠𝐼\\n𝑖) ∗𝜆2,𝑖𝑓𝑑𝑏(𝑠𝐼\\n𝑖) < 𝑑𝑏(𝑠𝐼\\n𝑗)\\nwhere the sentence boundary function, 𝑑𝑏, will determine sentences 𝑠𝐼\\n𝑖that are closer to the section\\n𝐼boundaries to be more important. This is computed by:\\n𝑑𝑏(𝑠𝐼\\n𝑖) = 𝑚𝑖𝑛(𝑥𝐼\\n𝑖, 𝛼(𝑛𝐼−𝑥𝐼\\n𝑖))\\n𝑛𝐼is the number of sentences in section 𝐼and 𝑥𝐼\\n𝑖represents sentence i’s position in section I. Cosine\\nsimilarity between sentence and section adjusted by inter-section bias is calculated similarly except\\nthat the bias is computed based on the section’s position in the document. Finally, the resulting\\nadjusted centrality score for each sentence is:\\n𝑐(𝑠𝐼\\n𝑖) = 𝜇1 · 𝑐𝑖𝑛𝑡𝑒𝑟(𝑠𝐼\\n𝑖) + 𝑐𝑖𝑛𝑡𝑟𝑎(𝑠𝐼\\n𝑖)\\nwhere 𝜇1 is a weighting factor for inter-section centrality.\\nFor hyperparameter tuning, we set 𝜆1 = 0.5 and 𝜆2 = 1. Then, using arXiv validation samples, we\\nadjust 𝛼∈{0, 0.5, 0.8, 1.0, 1.2} to control the relative importance of the start and end of a section of a\\nsource document and 𝜇1 ∈{0.5, 1.0, 1.5} to control the weights of intra-section sentence importance\\nversus inter-section sectional importance. Importantly, the original paper uses 𝜆1 = 0 where less\\nimportant sentences are pruned, while we set 𝜆1 = 0.5 to down weight rather than prune the less\\nimportant sentences. For more details, we refer our reader to the original paper [25] with their\\ncodes available on https://github.com/mirandrom/HipoRank.\\nBERT + Long Document Discourse Bias. Same as Tf-Idf except that the Tf-Idf sentence encoder is\\nreplaced by BERT.\\nLastly, for the implementation of Transformer-based abstractive summarization models, the links\\nto original author’s pre-trained weights, codes and implementations are provided in the footnote\\nof our main article.\\n10.3\\nBERT NSP - Assessing Semantic Coherence of Candidate Summaries\\nBommasani and Cardie [4] suggested using the general pre-trained BERT model to evaluate the\\nsemantic coherence or fluency of a summary without any fine-tuning. We find the general pre-\\ntrained BERT model that was not trained on academic papers to have little discriminative ability\\nfor the semantic coherence of arXiv-related summaries. Thus, we fine-tune the pre-trained BERT\\nmodel using positive and negative sentence pairs. Positive sentences are extracted by taking any\\nsentence together with its following sentence in the reference summary and source text of the arXiv\\ndataset. Negative sentences are created by replacing the following sentences with any randomly\\nextracted sentences from either the same or other documents in the arXiv benchmark dataset. We\\nalso ensured that the total samples of positive and negative sentence pairs are balanced and an\\nequal amount of sentences are obtained from the reference summary and the source text. The codes\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 37}, page_content='1:38\\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\\nof all the metrics used in the main paper, including BERT NSP and intrinsic characteristics of the\\ndataset, are made publicly available on: https://github.com/huankoh/long-doc-summarization.\\n10.4\\nTextual Entailment as Factual Consistency Metric\\nAdaptation\\nGood\\nBad\\nOrdered R1\\n0.03 /0.10 /0.86\\n0.04 / 0.11 / 0.85\\nOrdered R2\\n0.05 / 0.11 / 0.84\\n0.04 / 0.13 / 0.83\\nOrdered RL\\n0.04 / 0.12 / 0.84\\n0.04 / 0.13 / 0.83\\nRandomised R1\\n0.04 / 0.25 / 0.71\\n0.04 / 0.30 / 0.66\\nRandomised R2\\n0.06 / 0.21 / 0.73\\n0.07 / 0.19 / 0.74\\nRandomised RL\\n0.05 / 0.25 / 0.70\\n0.05 / 0.24 / 0.71\\nTable 5. Textual Entailment Result (Entail/Contradict/Neutral) on annotated arXiv reference summaries\\nafter Long Document Adaptation. The Good column represents the results of reference summaries that are\\nannotated to be high quality and the Bad column for low-quality data.\\nConditional on the source text, the entailment task classifies summary sentences as entails,\\nneutral or contradicts. Ideally, a candidate summary should entail or be neutral to the source text,\\nbut never contradict the source text. As BERT textual entailment model without long document\\nadaptation have a token limit of 512, this will not be directly applicable for long document datasets\\nwith a token length of at least in the thousands. To use this in our experiment, we attempted to adapt\\nMaynez et al. [81]’s textual entailment BERT model by implementing a content selection mechanism\\nto reduce the input size of the source document. The selected subset is constructed using gold\\nlabel sequences by greedily optimizing the ROUGE score on the ground truth reference summaries,\\nfollowing the algorithm provided by Xiao and Carenini [118]. As we have different variants of the\\nROUGE score, we experimented with a greedy selection of Rouge-1, Rouge-2 and Rouge-L in the\\noriginally ordered sentences and in the randomized ordered sentences. The aim of randomized\\nordered sentences is to ensure that the salient contents are extracted more uniformly from the\\nsource text. To evaluate the discriminative ability of our adapted model, we use the annotated test\\ndata in section 3 to evaluate the discriminative ability of our adapted BERT textual entailment\\nmodel. As the annotated data has 15% of the randomly sampled data to be extremely low quality\\nand 30% to have zero errors in the sentences. A good BERT textual entailment model should then\\nevaluate the 30% high-quality samples as low contradiction and high entailment or neutrality while\\nevaluating the 15% low-quality samples with higher contraction and lower entailment or neutrality.\\nFrom Table 5, despite trying out various adaptations, our models have almost no discriminative\\nability and were thus not used in our experimental section in the main article. This is an important\\nlimitation of our experiment in section 5 of our main article.\\n10.5\\nMetric-related ACL main conference research papers\\nTable 6 next page details the 17 papers published from 2015 to September 2021 in ACL main\\nconferences, including ACL, NAACL, EACL, EMNLP, CoNLL, and AACL.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'), Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 38}, page_content='An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\\n1:39\\nPaper Title\\nYear\\nI. ACL\\nStudying Summarization Evaluation Metrics in the Appropriate Scoring Range\\n2019\\nFacet-Aware Evaluation for Extractive Summarization\\n2020\\nFEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization\\n2020\\nOn Faithfulness and Factuality in Abstractive Summarization\\n2020\\nImproving Factual Consistency of Abstractive Summarization via Question Answering\\n2021\\nA Training-free and Reference-free Summarization Evaluation Metric via Centrality-weighted Relevance and Self-referenced Redundancy\\n2021\\nGO FIGURE: A Meta Evaluation of Factuality in Summarization\\n2021\\nFocus Attention: Promoting Faithfulness and Diversity in Summarization\\n2021\\nEvaluating the Efficacy of Summarization Evaluation across Languages\\n2021\\nII. NAACL\\nQuestion Answering as an Automatic Evaluation Metric for News Article Summarization\\n2019\\nUnderstanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics\\n2021\\nIII. EMNLP\\nRe-evaluating Automatic Summarization with BLEU and 192 Shades of ROUGE\\n2015\\nBetter Summarization Evaluation with Word Embeddings for ROUGE\\n2015\\nAnswers Unite! Unsupervised Metrics for Reinforced Summarization Models\\n2019\\nWhat Have We Achieved on Text Summarization?\\n2020\\nEvaluating the Factual Consistency of Abstractive Text Summarization\\n2020\\nRe-evaluating Evaluation in Text Summarization\\n2020\\nTable 6. List of Metric-related ACL Main Conferences Papers. EACL, CoNLL, and AACL do not have metric-\\nrelated summarization research papers.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.')]\n",
      "[Document(metadata={'producer': 'Skia/PDF m131', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36', 'creationdate': '2025-01-28T18:43:11+00:00', 'source': 'rag-dataset/Summarize Large Documents or Text Using LLMs and LangChain _ by Ranjeet Tiwari _ Senior Architect - AI _ IITJ _ Medium.pdf', 'file_path': 'rag-dataset/Summarize Large Documents or Text Using LLMs and LangChain _ by Ranjeet Tiwari _ Senior Architect - AI _ IITJ _ Medium.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Summarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-28T18:43:11+00:00', 'trapped': '', 'page': 0}, page_content='Member-only story\\nSummarize Large Documents or\\nText Using LLMs and LangChain\\nRanjeet Tiwari | Senior Architect - AI | IITJ · Follow\\n4 min read · Jul 17, 2024\\n100\\nSummarizing long texts can be quite a challenge, but with LangChain and\\nLanguage Learning Model (LLM), it’s made simple. Imagine you’re reading a\\nlengthy book or a detailed report, and you need to condense it into a short,\\neasy-to-read summary.\\nLangChain(with LLM) provides several strategies to help you do just that.\\nLet’s dive into these strategies using real-world examples to make things\\nclearer.\\nLangChain\\nThe “Stuff” Strategy\\nThe simplest method is called the “stuff” strategy. If the entire text fits within the\\nLLM’s context window, you can directly input the raw text and get a summary.\\nFor example, suppose you have a short article about climate change:\\nInput Text:\\n“Climate change refers to long-term shifts and alterations in temperature\\nand weather patterns, primarily due to human activities like burning fossil\\nfuels, deforestation, and industrial processes. These activities increase levels\\nof greenhouse gases in the atmosphere, leading to global warming and its\\nThis member-only story is on us. Upgrade to access all of Medium.\\nOpen in app\\nSearch\\nWrite\\n28/1/25, 1:43 p.m.\\nSummarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium\\nhttps://medium.com/@AI-Simplified/summarize-large-documents-or-text-using-llms-and-langchain-3116543df4ba\\n1/6'), Document(metadata={'producer': 'Skia/PDF m131', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36', 'creationdate': '2025-01-28T18:43:11+00:00', 'source': 'rag-dataset/Summarize Large Documents or Text Using LLMs and LangChain _ by Ranjeet Tiwari _ Senior Architect - AI _ IITJ _ Medium.pdf', 'file_path': 'rag-dataset/Summarize Large Documents or Text Using LLMs and LangChain _ by Ranjeet Tiwari _ Senior Architect - AI _ IITJ _ Medium.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Summarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-28T18:43:11+00:00', 'trapped': '', 'page': 1}, page_content='associated impacts, such as more frequent extreme weather events, rising\\nsea levels, and changes in wildlife habitats.”\\nUsing the “stuff” strategy, you input the entire paragraph, and the LLM\\nprovides a summary:\\nSummary received from LLMs of your choice:\\n“Climate change is caused by human activities that increase greenhouse\\ngases, leading to global warming and extreme weather.”\\nThe “Map-Reduce” Strategy\\nOften, texts are too long to fit into the context window. In such cases, LangChain’s\\n“map-reduce” strategy is useful. This strategy involves breaking the text into\\nchunks, summarizing each chunk, and then summarizing those summaries.\\nStep-by-Step Example:\\n1. Creating Chunks of text extracted from documents/blogs/news:\\n“Climate change refers to long-term shifts and alterations in temperature\\nand weather patterns, primarily due to human activities like burning\\nfossil fuels, deforestation, and industrial processes.”\\n“These activities increase levels of greenhouse gases in the atmosphere,\\nleading to global warming and its associated impacts, such as more\\nfrequent extreme weather events, rising sea levels, and changes in\\nwildlife habitats.”\\n2. Summarize each chunks created based on given length and allowed\\nlimits of LLMs system:\\n“Climate change is caused by human activities altering weather patterns.”\\n“Increased greenhouse gases lead to global warming and extreme\\nweather.”\\n3. Combine Summaries provided by LLMs:\\n“Human activities cause climate change by altering weather patterns and\\nincreasing greenhouse gases, leading to global warming and extreme\\nweather.”\\nThe “Refine” Strategy\\nThe “refine” strategy involves starting with an initial summary of the first chunk\\nof text and gradually refining it with subsequent chunks. This approach allows for\\na more integrated and cohesive summary.\\nStep-by-Step Example:\\n1. Initial Summary:\\n28/1/25, 1:43 p.m.\\nSummarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium\\nhttps://medium.com/@AI-Simplified/summarize-large-documents-or-text-using-llms-and-langchain-3116543df4ba\\n2/6'), Document(metadata={'producer': 'Skia/PDF m131', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36', 'creationdate': '2025-01-28T18:43:11+00:00', 'source': 'rag-dataset/Summarize Large Documents or Text Using LLMs and LangChain _ by Ranjeet Tiwari _ Senior Architect - AI _ IITJ _ Medium.pdf', 'file_path': 'rag-dataset/Summarize Large Documents or Text Using LLMs and LangChain _ by Ranjeet Tiwari _ Senior Architect - AI _ IITJ _ Medium.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Summarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-28T18:43:11+00:00', 'trapped': '', 'page': 2}, page_content='“Climate change is caused by human activities.”\\n2. Refine with Additional Chunks:\\nAfter adding the second chunk: “Climate change is caused by human\\nactivities that increase greenhouse gases, leading to global warming.”\\nAfter adding the third chunk: “Climate change, driven by human\\nactivities, increases greenhouse gases and leads to global warming and\\nextreme weather.”\\nSummarizing Multiple Documents\\nSuppose you have a set of documents (PDFs, blogs , customer questions, etc.)\\nand you want to summarize the content.\\nLLMs are a great tool for this given their proficiency in understanding and\\nsynthesizing text. In the context of retrieval-augmented generation,\\nsummarizing text can help distill the information in a large number of\\nretrieved documents to provide context for a LLM.\\nUsing LangChain for Multi-Document Summarization\\nLangChain simplifies summarizing content from multiple documents with a\\nfew straightforward steps. Here’s a quick guide:\\n1. Set Up Your Environment:\\nUse a Jupyter Notebook for an interactive learning experience.\\nInstall LangChain and its dependencies.\\npip install langchain\\n2. Load Your Documents:\\nUse document loaders, like the WebBaseLoader, to load content from\\nvarious sources.\\nfrom langchain_community.document_loaders import WebBaseLoader \\nloader = WebBaseLoader(\"https://example.com/blog-post\") \\ndocs = loader.load()\\n3. Choose Your Summarization Strategy:\\nStuff: Concatenate documents into a single prompt.\\nMap-Reduce: Split documents into batches, summarize those, and then\\nsummarize the summaries.\\n28/1/25, 1:43 p.m.\\nSummarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium\\nhttps://medium.com/@AI-Simplified/summarize-large-documents-or-text-using-llms-and-langchain-3116543df4ba\\n3/6'), Document(metadata={'producer': 'Skia/PDF m131', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36', 'creationdate': '2025-01-28T18:43:11+00:00', 'source': 'rag-dataset/Summarize Large Documents or Text Using LLMs and LangChain _ by Ranjeet Tiwari _ Senior Architect - AI _ IITJ _ Medium.pdf', 'file_path': 'rag-dataset/Summarize Large Documents or Text Using LLMs and LangChain _ by Ranjeet Tiwari _ Senior Architect - AI _ IITJ _ Medium.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Summarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-28T18:43:11+00:00', 'trapped': '', 'page': 3}, page_content='Refine: Update a rolling summary by iterating over the documents in\\nsequence.\\nfrom langchain.chains.summarize import load_summarize_chain\\nfrom langchain_openai import ChatOpenAI\\nllm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-1106\")\\n# Using Stuff\\nchain = load_summarize_chain(llm, chain_type=\"stuff\")\\nresult = chain.invoke(docs)\\nprint(result[\"output_text\"])\\n# Using Map-Reduce\\nchain = load_summarize_chain(llm, chain_type=\"map_reduce\")\\nresult = chain.invoke(docs)\\nprint(result[\"output_text\"])\\n# Using Refine\\nchain = load_summarize_chain(llm, chain_type=\"refine\")\\nresult = chain.invoke(docs)\\nprint(result[\"output_text\"])\\nTo summarize\\nLangChain offers versatile strategies for summarizing text using LLMs,\\nmaking it easier to handle texts of any length. Whether you use the “stuff,”\\n“map-reduce,” or “refine” strategy depends on the text’s length and\\ncomplexity. By breaking down the text and refining summaries, you can\\nachieve clear and concise summaries suitable for any purpose.\\nWritten by Ranjeet Tiwari | Senior Architect - AI | IITJ\\n71 Followers · 8 Following\\nM.Tech from IIT Jodhpur, Senior Architect - AI in a IT Company, Follow me on\\nmedium and LinkedIn at https://www.linkedin.com/in/ranjeet-tiwari-9606a946/\\nFollow\\nNo responses yet\\nGenerative Ai Solution\\nLangchain\\nLarge Language Models\\nDeep Learning\\nArtificial Intelligence\\nWhat are your thoughts?\\nRespond\\n28/1/25, 1:43 p.m.\\nSummarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium\\nhttps://medium.com/@AI-Simplified/summarize-large-documents-or-text-using-llms-and-langchain-3116543df4ba\\n4/6'), Document(metadata={'producer': 'Skia/PDF m131', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36', 'creationdate': '2025-01-28T18:43:11+00:00', 'source': 'rag-dataset/Summarize Large Documents or Text Using LLMs and LangChain _ by Ranjeet Tiwari _ Senior Architect - AI _ IITJ _ Medium.pdf', 'file_path': 'rag-dataset/Summarize Large Documents or Text Using LLMs and LangChain _ by Ranjeet Tiwari _ Senior Architect - AI _ IITJ _ Medium.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Summarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-28T18:43:11+00:00', 'trapped': '', 'page': 4}, page_content='More from Ranjeet Tiwari | Senior Architect - AI | IITJ\\nSee all from Ranjeet Tiwari | Senior Architect - AI | IITJ\\nRecommended from Medium\\nBuilding Large Language Models\\nfrom Scratch: Initial Guide\\nOct 1, 2024\\nPrepare Instruction Dataset to\\nFine-Tune Large Language Model…\\nFine-tuning a Large Language Model (LLM)\\ncan seem daunting, but with a clear…\\nJul 15, 2024\\nSmall LLMs: Building a Multi-\\nAgentic RAG System\\nEver wondered how powerful a “small”\\nlanguage model can be? 🤔 Imagine buildin…\\nJan 5\\nLeveraging Celery and Kafka for\\nEfficient Distributed Processing i…\\nIn the realm of distributed processing and\\ntask management, the combination of Celer…\\nJul 9, 2023\\nAI’nt That Easy #20: Evaluating\\nText Summarization with LLM…\\nUse HuggingFace\\napply_chat_template when…\\nRanjeet Tiwari | Senior Architect - AI | IITJ\\n5\\nRanjeet Tiwari | Senior Architect - AI | IITJ\\n39\\nRanjeet Tiwari | Senior Architect - AI | IITJ\\n13\\nRanjeet Tiwari | Senior Architect - AI | IITJ\\n72\\n4\\nAakriti Aggarwal\\nManyi\\n28/1/25, 1:43 p.m.\\nSummarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium\\nhttps://medium.com/@AI-Simplified/summarize-large-documents-or-text-using-llms-and-langchain-3116543df4ba\\n5/6'), Document(metadata={'producer': 'Skia/PDF m131', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36', 'creationdate': '2025-01-28T18:43:11+00:00', 'source': 'rag-dataset/Summarize Large Documents or Text Using LLMs and LangChain _ by Ranjeet Tiwari _ Senior Architect - AI _ IITJ _ Medium.pdf', 'file_path': 'rag-dataset/Summarize Large Documents or Text Using LLMs and LangChain _ by Ranjeet Tiwari _ Senior Architect - AI _ IITJ _ Medium.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'Summarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-28T18:43:11+00:00', 'trapped': '', 'page': 5}, page_content='Lists\\nAI Regulation\\n6 stories · 678 saves\\nNatural Language Processing\\n1903 stories · 1556 saves\\nChatGPT\\n21 stories · 954 saves\\nGenerative AI Recommended\\nReading\\n52 stories · 1620 saves\\nSee more recommendations\\nLarge Language Models (LLMs) have\\nrevolutionized natural language processing,…\\nOct 20, 2024\\nTo build a chatbot, it is common practice to\\nuse an instruction-tuned model. An…\\nNov 27, 2024\\nIntroduction to Building\\nApplications with the DeepSeek…\\nIn today’s rapidly evolving world of artificial\\nintelligence (AI), large language models…\\n1d ago\\nPart 2A: Implementing a Graph\\nBuilder to Extract a Basic…\\nIn this post, I’ll show you how to construct a\\nbasic document-centric knowledge graph…\\n6d ago\\n6 AI Agents That Are So Good,\\nThey Feel Illegal\\nAI agents are the future because they can\\nreplace all the manual work with automation…\\nJan 11\\nIn\\nby\\nI am among the first people to gain\\naccess to OpenAI’s “Operator”…\\n“An AI completing multiple tasks at the same\\ntime” — DALL-E\\n4d ago\\n5\\n1\\nKamal Dhungana\\n55\\nNgoc\\n27\\nMohit Vaswani\\n1.3K\\n40\\nArtificial Intelligence in Plain En…\\nAustin Sta…\\n860\\n37\\n28/1/25, 1:43 p.m.\\nSummarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium\\nhttps://medium.com/@AI-Simplified/summarize-large-documents-or-text-using-llms-and-langchain-3116543df4ba\\n6/6')]\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for pdf in pdfs:\n",
    "    loader = PyMuPDFLoader(pdf)\n",
    "    temp = loader.load()\n",
    "    docs.extend(temp)\n",
    "\n",
    "    print(temp)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([x.page_content for x in docs])\n",
    "\n",
    "\n",
    "context = format_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creator': 'LaTeX with acmart 2021/08/29 v1.79 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'creationdate': '2022-07-05T00:43:24+00:00', 'source': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'file_path': 'rag-dataset/and_empirical_survey_on_long_document_summarization.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': 'An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics', 'author': '', 'subject': '-  Information systems  ->  Summarization.-  Computing methodologies  ->  Information extraction.', 'keywords': '', 'moddate': '2022-07-05T00:43:24+00:00', 'trapped': '', 'page': 0}, page_content='1\\nAn Empirical Survey on Long Document Summarization:\\nDatasets, Models and Metrics\\nHUAN YEE KOH, Monash University, Australia\\nJIAXIN JU, Monash University, Australia\\nMING LIU∗, Deakin University, Australia\\nSHIRUI PAN∗†, Monash University, Australia\\nLong documents such as academic articles and business reports have been the standard format to detail out\\nimportant issues and complicated subjects that require extra attention. An automatic summarization system\\nthat can effectively condense long documents into short and concise texts to encapsulate the most important\\ninformation would thus be significant in aiding the reader’s comprehension. Recently, with the advent of neural\\narchitectures, significant research efforts have been made to advance automatic text summarization systems,\\nand numerous studies on the challenges of extending these systems to the long document domain have emerged.\\nIn this survey, we provide a comprehensive overview of the research on long document summarization and\\na systematic evaluation across the three principal components of its research setting: benchmark datasets,\\nsummarization models, and evaluation metrics. For each component, we organize the literature within the\\ncontext of long document summarization and conduct an empirical analysis to broaden the perspective on\\ncurrent research progress. The empirical analysis includes a study on the intrinsic characteristics of benchmark\\ndatasets, a multi-dimensional analysis of summarization models, and a review of the summarization evaluation\\nmetrics. Based on the overall findings, we conclude by proposing possible directions for future exploration in\\nthis rapidly growing field.\\nCCS Concepts: • Information systems →Summarization; • Computing methodologies →Information\\nextraction.\\nAdditional Key Words and Phrases: document summarization, datasets, neural networks, language models,\\nTransformer\\n1\\nINTRODUCTION\\nSummarization of textual information is an exacting task for humans and the rate of information\\ngrowth in the era of big data has made summarizing most information manually to be impractical and\\nimpossible. This phenomenon is exacerbated when it comes to long form textual documents as the\\nknowledge and human labour effort required to process and summarize it increases exponentially\\nwith the length of documents. Inevitably, a significant amount of invaluable information and\\nknowledge have gone unnoticed, presenting an important bottleneck in the progress of social and\\n∗Corresponding Authors: Ming Liu and Shirui Pan.\\n†This work is done while Shirui Pan is with Monash University. From August 2022, he is with the School of Information\\nand Communication Technology, Griffith University, Southport, QLD 4222, Australia.\\nAuthors’ addresses: Huan Yee Koh, huan.koh@monash.edu, Monash University, Australia; Jiaxin Ju, jjuu0002@student.\\nmonash.edu, Monash University, Australia; Ming Liu, Deakin University, Australia, m.liu@deakin.edu.au; Shirui Pan,\\nMonash University, Australia, shiruipan@ieee.org.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\\n© 2022 Association for Computing Machinery.\\n0360-0300/2022/1-ART1 $15.00\\nhttps://doi.org/10.1145/3545176\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\\narXiv:2207.00939v1  [cs.CL]  3 Jul 2022')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "An Empirical Survey on Long Document Summarization:\n",
      "Datasets, Models and Metrics\n",
      "HUAN YEE KOH, Monash University, Australia\n",
      "JIAXIN JU, Monash University, Australia\n",
      "MING LIU∗, Deakin University, Australia\n",
      "SHIRUI PAN∗†, Monash University, Australia\n",
      "Long documents such as academic articles and business reports have been the standard format to detail out\n",
      "important issues and complicated subjects that require extra attention. An automatic summarization system\n",
      "that can effectively condense long documents into short and concise texts to encapsulate the most important\n",
      "information would thus be significant in aiding the reader’s comprehension. Recently, with the advent of neural\n",
      "architectures, significant research efforts have been made to advance automatic text summarization systems,\n",
      "and numerous studies on the challenges of extending these systems to the long document domain have emerged.\n",
      "In this survey, we provide a comprehensive overview of the research on long document summarization and\n",
      "a systematic evaluation across the three principal components of its research setting: benchmark datasets,\n",
      "summarization models, and evaluation metrics. For each component, we organize the literature within the\n",
      "context of long document summarization and conduct an empirical analysis to broaden the perspective on\n",
      "current research progress. The empirical analysis includes a study on the intrinsic characteristics of benchmark\n",
      "datasets, a multi-dimensional analysis of summarization models, and a review of the summarization evaluation\n",
      "metrics. Based on the overall findings, we conclude by proposing possible directions for future exploration in\n",
      "this rapidly growing field.\n",
      "CCS Concepts: • Information systems →Summarization; • Computing methodologies →Information\n",
      "extraction.\n",
      "Additional Key Words and Phrases: document summarization, datasets, neural networks, language models,\n",
      "Transformer\n",
      "1\n",
      "INTRODUCTION\n",
      "Summarization of textual information is an exacting task for humans and the rate of information\n",
      "growth in the era of big data has made summarizing most information manually to be impractical and\n",
      "impossible. This phenomenon is exacerbated when it comes to long form textual documents as the\n",
      "knowledge and human labour effort required to process and summarize it increases exponentially\n",
      "with the length of documents. Inevitably, a significant amount of invaluable information and\n",
      "knowledge have gone unnoticed, presenting an important bottleneck in the progress of social and\n",
      "∗Corresponding Authors: Ming Liu and Shirui Pan.\n",
      "†This work is done while Shirui Pan is with Monash University. From August 2022, he is with the School of Information\n",
      "and Communication Technology, Griffith University, Southport, QLD 4222, Australia.\n",
      "Authors’ addresses: Huan Yee Koh, huan.koh@monash.edu, Monash University, Australia; Jiaxin Ju, jjuu0002@student.\n",
      "monash.edu, Monash University, Australia; Ming Liu, Deakin University, Australia, m.liu@deakin.edu.au; Shirui Pan,\n",
      "Monash University, Australia, shiruipan@ieee.org.\n",
      "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\n",
      "provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\n",
      "the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\n",
      "Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\n",
      "prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n",
      "© 2022 Association for Computing Machinery.\n",
      "0360-0300/2022/1-ART1 $15.00\n",
      "https://doi.org/10.1145/3545176\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "arXiv:2207.00939v1  [cs.CL]  3 Jul 2022\n",
      "\n",
      "1:2\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "economic development. In response, there has been a strong demand for exhaustive research in the\n",
      "field of automatic long document summarization [2, 20, 25, 77, 104].\n",
      "Automatic text summarization involves a process of shortening a source text efficiently while\n",
      "keeping the main idea intact, which aids in reducing the amount of time required to process\n",
      "information, helps with faster search for information, and makes learning one topic easier [71, 73].\n",
      "While the potentiality of developing an effective automatic text summarization system has attracted\n",
      "significant interest and attention from the research community, automatic text summarization\n",
      "remains a challenging task and is not ready for wide practical use in day-to-day lives, particularly\n",
      "when it comes to summarizing long documents [8, 61, 62, 81]. Intuitively, long document sum-\n",
      "marization is harder than short document summarization due to the significant difference in the\n",
      "amount of lexical tokens and breadth of content between short and long documents. As the length\n",
      "increases, the content that would be considered important will also increase, resulting in a more\n",
      "challenging task for an automatic summarization model to capture all salient information in the\n",
      "limited output length [37]. Further, short documents are often generic text such as news articles\n",
      "[43, 84, 87, 102], while long documents are commonly domain-specific articles such as scientific\n",
      "papers that contain more complex formulas and terminologies [20, 49, 59]. Together with other\n",
      "reasons that will be explored in this survey, long document summarization poses a significantly\n",
      "more challenging task than short document summarization.\n",
      "In general, automatic text summarization can be conceptualized as having three approaches:\n",
      "extractive, abstractive, and hybrid approach [62]. The extractive approach directly copies salient\n",
      "sentences from the source document and combine them as the output [15, 38], whereas the abstrac-\n",
      "tive approach imitates human that comprehends a source document and writes a summary output\n",
      "based on the salient concepts of the source document [101, 103]. The hybrid approach attempts to\n",
      "combine the best of both approaches by rewriting a summary based on a subset of salient content\n",
      "extracted from the source document [36, 47, 73]. Each approach has its advantages and limitations\n",
      "that may suit certain summarization tasks better. For example, extractive summarization may\n",
      "be sufficient in summarizing certain news articles [15, 128] but inadequate to summarize a long\n",
      "dialogue where salient content are sparsely distributed [131]. This is because while the extractive\n",
      "summarization approach is always factually consistent with the source document, it does not\n",
      "modify the original text and thus lacks the ability to generate fluent and concise summary [120].\n",
      "Historically, to measure the performance of different summarization architectures, ROUGE score\n",
      "[70] has been the modus operandi for researchers in the summarization research field to compare\n",
      "and study the quality of different candidate summaries. The core idea of ROUGE score is to measure\n",
      "the lexical overlaps such as words and phrases between candidate summary and ground truth\n",
      "summary. While it is efficient, recent findings have shown that ROUGE score does not correlate\n",
      "well with how humans assess the quality of a candidate summary [3, 11, 44, 61]. As a result, there is\n",
      "a significant amount of effort in improving the way we measure the quality of candidate summaries\n",
      "and performance of summarization architectures [62, 80, 81, 126, 130]. Unfortunately, these efforts\n",
      "have entirely been focusing on the short document domains and the progress in measuring the\n",
      "quality of long document summarization approach has been lacking [3, 41, 48, 89, 93].\n",
      "Nevertheless, there has been a considerable amount of advancement made in the long document\n",
      "summarization research field and the area lacks a comprehensive survey [5, 28, 32, 105]. Our\n",
      "paper fills this gap by providing a comprehensive overview of the research on long document\n",
      "summarization and a systematic evaluation across the three principal components of its research\n",
      "setting: benchmark datasets, summarization models, and evaluation metrics.\n",
      "The contribution of our paper is as follows:\n",
      "Comprehensive Review. A comprehensive survey of the long document summarization research\n",
      "literature.\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:3\n",
      "Full-view of summarization research. Text summarization literature mainly explores the three key\n",
      "aspects of research setting: developing advanced models, releasing new datasets, and proposing\n",
      "alternative evaluation metrics. We empirically provide a detailed review of all three key components\n",
      "within the context of long document summarization.\n",
      "Empirical Studies and Thorough Analysis. To ensure wide coverage of emerging trends, we\n",
      "empirically analyze each component of the long document summarization research setting through\n",
      "fine-grained human analysis and ad-hoc experiments.\n",
      "Future Direction. We discuss the current progress of long document summarization, analyze the\n",
      "limitation of existing methods, and suggest promising future research directions in terms of model\n",
      "designs, quality and diversity of datasets, the practicality of evaluation metrics and, finally, the\n",
      "feasibility of implementing summarization techniques to real-life applications.\n",
      "The survey is organized as follows: firstly, an overview of the fundamentals of long document\n",
      "summarization in section 2. Secondly, a detailed study of ten summarization benchmark datasets\n",
      "is in section 3. A comprehensive survey on summarization models that are designed specifically\n",
      "or have to ability to summarize long documents in section 4. Then, in section 5, we analyze the\n",
      "performances of models that are representative of the different types of architectures commonly\n",
      "used by researchers through ad-hoc experiments. In section 6, we summarize the advancement in\n",
      "evaluation metrics and their applicability in the long document summarization domain. Section\n",
      "7 goes into the applications of long document summarization models and Section 8 discusses\n",
      "promising future research direction in this field. Finally, section 9 concludes this survey.\n",
      "2\n",
      "FUNDAMENTALS OF LONG DOCUMENT SUMMARIZATION\n",
      "To make clear the distinction between short and long documents, we conceptualize the summariza-\n",
      "tion task problem from three different fundamental aspects: 1) length of document, 2) breath of\n",
      "content, and 3) degree of coherence.\n",
      "2.1\n",
      "Length of Document\n",
      "Documents are commonly classified as \"long\" because the number of lexical tokens in the source\n",
      "document is enormous and it requires a considerable amount of time for an average human to\n",
      "consume the full text. While this definition makes intuitive sense, in the context of machine learning,\n",
      "a document is considered long when current state-of-the-art models for a normal document cannot\n",
      "be implemented similarly in an effective manner due to hardware and model limitations. For\n",
      "example, previous research [10] considers CNN/DM and NYT benchmark datasets in the news\n",
      "domain as long documents when in the present research context they are now considered to be\n",
      "short document datasets. Currently, a benchmark dataset with an average source document length\n",
      "that exceeds 3,000 lexical tokens could be well-considered as \"long documents\" [77, 127] due to\n",
      "the fact that most existing state-of-the-art summarization systems (e.g., pre-trained models) are\n",
      "limited to 512 to 1024 lexical tokens only [23, 129]. These limitations cannot be easily solved\n",
      "without novel techniques that help in assisting current architectures to reason over a long range of\n",
      "textual inputs [77, 82, 127]. Accordingly, this survey adopts a similar definition where a document\n",
      "is only considered as long if current state-of-the-art systems used in the short document cannot\n",
      "be extended and applied to a document with significantly longer text. Despite the potentially\n",
      "confusing definition, this enduring definition ensures that the model architectures implemented by\n",
      "researchers require novel techniques to overcome hardware limitations rather than just a mere\n",
      "replica of previous works.\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:4\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "2.2\n",
      "Breadth of Content\n",
      "On average, informative content that is non-redundant will increase together with the length of\n",
      "a document. However, despite the fact that reference summary length often increases together\n",
      "with the source document length, the length of a summary is usually constrained by what an\n",
      "average user considered as reasonable [20, 104]. Thus, while it may be sufficient for summaries of\n",
      "a reasonable length to cover the most or even all of the informative aspects for short documents,\n",
      "this is not necessarily true for summaries of long documents. In section 3, we empirically show\n",
      "that the relative length of summary against the source document becomes exponentially shorter as\n",
      "the source document length increases. Due to this elevated constraint, the ground truth summary\n",
      "of a long document will inevitably lose information that is not key to the central narrative of the\n",
      "original author or summary writer [37]. Furthermore, recent work [61] has also identified that\n",
      "human users could not agree on what should be considered important for a given document in\n",
      "the short document news domain due to the heterogeneity of user preferences and expectations.\n",
      "This issue is exacerbated when it comes to long document summarization as (a) the relative length\n",
      "of summary against the source document is shorter and (b) the chance of users having different\n",
      "preferences and expectations would increase as the breadth of content increases, making the long\n",
      "document summarization task significantly harder than short document.\n",
      "2.3\n",
      "Degree of Coherence\n",
      "As compared to short documents, long documents are often structured into sections for the ease\n",
      "of user comprehension [20, 59]. The content within each section also differ to a certain extent\n",
      "despite revolving around a key narrative of the long documents. This makes the long document\n",
      "summarization task more burdensome as summarization models cannot concatenate salient texts\n",
      "from different sections without considering its impact on the fluency, redundancy, and semantic\n",
      "coherence of the final summary outputs.\n",
      "Based on the fundamental aspects, the rest of this paper provides an empirical survey on long\n",
      "document summarization, covering the benchmark datasets, summarization models, and metrics.\n",
      "3\n",
      "DATASETS\n",
      "Publicly available benchmark datasets have been introduced to evaluate the performance of summa-\n",
      "rization models. Nonetheless, the benchmark datasets have different intrinsic characteristics that\n",
      "have been found to be crucial in the understanding of model performances [81, 111], summarization\n",
      "approach suitability (i.e., extractive or abstractive approach) [104, 128] and evaluation metrics effec-\n",
      "tiveness [31, 89]. Hence, only through a comprehensive understanding of the benchmark datasets,\n",
      "one can assess the underlying performance and applicability of a summarization model in the real-\n",
      "world settings [61]. Further, insights drawn from benchmark datasets have led to the introduction\n",
      "of state-of-the-art models across a wide range of natural language processing (NLP) tasks [13, 123],\n",
      "including the text summarization task [25, 37, 77]. In response, intrinsic dataset evaluation through\n",
      "large-scale automatic evaluation [4] or more fine-grained human evaluation at a smaller scale [111]\n",
      "has also been performed to enhance the understanding of various benchmarks. Nevertheless, none\n",
      "of the aforementioned works performed a large-scale automatic evaluation analysis nor a thorough\n",
      "human evaluation of benchmark datasets in the long document text summarization domain. To\n",
      "address this gap, this section explores the basic statistics and intrinsic characteristics of popular\n",
      "benchmarks in short and long document domain through the usage of large-scale automatic evalu-\n",
      "ation metrics and performs fine-grained human analysis on the arXiv benchmark to encourage a\n",
      "better appreciation of the most widely used long document summarization dataset [49, 77, 127].\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:5\n",
      "3.1\n",
      "Corpora\n",
      "Short document. Short-document datasets studied in this survey are CNN-DM, NWS, XSUM,\n",
      "Reddit-TIFU, and WikiHow. The first three news datasets are chosen due to their popularity while\n",
      "Reddit-TIFU and WikiHow are studied to ensure short documents from other domains are also\n",
      "included. The document-summary pairs from CNN-DM, NWS, and XSUM are typical of that\n",
      "in the news domain, where the source document represents news article while the summary\n",
      "represents either human-curated summary [43, 87] or summary created by concatenating bullet-\n",
      "point sentences in the original source document [84]. On the other hand, Reddit-TIFU is a dataset\n",
      "collected from the subreddit r/TIFU [56], while the WikiHow benchmark is created using the first\n",
      "sentence of each WikiHow web page’s paragraph as the summary and the rest as source text [60].\n",
      "Long document. For long document summarization research, arXiv, PubMed, BIGPATENT, Bill-\n",
      "Sum, and GovReport have been used in prior research to test and compare novel long document\n",
      "summarization models. arXiv and PubMed [20] are scientific long document summarization datasets\n",
      "collected from arXiv.org and PubMed.com scientific repository. Both datasets represent the earliest\n",
      "work on large-scale long document summarization datasets. BIGPATENT [104] is an enormous\n",
      "dataset with over 1.3 million document-summary records of U.S. patent documents along with\n",
      "human written abstractive summary. BillSum [59] is a dataset on summarizing Congressional and\n",
      "California state bills where the content structures and stylistic features of writing are considerably\n",
      "different from documents in other domains. GovReport [49], assembled from reports published by\n",
      "U.S. Government Accountability Office, is markedly longer than the other long document datasets.\n",
      "Other long document benchmark datasets that are worth mentioning but are no longer widely used\n",
      "due to the limited amount of document-summary pairs are CL-SciSumm and SciSummNet [50, 121].\n",
      "Some other benchmark datasets that are released more recently in the podcast [19] and dialogue\n",
      "domains [9, 51, 97, 138] may also be classified as long document summarization benchmark [77]\n",
      "but are not explored in this survey as dialogue summarization has been recognized as another\n",
      "sub-domain due to its distinctive features as compared to other document types.\n",
      "3.2\n",
      "Data Metrics\n",
      "Given a document, 𝐷, and a corresponding reference summary, 𝑆, each document will have a\n",
      "sequence of tokens 𝐷𝑡𝑜𝑘𝑒𝑛= {𝑡1,𝑡2, ...,𝑡𝑛} and each summary will also have a sequence of tokens\n",
      "𝑆𝑡𝑜𝑘𝑒𝑛= {𝑡∗\n",
      "1,𝑡∗\n",
      "2, ...,𝑡∗\n",
      "𝑚}. Similarly, each document and summary have𝑙and𝑜sentences as represented\n",
      "by 𝐷𝑠𝑒𝑛𝑡= {𝑠1,𝑠2, ...,𝑠𝑙} and 𝑆𝑠𝑒𝑛𝑡= {𝑠∗\n",
      "1,𝑠∗\n",
      "2, ...,𝑠∗\n",
      "𝑜} respectively. Length of document and summary\n",
      "measured in number of tokens are represented as |𝐷| and |𝑆| while length measured in number\n",
      "of sentences are represented as ||𝐷|| and ||𝑆||. Extending on the works in the short document\n",
      "summarization domain [4, 43], the following discusses each of the five metrics used to evaluate the\n",
      "benchmark datasets shown in Table 1: compression ratio, extractive coverage, extractive density,\n",
      "redundancy and uniformity.\n",
      "Compression Ratio measures the ratio of a source document length against its reference summary\n",
      "length. A higher compression ratio indicates larger information loss in the original document after\n",
      "being summarized. Compression ratios are measured based on tokens and sentences:\n",
      "𝐶𝑂𝑀𝑃𝑅𝐸𝑆𝑆𝐼𝑂𝑁𝑡𝑜𝑘𝑒𝑛= |𝐷|\n",
      "|𝑆|\n",
      "and\n",
      "𝐶𝑂𝑀𝑃𝑅𝐸𝑆𝑆𝐼𝑂𝑁𝑠𝑒𝑛𝑡= ||𝐷||\n",
      "||𝑆||\n",
      "Extractive Coverage and Extractive Density are introduced by Grusky et al. [43] based on the\n",
      "notion of matching fragments. Fragments are obtained by greedily matching the longest shared\n",
      "token sequence where F (𝐷,𝑆) reflects a set of fragments with each fragment having a length\n",
      "represented by |𝑓|. Extractive coverage calculates the percentage of tokens in summary that is a\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:6\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "derivation of the original source text, whereas, extractive density relates to the average squared\n",
      "length of the extractive fragments in the summary. The former indicates the need for a model to\n",
      "coin novel tokens that are not in the original source text while the latter measures whether a model\n",
      "can match the ground truth summary merely by extracting from the original source text without\n",
      "rearranging or paraphrasing text.\n",
      "𝐶𝑂𝑉𝐸𝑅𝐴𝐺𝐸(𝐷,𝑆) = 1\n",
      "|𝑆|\n",
      "∑︁\n",
      "𝑓∈F(𝐷,𝑆)\n",
      "|𝑓|\n",
      "𝐷𝐸𝑁𝑆𝐼𝑇𝑌(𝐷,𝑆) = 1\n",
      "|𝑆|\n",
      "∑︁\n",
      "𝑓∈F(𝐷,𝑆)\n",
      "|𝑓|2\n",
      "Redundancy relates to the redundancy of ground truth summary by measuring the average\n",
      "ROUGE-L F1-score of all distinct pairs of summary sentences [4]. As ROUGE-L measures the\n",
      "longest common sub-sequence overlap between two texts [70], a higher redundancy score would\n",
      "suggest that a candidate summary is more redundant as the sentence pairs in the ground truth\n",
      "summary contain more similar content in each sentence pair. For each summary consisting of m\n",
      "sentences, S, we have a set of distinct pairs of sentences, S × S, where the redundancy score is\n",
      "calculated as:\n",
      "𝑅𝐸𝐷𝑈𝑁𝐷𝐴𝑁𝐶𝑌(𝑆) =\n",
      "𝑎𝑣𝑒𝑟𝑎𝑔𝑒\n",
      "(𝑥𝑖,𝑥𝑗) ∈S×S,𝑥𝑖≠𝑥𝑗\n",
      "𝑅𝑂𝑈𝐺𝐸(𝑥𝑖,𝑥𝑗)\n",
      "Uniformity measures whether content that are considered important by the reference summary\n",
      "are uniformly scattered across the entire source document. A higher score indicates that important\n",
      "content are scattered across the entire document with no obvious layout bias to take advantage of.\n",
      "This is calculated based on the normalized entropy of the decile positions of salient unigrams in\n",
      "the source text, where salient unigrams are the top 20 keywords extracted1, excluding stopwords,\n",
      "from the reference summary.\n",
      "𝑈𝑁𝐹(𝑢𝑛𝑖𝑔𝑟𝑎𝑚𝑝𝑜𝑠) = 𝐻𝑛𝑜𝑟𝑚(𝑢𝑛𝑖𝑔𝑟𝑎𝑚𝑝𝑜𝑠)\n",
      "3.3\n",
      "Intrinsic Characteristics of Datasets\n",
      "3.3.1\n",
      "Short vs Long Document Benchmark Dataset.\n",
      "Based on Table 1 below, the following discusses the findings of intrinsic characteristics of long\n",
      "document benchmark datasets in comparison to short document benchmark datasets.\n",
      "Finding 1. Length of Long Documents: A basic yet important finding is that, except for\n",
      "BillSum, all the other long document datasets have an average source document length of at least\n",
      "3,000 tokens. In contrast, the longest short document dataset, CNN-DM, has an average document\n",
      "length of 774 tokens. This indicates that a vanilla pre-trained Transformer-based models [66, 96, 129]\n",
      "which commonly have an input length limit of 1,024 tokens would need to truncate at least half\n",
      "of the source document in the long document benchmark datasets. Thus, if pre-trained models\n",
      "that have proven to work well under short document settings are implemented without any long\n",
      "document adaptations in their architectural settings and mechanisms, they are unlikely to generate\n",
      "high-quality summaries for long documents [45, 81, 100].\n",
      "Finding 2. High Compression Ratio and its Implications: On average, the token-level and\n",
      "sentence-level compression ratio of the long document summarization datasets is greater than the\n",
      "short document datasets by 1.4 and 2.2 times respectively. For long documents, this suggests that\n",
      "either a) there is a greater information loss in the summaries, b) the salient content is more sparsely\n",
      "distributed across the source documents, and/or c) the source document contains significantly\n",
      "1We use NLTK-RAKE for keywords extraction.\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:7\n",
      "Short Document Datasets\n",
      "Long Document Datasets\n",
      "Long vs. Short\n",
      "CNN-DM\n",
      "NWS\n",
      "XSum\n",
      "WikiHow\n",
      "Reddit\n",
      "ArXiv\n",
      "PubMed\n",
      "BigPatent\n",
      "BillSum\n",
      "GovReport\n",
      "Avg. Ratio\n",
      "# doc-summ.\n",
      "278K\n",
      "955K\n",
      "203K\n",
      "231K\n",
      "120K\n",
      "215K\n",
      "133K\n",
      "1.34M\n",
      "21.3K\n",
      "19.5K\n",
      "-\n",
      "summ tokens\n",
      "55\n",
      "31\n",
      "24\n",
      "70\n",
      "23\n",
      "242\n",
      "208\n",
      "117\n",
      "243\n",
      "607\n",
      "6.9x\n",
      "doc tokens\n",
      "774\n",
      "767\n",
      "438\n",
      "501\n",
      "444\n",
      "6446\n",
      "3143\n",
      "3573\n",
      "1686\n",
      "9409\n",
      "8.3x\n",
      "summ sents\n",
      "3.8\n",
      "1.5\n",
      "1\n",
      "5.3\n",
      "1.4\n",
      "6.3\n",
      "7.1\n",
      "3.6\n",
      "7.1\n",
      "21.4\n",
      "3.7x\n",
      "doc sents\n",
      "29\n",
      "31\n",
      "19\n",
      "27\n",
      "22\n",
      "251\n",
      "102\n",
      "143\n",
      "42\n",
      "300\n",
      "6.5x\n",
      "Compressiontoken\n",
      "14.8\n",
      "31.7\n",
      "19.7\n",
      "7.2\n",
      "18.4\n",
      "41.2\n",
      "16.6\n",
      "36.3\n",
      "12.2\n",
      "18.7\n",
      "1.4x\n",
      "Compressionsent\n",
      "8.3\n",
      "22.4\n",
      "18.9\n",
      "3.3\n",
      "14.5\n",
      "44.3\n",
      "15.6\n",
      "58.7\n",
      "9.7\n",
      "18.1\n",
      "2.2x\n",
      "Coverage\n",
      "0.890\n",
      "0.855\n",
      "0.675\n",
      "0.610\n",
      "0.728\n",
      "0.920\n",
      "0.893\n",
      "0.861\n",
      "0.913\n",
      "0.942\n",
      "1.2x\n",
      "Density\n",
      "3.6\n",
      "9.8\n",
      "1.1\n",
      "1.1\n",
      "1.4\n",
      "3.7\n",
      "5.6\n",
      "2.1\n",
      "6.6\n",
      "7.7\n",
      "1.5x\n",
      "Redundancy\n",
      "0.157\n",
      "0.088\n",
      "-\n",
      "0.324\n",
      "0.078\n",
      "0.144\n",
      "0.146\n",
      "0.223\n",
      "0.163\n",
      "0.124\n",
      "1.0x\n",
      "Uniformity\n",
      "0.856\n",
      "0.781\n",
      "0.841\n",
      "0.813\n",
      "0.777\n",
      "0.894\n",
      "0.896\n",
      "0.922\n",
      "0.903\n",
      "0.932\n",
      "1.2x\n",
      "Table 1. Comparison of Short and Long Document Summarization Datasets. Intrinsic characteristics are\n",
      "computed based on the average result of test samples. Average Ratios are computed based on the average\n",
      "long over short document statistics.\n",
      "more redundant information. As the high compression ratio of long document benchmark is more\n",
      "likely to be the results of the two former factors, this increases the relative difficulty of the long\n",
      "document summarization task as a model would have to clearly identify the key narrative from the\n",
      "source while excluding the content that are expected to be less important by the summary readers.\n",
      "Moreover, if there is a greater information loss in the summary of a long document, the generated\n",
      "summary will inevitably miss an even greater amount of information that is considered important\n",
      "by some readers, diminishing the effectiveness of a generalized summarization approach to satisfy\n",
      "the needs of summary readers. This finding supports the efforts in controllable summarization,\n",
      "where the final generated summaries will be based on the reader’s needs and expectations [45, 116].\n",
      "Finding 3. Abstractiveness and Diversity of Datasets: With the exception of BIGPATENT,\n",
      "all long document datasets have greater coverage and density values than the short document\n",
      "datasets. This is likely due to the genres of benchmark datasets where long documents are often\n",
      "related to domain-specific articles such as scientific papers that contain more complex formulas\n",
      "and terminologies. Nonetheless, this indicates that a model that merely extracts lexical fragments\n",
      "from the original source text of a long document can still generate a summary that more closely\n",
      "resembles the reference summary. As abstractive summarization models have recently been found to\n",
      "contain factual inconsistencies in up to 30% of the summary outputs in the short document domain\n",
      "[8, 62] while extractive summarization model will faithfully preserve the original content, this\n",
      "finding is encouraging for the development of long document extractive models in the real-world\n",
      "production level settings. Finally, as the abstractiveness of datasets have been found to greatly\n",
      "affect the summarization strategies of a supervised model [115, 119, 128], efforts to introduce\n",
      "benchmark datasets with greater abstractiveness (low extractive coverage and density value) should\n",
      "be encouraged to improve the diversity of long document benchmark datasets.\n",
      "Finding 4. Lesser Layout Bias in Long Document: Kryściński et al. [61] found substantial\n",
      "layout bias in the source text where nearly 60% of important sentences are contained in the first\n",
      "30% of the source articles and argued that such layout bias does not apply to the other domains.\n",
      "Our findings on the uniformity of salient content in Table 1 validates their arguments where the\n",
      "salient content of long documents are scattered across the entire source text more uniformly than\n",
      "the short documents. This suggests that unlike practices in the short document summarization\n",
      "domain where models are often benefited by taking advantage of layout biases [36, 90, 103], long\n",
      "document models that implement a truncation strategy to process only a small subset of the leading\n",
      "content of the long documents will likely suffer from significant performance degradation.\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:8\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "Long\n",
      "Short\n",
      "a) Pairwise Correlations Between Metrics\n",
      "b) Facets Covered by ArXiv Ground Truth Summary\n",
      "Fig. 1. Pairwise Correlations between Metrics (figure 1a) and Facets Covered by ArXiv Ground Truth Summary\n",
      "(figure 1b). For figure 1a, the upper diagonal reflects Pearson correlation coefficient for long document\n",
      "benchmarks while the lower diagonal reflects values for short document benchmarks. Figure 1b’s barplot is\n",
      "constructed based on human annotated testset data as described in section 3.5.\n",
      "Finding 5. Relationship between Intrinsic Characteristics: Other than the intrinsic char-\n",
      "acteristic measured in Table 1, the statistical relationship between these metrics could yield insights\n",
      "regarding the underlying properties of a benchmark dataset. More importantly, whether the rela-\n",
      "tionship between these metrics differs significantly under short and long document summarization\n",
      "settings should also be of great interest to practitioners. To quantify this, we report the pairwise\n",
      "correlations between each metric pair for both short document (lower diagonal) and long document\n",
      "(upper diagonal) benchmark datasets in figure 1a. The values reported are calculated using the\n",
      "Pearson correlation coefficient, 𝜌. As represented by darker blue color in figure 1a, 𝜌= 1 reflects\n",
      "a perfectly positive correlation between the metric pair and 𝜌= −1 when it is perfectly negative\n",
      "(shown in darker red color).\n",
      "For positive controls, we see a strong positive relationship between the two compression ratios\n",
      "and the two extractive metrics (coverage and density) under short and long document settings.\n",
      "We also see a lack of statistical correlation when uniformity is measured against other metrics as\n",
      "uniformity relates more to the genres of documents rather than the other characteristics. We further\n",
      "observe redundancy to be inversely related to coverage and density, where a more abstractive\n",
      "reference summary often contains more redundant information. This finding is consistent with\n",
      "a human evaluation study by Kryscinski et al. [62] where writers are found to be more verbose\n",
      "and write summary content that do not add information when they are writing unconstrained,\n",
      "abstractive summaries. Intriguingly, we see a weakly positive correlation between the extractive\n",
      "metrics and the compression metrics under the short document setting but a strongly negative\n",
      "correlation under the long document setting. It is hypothesize that when authors have to write\n",
      "a concise summary, they are forced to paraphrase the original content more to ensure that the\n",
      "summary can cover the salient content within the constrained summary length.\n",
      "3.3.2\n",
      "Comparison between Long Document Dataset Benchmarks.\n",
      "Looking at the intrinsic characteristics between long document benchmark datasets, arXiv and\n",
      "BIGPATENT have significantly higher compression ratios but lower extractive density values than\n",
      "the others, indicating that two of these datasets require a summarization model to generate a\n",
      "significantly shorter summary that is not written in the same way as the source text. As discussed\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:9\n",
      "above, this is likely because for a summary to cover more content within a constrained summary\n",
      "length, one has to paraphrase the original content more. This is also evidenced by the diverging\n",
      "values between extractive coverage and density metric for arXiv benchmark dataset that suggest\n",
      "summaries of arXiv scientific papers have high matching tokens and terminologies with the source\n",
      "document (high coverage) but low matching phrases (low density). Overall, the BIGPATENT dataset\n",
      "is the most suitable benchmark for long document supervised abstractive summarization due to its\n",
      "low coverage and density, substantial training sample pairs to serve as supervisory signals, and\n",
      "high uniformity in salient content. However, only a handful of fully-supervised abstractive long\n",
      "document summarization works [94, 127, 129] evaluated their models on BIGPATENT, limiting the\n",
      "visibility of current progress on long document summarizers in general applications. This is despite\n",
      "the fact that BIGPATENT was introduced not long after arXiv and PubMed. Encouragingly, with\n",
      "the recent introduction of long document datasets in domains other than scientific papers including\n",
      "financial reports [75] and books [63], the research progress of long document summarization\n",
      "models towards general application should become clearer in the near future.\n",
      "3.4\n",
      "Fine-grained Analysis on ArXiv\n",
      "To perform fine-grained human analysis on the arXiv benchmark, this survey implements a stratified\n",
      "random sampling strategy based on the 6 different categories of scientific domains contained\n",
      "in the arXiv.org scientific repository: physics (ph), computer-science (cs), mathematics (math),\n",
      "quantitative-biology (q-bio), quantitative-finance (q-fin) and statistics (stat). In total, we obtain over\n",
      "700 annotated ground truth summaries with physics having the most samples (369) followed by\n",
      "computer science (140). Based on fine-grained human analysis of 743 ground truth summaries in\n",
      "the arXiv test set, this subsection reports the disturbing data quality results and studies the degree\n",
      "of diversity in formatting style of reference summary.\n",
      "Noise in arXiv benchmark dataset: With the advent of data-hungry neural architectures, there\n",
      "has been an enormous demand for benchmark datasets with document-summary pairs that are\n",
      "at least in the tens of thousands created through heuristic means such as scraping it directly\n",
      "from the web. As a result, depending on the means of extracting these datasets, the quality of\n",
      "benchmark datasets may vary significantly from one another. To this end, Kryściński et al. [61]\n",
      "have quantified the percentage of samples with noise for CNN-DM and Newsroom from the short\n",
      "document summarization datasets to be 4.19% and 3.17% using simple heuristic methods. The\n",
      "noises found in the datasets through heuristic means can only suggest a lower bound of what\n",
      "the true amount of noises are as heuristic approaches can only detect obvious structural flaws in\n",
      "the samples. This suggests that the true underlying noises are extremely widespread and often\n",
      "understated. Glaringly, in our experiment, the problem of noisy data affects more than 60% of the\n",
      "annotated ground truth summaries in the randomly sampled arXiv test set. This is greater than 54%\n",
      "detected in XSUM dataset [111]. While many of the errors and noises are minor, more than 15% of\n",
      "the reference summaries have significant errors where at least half of the summary contains errors,\n",
      "rendering the summaries to be unreadable. Reassuringly, the rest of the test sets with identified\n",
      "noises are not overly significant and often only affect one or two sentences in a benchmark dataset\n",
      "with an average of 10 sentences in the reference summary. To further understand why the noises\n",
      "and errors occurred, we trace the original data based on the arXiv id provided by the benchmark\n",
      "datasets. It was found that many errors occurred such as missing content or sentence breaking after\n",
      "a newline could be due to large-scale scraping of the original data using pandoc [20]. As neural\n",
      "summarization models may overfit to these problematic noises and contribute to less interpretable\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:10\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "benchmarking results, we release the annotated data to allow a better understanding of the common\n",
      "noises and to encourage quality improvement in future benchmark datasets2.\n",
      "Content Coverage of Reference Summary: Other than analyzing the noises in the arXiv benchmark\n",
      "dataset, we also explore to what extent the ground truth summary covers various sections or\n",
      "facets of the source article. Figure 1b shows the average distribution of sections covered by ground\n",
      "truth summaries for different domains. The distribution is plotted based on the assumption that\n",
      "each sentence in the reference summary covers a single facet of the original article and human\n",
      "annotators are asked to identify the section covered by the summary sentences. The facets studied\n",
      "are Introduction, Methodology, Result, Conclusion and Limitation of Research. Interestingly, while\n",
      "the reference summaries in all domains have covered introduction and methodology sections\n",
      "with similar emphasis, we see a negative correlation between contribution and conclusion (i.e.,\n",
      "papers that emphasize contributions will write less on conclusions, and vice-versa). Notably, we see\n",
      "scientific papers in the mathematics domain emphasize more on the contribution while papers in\n",
      "the physics domain emphasize more on conclusions. These results make intuitive sense as findings\n",
      "in mathematics often do not lead to a strong substantive conclusion. The trade-off between various\n",
      "sections also illustrates the inevitable information loss when summarizing a long document as the\n",
      "summary can only describe certain aspects of the source document but not all.\n",
      "Style of Writing and External Knowledge: Importantly, except for quantitative-biology, all scientific\n",
      "papers do not discuss the limitations of their research. This is consistent with common practices\n",
      "of writing abstracts to attract readers in reading the original paper by emphasizing on the result\n",
      "findings and contributions of the authors. Nevertheless, most researchers would find a discussion\n",
      "on the limitations of research works to be informative and significant. Whether the abstract itself\n",
      "represents the best possible summary for a summarization architecture to imitate from and learn\n",
      "how to appropriately summarize all the salient content including the limitation discussed in the\n",
      "original paper remains an important question to be answered. Recent progress on summarization\n",
      "approaches that generate user-specific summaries based on the need of readers are also important\n",
      "directions towards general applicability of summarization models in commercial settings [45, 116].\n",
      "Lastly, to summarize the limitations of a paper often requires more external knowledge outside of\n",
      "the content related to the source document and whether current summarization models are able to\n",
      "infer such knowledge from the benchmark dataset is an interesting study left for future works.\n",
      "4\n",
      "MODELS\n",
      "4.1\n",
      "Overview\n",
      "The following describes the differences between the extractive, abstractive and hybrid summariza-\n",
      "tion approaches and the general taxonomy of a summarization system.\n",
      "A. Extractive, Abstractive and Hybrid Approach.\n",
      "The works in automatic text summarization research are traditionally classified into three different\n",
      "summarization approaches: (i) the extractive approach that involves direct extraction of salient\n",
      "fragments such as sentences of the original documents into a summary [15, 38], (ii) the abstractive\n",
      "approach imitates human behavior of paraphrasing important parts of a document into a summary\n",
      "[101, 103] and (iii) the hybrid approach that attempts to combine the best of both approaches\n",
      "[37, 77]. Intuitively, the extractive summarization method is an easier machine learning task and\n",
      "can be thought of as a classification and/or ranking problem of extracting lexical fragment units\n",
      "(e.g., sentences) into a summary. Contrastively, abstractive summarization requires paraphrasing\n",
      "2The annotated dataset are released: https://github.com/huankoh/long-doc-summarization\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:11\n",
      "Important sentence\n",
      "Less Important sentence\n",
      "Unimportant sentence\n",
      "Word/Sentence/Section \n",
      "Attention Mechanism\n",
      "R \n",
      "N \n",
      "N\n",
      "R \n",
      "N \n",
      "N\n",
      "R \n",
      "N \n",
      "N\n",
      "X1\n",
      "X2\n",
      "Xn\n",
      "Encoder\n",
      "...\n",
      "R \n",
      "N \n",
      "N\n",
      "R \n",
      "N \n",
      "N\n",
      "R \n",
      "N \n",
      "N\n",
      "Y1\n",
      "Y2\n",
      "Yn\n",
      "Decoder\n",
      "C1\n",
      "Cn-1\n",
      "...\n",
      "R \n",
      "N \n",
      "N\n",
      "R \n",
      "N \n",
      "N\n",
      "R \n",
      "N \n",
      "N\n",
      "Y1\n",
      "Y2\n",
      "Yn\n",
      "Decoder\n",
      "...\n",
      "Extractive\n",
      "Abstractive\n",
      "Context vector\n",
      "C\n",
      "C0\n",
      "Classifier\n",
      "Predict/\n",
      "generate\n",
      "A - Classic Graph (Extractive)\n",
      "B - RNN-based Model Architecture\n",
      "C - Transformers\n",
      "Softmax\n",
      "Linear\n",
      "Add & Norm\n",
      "Feed \n",
      "Forward\n",
      "Add & Norm\n",
      "Multi-Head\n",
      "Attention\n",
      "Add & Norm\n",
      "Masked \n",
      "Multi-Head \n",
      "Attention\n",
      "Output\n",
      "Embedding\n",
      "Outputs \n",
      "(Shifted right)\n",
      "Add & Norm\n",
      "Feed \n",
      "Forward\n",
      "Add & Norm\n",
      "Multi-Head\n",
      "Attention\n",
      "Input \n",
      "Embedding\n",
      "Inputs\n",
      "N x\n",
      "N x\n",
      "Output \n",
      "Probabilities\n",
      "Positional \n",
      "Encoding\n",
      "Positional \n",
      "Encoding\n",
      "Fig. 2. Overview of Model Architectures.\n",
      "important ideas of a document into a summary either by rearranging words and phrases from\n",
      "original text or contriving novel wordings while maintaining the factual consistency of the generated\n",
      "summary with the original document.\n",
      "Since the extractive summarization approach only extracts and arranges the original text that\n",
      "it believes to be salient and does not alter the original text, it enjoys the benefit of generating\n",
      "summaries that are factually consistent with the source document [21]. Nevertheless, as human-\n",
      "based summarization often involves paraphrasing ideas and concepts into shorter, concise sentences,\n",
      "the extracted sentences of this approach often contain redundant and uninformative phrases [42].\n",
      "While there exist extractive summarization models that break a source document into lower lexical\n",
      "units than sentences (e.g., elementary discourse units) [120], they are often not applied in the long\n",
      "document summarization domain due to the extreme length of the input document.\n",
      "On the other hand, mimicking how humans write summaries, the abstractive summarization\n",
      "approach presents a blue-sky potential of generating summaries that are fluent, concise and relevant\n",
      "to the source document [103]. It can also incorporate external knowledge to the summary depending\n",
      "on the needs of a user [81]. However, at the current stage of development, summaries generated\n",
      "by the state-of-the-art abstractive models often contain a significant amount of content that is\n",
      "factually inconsistent with the source document, limiting its application in commercial settings\n",
      "[8, 62].\n",
      "Finally, in response to the limitation of current model architectures and designs, the hybrid\n",
      "summarization approach only differs from the abstractive summarization approach in that it takes\n",
      "in a carefully chosen subset of the original input document rather than the entire input document\n",
      "in its original form [37, 94]. This extra step reduces the burden on the abstractive summarization\n",
      "models that have to generate an abstract summary and select important content at the same time.\n",
      "This approach is used more often in the long document summarization domain because current\n",
      "models still fail either (a) at reasoning over extremely long texts [77, 82] and/or (b) suffers from\n",
      "memory complexity issues and hardware limitations that prevent it from processing over a long\n",
      "input text [49, 127].\n",
      "B. General Taxonomy.\n",
      "In each long document summarization model, this paper breaks down a model into two different\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:12\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "constituents: (i) Main Architecture and (ii) its Mechanisms. The main architecture refers to the\n",
      "core framework structure that a model uses and the mechanisms are the different settings or\n",
      "modifications implemented by a model to the main architecture. Two differing models may use\n",
      "the same main architectures but are implemented with different mechanisms, and vice-versa. For\n",
      "example, models that use graph-based main architecture may use different encoding mechanisms\n",
      "in vectorizing the sentences of an input document. The following describes the various main archi-\n",
      "tectures of the summarization models together with how previous works differ in the mechanisms\n",
      "employed to generate long document summaries.\n",
      "4.2\n",
      "Main Architecture and its Mechanisms\n",
      "In the search for optimal architectural settings of summarization systems, the research field started\n",
      "out with many different novel designs of main architectures and mechanisms but often converge\n",
      "towards a few ideas that are often most effective until another ground-breaking idea that leap-frogs\n",
      "the performance of previous systems, and the cycle repeats.\n",
      "1. Graph Architecture:\n",
      "For the extractive summarization approach, the classic graph architecture involves a two-stage\n",
      "process of mapping a document into a graph network, where the vertices are sentences and the\n",
      "edges are the similarity between these sentences, and extracting the top-𝐾sentences. The sentences\n",
      "are ranked based on the graph centrality scoring of each sentence [29, 83]. As there are many\n",
      "different ways to (a) encode or vectorize a sentence before calculating the similarity between them\n",
      "and (b) calculate the centrality score of each sentence, research involving this architecture often\n",
      "differs only in these two mechanisms. For example, with respect to the former mechanism, graph\n",
      "architecture in the past [29, 83] encodes sentences based on word-occurrence or term frequency-\n",
      "inverse document frequency (Tf-Idf) while graph architecture today [69, 135] encodes sentences\n",
      "with state-of-the-art pre-trained models. On the other hand, to improve the centrality scoring\n",
      "mechanism, PacSum [135] and FAR [69] adjust the centrality score of a sentence based on whether\n",
      "the other sentences come before or after it, while HipoRank [25] exploits the discourse structure\n",
      "contained in by adjusting the centrality score with positional and sectional bias. In general form,\n",
      "given a set of sentences in the original source document, 𝐷= {𝑠1,𝑠2, ...,𝑠𝑚} with the inter-sentential\n",
      "similarity relations represented as 𝑒𝑖𝑗= (𝑠𝑖,𝑠𝑗) ∈𝐸where 𝑖≠𝑗, the following illustrates the\n",
      "aforementioned architecture in computing the scoring for each sentence:\n",
      "𝑐𝑒𝑛𝑡𝑟𝑎𝑙𝑖𝑡𝑦(𝑠𝑖) =\n",
      "∑︁\n",
      "𝑗∈{1,...,𝑖−1,𝑖+1,...,𝑚}\n",
      "𝑒𝑖𝑗∗𝐵𝑖𝑎𝑠(𝑒𝑖𝑗)\n",
      "The similarity between each sentence is computed using similarity measures such as dot product\n",
      "or cosine similarity, and the sentences are vectorized using Tf-Idf or BERT representation values.\n",
      "The final summary is generated by extracting the top-k sentences ranked by 𝑐𝑒𝑛𝑡𝑟𝑎𝑙𝑖𝑡𝑦(𝑠𝑖). Im-\n",
      "portantly, while there are other classical architectures [38, 112], the graph architecture is worth\n",
      "a separate mentioning here due to the fact that (a) it remains as a strong baseline against other\n",
      "advanced architectures, (b) it can effectively incorporate external knowledge as an inductive bias to\n",
      "the calculation of the importance of a sentence and (c) it achieves state-of-the-art result in long doc-\n",
      "ument unsupervised extractive summarization setting when integrated with current state-of-the-art\n",
      "pre-trained models [25, 69]. Lastly, other than the multi-sentence compression approach [6, 54, 132]\n",
      "that may be extended to long document summarization tasks, there has been no applicable work\n",
      "on classical graph-based architecture for long document abstractive summarization.\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:13\n",
      "2. Other Classical Architectures:\n",
      "In the early work of automated, non-neural text summarization models, past research mostly\n",
      "focused on the extractive summarization approach due to the difficulty of the abstractive sum-\n",
      "marization task. The main architectures that were tested ranged from support vector machines\n",
      "[12, 106], Bayesian classifiers [64], decision trees [57, 78] to citation network-based summarization\n",
      "[95]. The ones that remained relevant when comparing model performance across various bench-\n",
      "marks in long document summarization settings are LSA [38], which is based on Singular Value\n",
      "Decomposition (SVD), and SumBasic [112] that ranks sentences by simple average word-occurrence\n",
      "probability [112].\n",
      "3. Recurrent Neural Networks:\n",
      "Extractive summarization that employed neural networks with continuous representations rather\n",
      "than pre-trained word embeddings on traditional techniques [58, 124] was proposed by Cheng\n",
      "and Lapata [15]. The model implemented an RNN encoder-decoder architecture with attention\n",
      "mechanism to locate the region of focus during sentence extraction process. Nevertheless, due to\n",
      "the lack of a large-scale long document dataset and the RNN’s inability in capturing long-range\n",
      "temporal dependencies across a long input text, it wasn’t until Xiao and Carenini [118] that tried\n",
      "implementing LSTM-minus (a variant of RNN) on solving long document summarization task.\n",
      "Typical of a long document summarization system, it incorporates discourse-information (i.e. section\n",
      "structure) of the source document by encoding the section-level and document-level representation\n",
      "into each sentence to significantly boost the model performance. Pilault et al. [94] also suggested\n",
      "two different variants of RNNs on extractive summarization for long document summarization.\n",
      "Rather than utilizing pre-trained word embeddings, they implemented a hierarchical LSTM to\n",
      "encode words and sentences separately.\n",
      "When it comes to the abstractive summarization approach, Celikyilmaz et al. [10] proposed\n",
      "multiple communicating agents to address the task of long document summarization. However,\n",
      "as compared to other simpler architecture, this approach did not gain significant traction after\n",
      "the introduction of the first large-scale dataset on long scientific documents. Together with the\n",
      "contribution of two most commonly used long scientific document datasets, arXiv and PubMed,\n",
      "Cohan et al. [20] presented an LSTM encoder-decoder architecture where the decoder attends to\n",
      "each section of the source document to determine section-level attention weights before attending\n",
      "to each word. While similar architectures have been widely used in prior works, this work ef-\n",
      "fectively incorporates discourse information that suits the long document summarization task well.\n",
      "4. Transformers:\n",
      "The Transformer model proposed in 2017 together with the pre-trained Bidirectional Encoder\n",
      "Representation from Transformers (BERT) model that was based on the Transformer model itself\n",
      "have taken the NLP area by storm [23, 113]. Like other NLP tasks, subsequent summarization\n",
      "model architectures have changed significantly to take advantage of these two momentous ideas.\n",
      "Importantly, BERTSum [74] showed that by modifying the BERT-segmentation embeddings, it\n",
      "can capture not only sentence-pair inputs but multi-sentential inputs. The BERTSum model could\n",
      "effectively solve both extractive and abstractive summarization tasks. The extractive summarization\n",
      "model proposed by BERTSum involves stacking a Transformer-based classifier on top of the fine-\n",
      "tuned BERT to select and extract salient sentences while the abstractive summarization model\n",
      "involves a classic encoder-decoder Transformer framework where the encoder is a fine-tuned\n",
      "BERT and the decoder is a randomly-initialized Transformer that is jointly trained together in\n",
      "an end-to-end manner. While effective, this architecture cannot be implemented to solve long\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:14\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "document summarization tasks due to the BERT’s input length limit of 512 tokens. In this survey,\n",
      "we define Transformer as the main architecture and settings related to the Transformer model as\n",
      "the mechanisms including the use of different types of pre-trained models. As Transformer has\n",
      "effectively replaced most main architectures in both summarization approaches as state-of-the-art\n",
      "models, the various mechanisms applied in the long document summarization context will be\n",
      "thoroughly discussed in section 4.3.\n",
      "4.3\n",
      "Mechanisms of Transformer-based Architectures\n",
      "Transformer-based model is ubiquitously state-of-the-art across a wide range of tasks in the\n",
      "NLP domain. In line with this development, recent works in the long document summarization\n",
      "models often involve using the same Transformer base architecture but with different proposing\n",
      "mechanisms. These Transformer-based models involve implementing novel mechanisms with long\n",
      "document adaptations to ensure the task of summarizing a document with significantly longer input\n",
      "sequence texts can be effectively addressed. The mechanisms used by extractive, abstractive and\n",
      "hybrid Transformer-based summarization models are described in the following with an overview\n",
      "of mechanisms used by abstractive and hybrid summarization models shown in Figure 3.\n",
      "Long Doc \n",
      "Adaptation\n",
      "Discourse \n",
      "Structure\n",
      "Truncation\n",
      "Content \n",
      "Selection\n",
      "Transformer-based \n",
      "Abstractive \n",
      "Summarization \n",
      "System\n",
      "Eﬀicient \n",
      "Attention\n",
      "Pre-training \n",
      "Tasks\n",
      "Signal \n",
      "Guidance\n",
      "Prompt \n",
      "Engineering\n",
      "Source Doc\n",
      "Fig. 3. Overview of Transformer-based Abstractive & Hybrid Summarization Models.\n",
      "A. Extractive Transformer.\n",
      "As Transformer and its pre-trained models are optimized for short document settings, they may not\n",
      "reason well over long text sequences if not properly fine-tuned. To this end, Cui et al. [22] proposed\n",
      "combining neural topic modeling together with BERT in learning a topic-enhanced, inter-sentence\n",
      "relationship across the entire document. Nonetheless, the issues of memory complexity and input\n",
      "token length limits were not resolved and significant source text is truncated under this research\n",
      "setting. Recently, Cui and Hu [21] proposed a memory network that incorporates graph attention\n",
      "networks and gated recurrent units to dynamically select important sentences through sliding a\n",
      "window along the entire source document. This approach can effectively integrate the pre-trained\n",
      "BERT model for long document summarization task by limiting its usage within each window,\n",
      "where the window size is set to be lower than or equal to 512 tokens.\n",
      "B. Abstractive Transformer.\n",
      "1) General Sequence-to-Sequence Pre-training Task\n",
      "Since the advent of BERT [23], various large-scale models with different pre-training tasks have been\n",
      "introduced. As summarization with the abstractive approach is naturally a sequence-to-sequence\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:15\n",
      "task, a pre-trained model with a sequence-to-sequence objective task would suit it better rather than\n",
      "an encoder-only (e.g., BERT/RoBERTa) or a decoder-only (e.g., GPT-2/GPT-3) pre-trained model.\n",
      "In the summarization domain, Bidirectional and Auto-Regressive Transformers (BART) [66] and\n",
      "Text-to-Text Transfer Transformer (T5) [96] are the two most widely used sequence-to-sequence\n",
      "pre-trained models. BART is pre-trained on a self-supervised task of reconstructing arbitrarily\n",
      "corrupted text while T5 is pre-trained on both unsupervised and supervised objectives, such as\n",
      "token masking, as well as translation and summarization. Interestingly, none of the supervised\n",
      "Transformer models in the long document summarization domain has implemented a summarizer\n",
      "with T5 pre-training task despite its success in the short document domain [100].\n",
      "2) Gap-Sentence Generation (GSG) Pre-training Task\n",
      "Other than the generalized pre-training task like BART and T5, PEGASUS [129] attempted to signif-\n",
      "icantly advance the progress in the abstractive summarization field through large-scale pre-training\n",
      "with objectives that are specific to the summarization task. The proposed model is self-trained on\n",
      "two large scale datasets (C4 and HugeNews) with the gap-sentence generation pretraining task.\n",
      "Gap-sentence generation pretraining task draws a close resemblance with the general summariza-\n",
      "tion task by self-supervising the model to generate sentences that are masked entirely in a given\n",
      "document. At the time of PEGASUS model release, the model effectively achieves state-of-the-art\n",
      "results across 12 different benchmark datasets, including long document arXiv and PubMed dataset.\n",
      "3) Efficient Attentions\n",
      "The vanilla Transformer models that utilize full attention have a memory complexity 𝑂(𝑛2). This\n",
      "attribute limits its wider usage across many domains, including long document summarization.\n",
      "For example, to circumvent the input tokens limits of PEGASUS, DANCER [37] summarizes each\n",
      "section of the long document separately and concatenates each of them to form the final summary.\n",
      "As not all benchmark datasets contain discourse information such as section structures, this limits\n",
      "the model usage in many long document summarization settings. To this end, researchers have\n",
      "proposed various ingenious ideas to reduce the memory and time complexity of Transformer\n",
      "models. The variants of Transformer models that require less memory are often known as efficient\n",
      "Transformers [109, 110] and the mechanism is referred to as efficient attentions [49].\n",
      "Longformer [2] combines local attention, stride patterns and global memory for fine-tuning\n",
      "pre-trained BART to effectively summarize long documents with a maximum input length of\n",
      "16,384 tokens as opposed to the 1,024 token limit of the original BART model. The model achieved\n",
      "state-of-the-art results in the long document summarization along with other NLP tasks when\n",
      "the model was introduced. BigBird [127] also implemented the efficient attention mechanism\n",
      "on Transformer-based abstractive summarizer by utilizing the same attention modifications as\n",
      "Longformer with an additional random pattern to achieve matching performance results in terms\n",
      "of ROUGE score. An important work by Huang et al. [49] explores and compares the performance\n",
      "of different variants of efficient transformers in the context of long document summarization.\n",
      "4) Prompt-Engineering\n",
      "The GPT-3 model [7] has strongly demonstrated that large-scale pre-trained language models\n",
      "can achieve impressive results on numerous downstream language tasks in zero- and few-shots\n",
      "experimental settings. Rather than fine-tuning the language models for specific tasks in a conven-\n",
      "tional way, natural language prompts and task demonstrations were created for GPT-3 to infer and\n",
      "complete the tasks. Importantly, this is different from the conventional tagging method such as\n",
      "<bos> token for conditional generations or taking [CLS] tag from BERT for classification tasks.\n",
      "Prompt engineering refers to taking the extra step to design a natural language prompt or template\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:16\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "Classification Task\n",
      "Summarization Task\n",
      "Conventional \n",
      "Prompt Engineering \n",
      "Conventional \n",
      "Prompt Engineering\n",
      "CLS\n",
      "this is a long doc  . [SEP]\n",
      "CLS Head\n",
      "label: 0\n",
      "label: 1\n",
      "[CLS] this is a long doc . [SEP]   this is                      .\n",
      "[MASK]\n",
      "MLM head\n",
      "correct (label: 1)\n",
      "incorrect (label: 0)\n",
      "INPUT\n",
      "PROMPT\n",
      "INPUT\n",
      "this is a doc relates to survey on summarization . \n",
      "<bos> a summarization survey . <eos>\n",
      "ENCODER\n",
      "DECODER\n",
      "this is a doc relates to survey on summarization . \n",
      "<bos> survey: a summarization survey . <eos>\n",
      "PROMPT\n",
      "ENCODER\n",
      "DECODER\n",
      "Fig. 4. How prompt engineered language tasks differ. Classification task example from [34] and summarization\n",
      "task using keywords from original document as language prompt from [45].\n",
      "that can optimize the pre-trained model for a specific task. Figure 4 illustrates this important\n",
      "difference.\n",
      "Many works have explored the ways of uncovering the right prompt for various downstream\n",
      "language tasks to significantly boost the performance of pre-trained models [34, 108]. In the long\n",
      "document summarization research area, CRTLSum [45] achieves significant improvement on vanilla\n",
      "fine-tuned BART model on arXiv dataset through prompt-engineering. The model attempts to\n",
      "more effectively use the pre-trained BART model with the help of extracted keyword prompts, as\n",
      "shown in Figure 4. Further, the work also showed that, given an optimized language prompt, the\n",
      "implemented BART summarization model can achieve ROUGE score that matches ROUGE score of\n",
      "oracle summaries in the test dataset.\n",
      "5) Signal Guidance\n",
      "Unlike a prompt-engineering mechanism that requires an engineered language prompt or template\n",
      "for a given task, the signal guidance mechanism relates to utilizing signals as inputs to lead models\n",
      "in better identifying and summarizing important content of source texts. Using this approach,\n",
      "the GSum [26] model implemented a fine-tuned BART model with dual encoders, one for input\n",
      "document and another for extracted signals, and a decoder that attends to both encoded represen-\n",
      "tations. Similar signal based approach is also used by Ju et al. [55] to implement an unsupervised\n",
      "pipeline-based long document summarization model.\n",
      "6) Discourse Bias\n",
      "Similar to the signal guidance mechanism, discourse bias involves the inclusion of the discourse\n",
      "structure of a source document such as the section of a sentence as signals for summarization\n",
      "systems to better identify and summarize important content in the original source text [17, 122].\n",
      "This mechanism can be classified under signal guidance but is mentioned separately due to its\n",
      "effectiveness and popularity in Transformer [37, 94] and non-Transformer [20, 25] based long\n",
      "document summarization models. Unlike short documents, long documents often contain discourse\n",
      "structure information such as table of content, section structures, references and others to guide a\n",
      "human reader in comprehending the original document and previous works have exploited this\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:17\n",
      "information to achieve state-of-the-art results. Nonetheless, previous works in the long document\n",
      "summarization domain only utilized discourse information that is made available by the benchmark\n",
      "datasets and did not implement automatic discourse parsing using RST trees [79] and coreference\n",
      "mentions due to the difficulties of building an effective representation for a document with extreme\n",
      "input length [52, 120].\n",
      "C. Hybrid Transformer - Content Selection + Abstractive Summarization.\n",
      "The hybrid summarization approach only differs from the abstractive approach in that it takes in a\n",
      "carefully chosen subset of the input document rather than the entire input document. This extra\n",
      "step reduces the burden on the neural summarizers that have to generate an abstract summary and\n",
      "select important content at the same time. Some also refer models that utilize the hybrid approach\n",
      "as retrieve-the-summarize model because it involves retrieving a subset of long document text\n",
      "before summarizing it [131]. TLM+Ext [94] first implemented this method by limiting inputs of\n",
      "the scientific articles in arXiv datasets as the introduction of the document, a subset of carefully\n",
      "selected sentences of the original article using extractive summarization approach, and, finally,\n",
      "include the remaining text if there remains extra space for Transformer-based decoder. However,\n",
      "given the effectiveness of sequence-to-sequence neural models, one limitation of this work is that\n",
      "it only utilizes a decoder framework to generate the final summary rather than an encoder-decoder\n",
      "framework that most subsequent works on abstractive and hybrid summarization approach do.\n",
      "Consequently, LoBART [77] proposes a hybrid summarization system that completes a summary\n",
      "generation in two separate steps, (i) content selection: using a multi-task RNN, select salient content\n",
      "from the original source document until the total text output reaches the limit of the sequence-\n",
      "to-sequence pre-trained BART model and (ii) abstractive summarization: summarize the carefully\n",
      "selected subset using a pre-trained BART model with efficient transformer mechanism. SEAL\n",
      "[134] presents a generalized encoder-decoder framework for transformer-based long document\n",
      "summarization and proposed an abstractive summarization system that selects salient content and\n",
      "dynamically choose segments of the selected content for the decoder to attend and summarize in\n",
      "an end-to-end manner. The architecture, however, did not attempt in exploiting the large-scale\n",
      "pre-trained models that were used in most summarization research works. Lastly, facing a similar\n",
      "issue, development in the open-domain question-answering and knowledge-intensive language\n",
      "tasks reflect an interesting parallel with the progress in the long document summarization domain\n",
      "[67, 91].\n",
      "4.4\n",
      "Summary of Trends in Long Document Summarization Systems\n",
      "Table 2 summarizes the trends and developments in long document summarization models as\n",
      "discussed above. The two standout base architectures that are used in the long document sum-\n",
      "marization domains are graph-based ranking algorithm for unsupervised extractive models and\n",
      "pre-trained Transformer for supervised abstractive models. While both architectures were ini-\n",
      "tially proposed and tested on short documents, they can be effectively adapted to summarize long\n",
      "documents after incorporating novel mechanisms.3\n",
      "Finding 1. Graph-based Extractive Models with Discourse Bias: Classical graph-based\n",
      "unsupervised extractive models have been found to suffer from picking similar sentences that\n",
      "results in a summary with redundant sentences [69]. To this end, HipoRank [25] implements the\n",
      "graph-based architecture for unsupervised extractive summarization by including the sectional\n",
      "information of ArXiv/PubMed as inductive bias when calculating centrality scoring to achieve\n",
      "state-of-the-art results. The discourse bias mechanism is commonly incorporated by other proposed\n",
      "summarization models, including models with RNN and Transformer base architectures.\n",
      "3For a brief description of each model, please refer to the Supplementary Materials.\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:18\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "Model\n",
      "Architecture\n",
      "Pre-Train\n",
      "Long Document Mechanism\n",
      "Max Token\n",
      "Unsupervised\n",
      "Extractive\n",
      "PacSum [135]\n",
      "Graph\n",
      "BERT\n",
      "Discourse Bias\n",
      "-\n",
      "HipoRank [25]\n",
      "Graph\n",
      "BERT\n",
      "Discourse Bias\n",
      "-\n",
      "FAR [69]\n",
      "Graph\n",
      "BERT\n",
      "Facet-Aware Scoring\n",
      "-\n",
      "IBSumm [55]\n",
      "Pipeline\n",
      "SciBERT\n",
      "Signal Guidance\n",
      "-\n",
      "Supervised\n",
      "Extractive\n",
      "GlobalLocal [118]\n",
      "RNN\n",
      "-\n",
      "Discourse Bias\n",
      "Sent-CLF/PTR [94]\n",
      "RNN\n",
      "-\n",
      "-\n",
      "-\n",
      "Topic-GraphSum [22]\n",
      "GAT\n",
      "BERT\n",
      "Neural Topic Modelling\n",
      "-\n",
      "SSM-DM [21]\n",
      "DMN\n",
      "BERT\n",
      "-\n",
      "-\n",
      "Supervised\n",
      "Abstractive\n",
      "Discourse-Aware [20]\n",
      "RNN\n",
      "-\n",
      "Discourse Bias\n",
      "-\n",
      "Longformer [2]\n",
      "Transformer\n",
      "BART\n",
      "Efficient Attention\n",
      "16,384\n",
      "BigBird [127]\n",
      "Transformer\n",
      "PEGASUS\n",
      "Efficient Attention\n",
      "4,096\n",
      "GSUM [26]\n",
      "Transformer\n",
      "BART\n",
      "Signal Guidance\n",
      "4,096\n",
      "CRTLSum [45]\n",
      "Transformer\n",
      "BART\n",
      "Prompt Engineering\n",
      "1,024\n",
      "HAT-BART [99]\n",
      "Transformer\n",
      "BART\n",
      "Hierarchical Attention\n",
      "1,024\n",
      "HEPOS [49]\n",
      "Transformer\n",
      "BART\n",
      "Efficient Attention\n",
      "10,240\n",
      "Supervised\n",
      "Hybrid\n",
      "TLM+Ext [94]\n",
      "Transformer\n",
      "-\n",
      "Content Selection + Discourse Bias\n",
      "-\n",
      "DANCER [37]\n",
      "Transformer\n",
      "PEGASUS\n",
      "Content Selection + Discourse Bias\n",
      "-\n",
      "SEAL [134]\n",
      "Transformer\n",
      "-\n",
      "Content Selection w/ Segment-wise Scorer\n",
      "-\n",
      "LoBART [77]\n",
      "Transformer\n",
      "BART\n",
      "Content Selection + Efficient Attention\n",
      "-\n",
      "Table 2. Long Document Summarization Models in Chronological Order. Max token represents the maximum\n",
      "input sequence length that the model can process and any text that exceeds this cutoff point will be truncated.\n",
      "Finding 2. Pre-training task for Abstractive Summarization Models: Interestingly, despite\n",
      "having other pre-trained sequence-to-sequence models such as T5 [96], BART and PEGASUS are the\n",
      "only two Transformer-based pre-trained models that were used for long document summarization\n",
      "[66, 129]. Nevertheless, as both pre-trained models are trained on short documents, they have an\n",
      "input limit of 1,024 tokens. To process long documents that are longer than this limit, the pre-trained\n",
      "Transformers will have to incorporate long document mechanisms to extend the input limits.\n",
      "Finding 3. Long Document Mechanisms for Transformer: As pre-trained models were of-\n",
      "ten trained on large-scale datasets with input limit length between 512 to 1,024 [23, 66], these\n",
      "Transformer-based pre-trained models were optimized for short document language tasks rather\n",
      "than long documents. Without any long document mechanisms to adapt these models for the long\n",
      "document summarization task, Meng et al. [82] has shown that BART cannot summarize a long\n",
      "document effectively. Other than the discourse bias mechanism, we observe that (a) efficient atten-\n",
      "tion and (b) content selection mechanisms are the two most notable long document mechanisms.\n",
      "As the content selection mechanism requires a separate retriever to extract salient content from\n",
      "the source (i.e., the hybrid approach), we distinguish Transformer models with content selection\n",
      "mechanism as the retrieve-then-summarize model [131] and the pure encoder-decoder Transformer\n",
      "without this mechanism as an end-to-end model for the rest of this work. Lastly, it is also important\n",
      "to note that both mechanisms can be jointly implemented within a single architecture, where the\n",
      "content selection mechanism will extract a longer subset of input to be processed by a Transformer\n",
      "with efficient attention [77].\n",
      "5\n",
      "MULTI-DIMENSIONAL ANALYSIS OF LONG DOCUMENT SUMMARIZERS\n",
      "Given the important findings in the graph-based unsupervised extractive model and the Transformer-\n",
      "based supervised abstractive model in the previous section, we design an experiment with the aim\n",
      "of thoroughly understanding the reasons behind the popularity of these architectures and its mech-\n",
      "anisms. Our experiment tests out the graph-based extractive and Transformer-based abstractive\n",
      "summarization architectures and its mechanisms on the arXiv benchmark dataset. The documents\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:19\n",
      "in the arXiv dataset have an average length of 6,446 tokens. Mechanisms of the supervised extractive\n",
      "approach are not examined as the architectures used between the proposed models vary greatly.\n",
      "5.1\n",
      "Implementation\n",
      "5.1.1\n",
      "Graph - Unsupervised Extractive.\n",
      "To investigate the effect of incorporating long document discourse structure information, we\n",
      "experiment with four unsupervised graph models by varying the two following mechanisms:\n",
      "• Sentence Encoder Mechanism: Sentences of source text are encoded either using Term\n",
      "frequency–inverse document frequency (Tf-Idf) [53] or BERT SentenceTransformer [98].\n",
      "• Discourse Bias Mechanism: For both models that implement Tf-Idf or BERT sentence\n",
      "encoder, we experiment a model with the long document discourse bias mechanism and\n",
      "one without the bias. For models without a discourse bias mechanism, the centrality score\n",
      "of each sentence is computed based on the summation of cosine similarity between other\n",
      "sentences. For models with a discourse bias mechanism, the mechanism implemented follows\n",
      "the work of Dong et al. [25]. For each sentence, we adjust centrality score based on the\n",
      "sentence position within each section, 𝑐𝑖𝑛𝑡𝑟𝑎(𝑠𝑖), and the sentence’s section position within\n",
      "the document, 𝑐𝑖𝑛𝑡𝑒𝑟(𝑠𝑖). Sentences that are closer to the section and document boundaries\n",
      "will be given higher importance. The \"discourse-aware\" centrality score for each sentence is:\n",
      "𝑐(𝑠𝑖) = 𝜇1 · 𝑐𝑖𝑛𝑡𝑒𝑟(𝑠𝑖) + 𝑐𝑖𝑛𝑡𝑟𝑎(𝑠𝑖)\n",
      "where 𝜇1 is a weighting factor for inter-section centrality. Following the original author, we\n",
      "fine-tune the weighting factor based on validation set. To ensure comparability, the maximum\n",
      "length of summary for all unsupervised extractive models is set to be 242 tokens4.\n",
      "5.1.2\n",
      "Transformer - Supervised Abstractive.\n",
      "To study the current state-of-the-art abstractive neural summarizers, we experiment with two\n",
      "different pre-trained Transformers, BART and PEGASUS. For BART, we analyze the long document\n",
      "mechanisms from the perspective of two common approaches:\n",
      "• End-to-End: We experiment with three end-to-end BART models. Firstly, a vanilla BART\n",
      "model with full self-attention that will truncate any input text that exceed 1,024 tokens.\n",
      "Then, two BART models with efficient longformer attention [2] that can extend up to 4,096\n",
      "and 16,384 input tokens respectively. The main goal of assessing end-to-end BART with\n",
      "and without efficient attention is to assess how the quality of the generated summary is\n",
      "affected when BART is adapted from a short document summarizer into long document\n",
      "summarizer by allowing it to process long input sequences at the cost of full self-attention.\n",
      "For implementation, due to lack of computational resources, we only fine-tuned original\n",
      "BART on arXiv and obtain the weights of longformer that was trained on arXiv from the\n",
      "original author5.\n",
      "• Retrieve-then-summarize For BART retrieve-then-summarize model, our experiment fol-\n",
      "lows entirely the implementation of LoBART by [77]6. LoBART has two variants: (a) BART\n",
      "with full self-attention that takes in a selected subset of input text with a maximum length of\n",
      "1,024 tokens and (b) BART with efficient local attention where maximum length of subset\n",
      "input is 4,096 tokens. The only difference between the two variants is the amount of content\n",
      "to be retrieved from the original source text before feeding it into the Transformer BART\n",
      "model. The two main objectives of experimenting with the retrieve-then-summarize BART\n",
      "4Details of implementation are reported in the Supplementary Materials.\n",
      "5https://huggingface.co/allenai/led-large-16384-arxiv\n",
      "6https://github.com/potsawee/longsum0\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:20\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "models are to assess: (1) the effectiveness of content selection mechanism in adapting short\n",
      "document BART to summarize a long document and (2) whether the performance is improved\n",
      "when content selection mechanism is combined with efficient attention mechanism.\n",
      "As there is no existing framework that applies content selection mechanism on PEGASUS, we\n",
      "only experiment two end-to-end PEGASUS model: original PEGASUS7 and PEGASUS with BigBird\n",
      "efficient attention8. The models have input token limit of 1,024 and 4,096 respectively. Pre-trained\n",
      "weights for both model variants on arXiv benchmark are obtained directly from the original author.\n",
      "5.1.3\n",
      "Assessing Model Outputs.\n",
      "Rather than relying entirely on ROUGE score like most summarization research settings, we use\n",
      "four different metrics to analyse model summary outputs from three important dimensions (D#):\n",
      "relevance, informativeness and semantic coherence.\n",
      "• D1 - Relevance of a summary is the extent to which a summary contains the main ideas\n",
      "of a source. We use ROUGE [70] and BERTScore [130] metrics to measure relevancy of the\n",
      "candidate summary.\n",
      "• D2 - Informativeness is the amount of new information and knowledge a summary brings\n",
      "to the reader [92]. This information may not necessarily be key to the narrative of the source\n",
      "but should add value to readers. For example, limitations of an academic article are not central\n",
      "to the narrative but do add value to readers. This metric tests a model architecture’s ability\n",
      "to effectively generate summary that can cover different aspects of the original source text.\n",
      "This is approximated by the percentage of sections that are covered by a candidate summary,\n",
      "where we assume each sentence of the candidate summary covers a particular section and\n",
      "the sentence belongs to the section where it achieves the highest ROUGE-L score.\n",
      "• D3 - Semantic Coherence measures whether a summary is fluent and semantically coher-\n",
      "ent. Following Bommasani and Cardie [4]’s implementation, this is approximated as:\n",
      "𝑆𝐶(𝑆) =\n",
      "Í||𝑆||\n",
      "𝑖=2 𝑁𝑆𝑃(𝑠𝑖|𝑠𝑖−1)\n",
      "||𝑆||\n",
      "where 𝑁𝑆𝑃(.) is BERT NSP function, and 𝑠𝑖denotes the position of a sentence in the candidate\n",
      "summary. However, Bommasani and Cardie [4] did not fine-tune the general pre-trained\n",
      "BERT model while our BERT NSP model is fine-tuned on arXiv using positive and negative\n",
      "sentence-pairs with a final F1-score of 0.92.\n",
      "5.1.4\n",
      "Other Implementation Details.\n",
      "For all the model variants implemented, the train, validation and test sample split on the arXiv\n",
      "benchmark dataset are 203,037/6,436/6,440, which is the same for all prior works as they follow\n",
      "the same configuration by the original author [20]. To ensure consistent preprocessing pipeline,\n",
      "we follow pre-processing of LoBART in all model implementations [77]. All models that require\n",
      "fine-tuning are trained on the same RTX 3090 GPU with 24 GiB of GPU memory. We use pyrouge\n",
      "package for ROUGE metric.\n",
      "5.2\n",
      "Results and Analysis\n",
      "The following discusses the experimental findings based on the results shown in Table 3.\n",
      "7https://github.com/google-research/pegasus\n",
      "8https://github.com/google-research/bigbird\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:21\n",
      "Architecture and its Mechanisms\n",
      "Relevance\n",
      "Informativeness\n",
      "Semantic Coherence\n",
      "R-1\n",
      "R-2\n",
      "R-L\n",
      "BERTScore\n",
      "Unsupervised Extractive - Graph\n",
      "Tf-Idf\n",
      "0.344\n",
      "0.095\n",
      "0.285\n",
      "0.822\n",
      "0.355\n",
      "0.779\n",
      "BERT\n",
      "0.351\n",
      "0.098\n",
      "0.290\n",
      "0.819\n",
      "0.398\n",
      "0.792\n",
      "Tf-Idf + Discourse Bias\n",
      "0.357\n",
      "0.113\n",
      "0.311\n",
      "0.820\n",
      "0.447\n",
      "0.677\n",
      "BERT + Discourse Bias\n",
      "0.361\n",
      "0.112\n",
      "0.322\n",
      "0.822\n",
      "0.513\n",
      "0.667\n",
      "Supervised Abstractive - BART Variants\n",
      "End-to-End (Max Token)\n",
      "BART-only (1,024)\n",
      "0.413\n",
      "0.153\n",
      "0.368\n",
      "0.846\n",
      "0.331\n",
      "0.851\n",
      "BART+LongformerAttn (4,096)\n",
      "0.463\n",
      "0.187\n",
      "0.412\n",
      "0.852\n",
      "0.361\n",
      "0.855\n",
      "BART+LongformerAttn (16,384)\n",
      "0.467\n",
      "0.196\n",
      "0.418\n",
      "0.865\n",
      "0.433\n",
      "0.877\n",
      "Retrieve-then-summarize\n",
      "BART+CS\n",
      "0.472\n",
      "0.193\n",
      "0.419\n",
      "0.837\n",
      "0.403\n",
      "0.812\n",
      "BART+LocalAttn+CS\n",
      "0.486\n",
      "0.201\n",
      "0.422\n",
      "0.845\n",
      "0.427\n",
      "0.835\n",
      "Supervised Abstractive - PEGASUS Variants\n",
      "End-to-End (Max Token)\n",
      "PEGASUS (1,024)\n",
      "0.439\n",
      "0.171\n",
      "0.381\n",
      "0.857\n",
      "0.333\n",
      "0.863\n",
      "PEGASUS+BigBirdAttn (4,096)\n",
      "0.462\n",
      "0.190\n",
      "0.415\n",
      "0.855\n",
      "0.359\n",
      "0.869\n",
      "Table 3. Experimental Results of Graph-based Unsupervised Extractive and Transformer-based Supervised\n",
      "Abstractive. The best results are in boldface, and the second highest scores are underlined. Max token\n",
      "represents the maximum input length where texts that exceed this cutoff point are truncated. LocalAttn\n",
      "represents local attention where each token only attends to its neighbouring 1,024 tokens. LongformerAttn\n",
      "and BigBirdAttn represents efficient attention variants proposed by Longformer [2] and BigBird [127].\n",
      "5.2.1\n",
      "Graph - Unsupervised Extractive.\n",
      "Finding 1. Global Word Representation versus Contextual Embedding: Using BERT as\n",
      "the sentence encoder mechanism boosts the unsupervised summarization model performance in\n",
      "the relevancy and informativeness dimension. We hypothesize this is due to the semantic reasoning\n",
      "capability of BERT in encoding important sentences that are informative but are not worded in the\n",
      "same way as the other important sentences when similarity and centrality scoring are computed.\n",
      "This is particularly important for long document as it has higher compression ratio and a higher\n",
      "chance of having sentences with the exact same information being repeated multiple times in the\n",
      "source text. Thus, not encoding the sentences with semantically rich encoding may result in a\n",
      "summary with more redundancy.\n",
      "Finding 2. Discourse Bias Mechanism boosts Relevance and Informativeness: The inclu-\n",
      "sion of positional and sectional bias when computing sentence centrality score greatly improves\n",
      "the architecture’s ability in capturing more relevant sentences in the long document and generating\n",
      "a more informative summary, validating our hypothesis in section 4. However, since extractive\n",
      "models merely combine the extracted sentences, these sentences that come from various sections\n",
      "likely caused a drop in the semantic coherence of the final summary outputs.\n",
      "5.2.2\n",
      "Transformer - Supervised Abstractive.\n",
      "Finding 1. Diminishing Return of PEGASUS Pre-training: As compared to the BART-only\n",
      "model, the PEGASUS-only model achieved greater performance across all dimensions, indicating\n",
      "that PEGASUS pre-training mechanism helps a Transformer-based model in writing more relevant,\n",
      "informative and semantically coherent summaries. As PEGASUS is pre-trained on a different corpus\n",
      "as compared to BART [66, 129], it is not conclusive whether the GSG pre-training task and/or\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:22\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "the difference in the pre-training corpus contributed to the superior performance of PEGASUS.\n",
      "Interestingly, the performance gain is not as obvious when the input sequence length is allowed to\n",
      "be extended to 4,096 with efficient attention. This could be due to the difference in the efficient\n",
      "attention mechanism used or the need for predicting salient content outside the truncated text has\n",
      "diminished.\n",
      "Finding 2. Mixed Results on Retrieve-then-summarize Models: For both retrieve-then-\n",
      "summarize BART models, we see state-of-the-art result achieved in terms of the standard ROUGE\n",
      "score metric. The result showed improvement across all dimensions when the Transformer model\n",
      "processed a longer subset extracted by the retriever, demonstrating the effectiveness of Transformer\n",
      "in computing pairwise relations between tokens to identify salient content. Except for the BART\n",
      "with longformer attention (16,384), retrieve-then-summarize models also performed better in\n",
      "informativeness as the models are allowed to process the entire source documents in arXiv dataset.\n",
      "However, the retrieve-then-summarize models performed the worst in semantic coherence when\n",
      "compared to the other abstractive summarization models. As the content selection mechanism is\n",
      "not trained in an end-to-end manner, we hypothesize that this is due to the inevitable disconnect\n",
      "between the content selection mechanism and the encoder-decoder Transformer model at the\n",
      "inference stage. Further, it is possible when the retrieved subset extracted by the content selection\n",
      "mechanism are not ordered in its original form, the incoherence of the subset cascade downwards\n",
      "to the final summary output, causing a drop in semantic coherence. This finding also illustrates\n",
      "the importance of measuring model performance in a multi-dimensional way rather than relying\n",
      "entirely on ROUGE score that has found to have important limitations [3, 61].\n",
      "Finding 3. Transformer’s Reasoning Capability over Long Sequences: Holding the pre-\n",
      "trained BART model constant, extending the total input token limits for the pre-trained Transformer\n",
      "improves the summarizer’s ability in generating a summary that is more relevant, informative\n",
      "and semantically coherent. This finding is consistent with a human evaluation experiment by\n",
      "[49], providing confidence to the automatic evaluation metrics used in this work. The impact of\n",
      "processing only 1,024 tokens is particularly obvious when it comes to the informativeness of the\n",
      "summary output where BART (1,024) informativeness score is 10 points lower than BART (16,384).\n",
      "Importantly, ROUGE score again did not fully capture this performance difference, highlighting the\n",
      "limitation of traditional summarization research setting of measuring model performance using\n",
      "only ROUGE score. Lastly, this finding suggests that unlike the result of Meng et al. [82], our\n",
      "experiment demonstrates that Transformer can reason over long sequences given that the right\n",
      "configuration is made to fine-tuned the model for specific downstream task.\n",
      "Through ad-hoc experiment, we systematically analyze the common approaches in long document\n",
      "summarization domain. The experimental result demonstrated that exploiting explicit discourse\n",
      "structures of long documents in unsupervised models and processing longer inputs with long\n",
      "document adaptation on pre-trained Transformer models can yield promising outcomes for the\n",
      "long document summarization task. The result also showed that retrieve-then-summarize model can\n",
      "achieve state-of-the-art results in terms of ROUGE score but may generate less coherent summaries.\n",
      "5.3\n",
      "Limitation of Experiment\n",
      "Recent studies have found that summary outputs of state-of-the-art abstractive summarization\n",
      "models contain factual inconsistency in up to 30% of summary output [8, 62, 81]. To address the\n",
      "aforementioned issues, various models and metrics have been proposed to measure the factual\n",
      "consistency of candidate summaries conditioned on the source documents [27, 40]. Nonetheless,\n",
      "due to the limitations of the proposed metrics including the input length limit of pre-trained\n",
      "models, difficulty of implementation and performance variation across benchmarks [89], we did not\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:23\n",
      "measure the factual consistency of summary outputs and represents an important limitation of the\n",
      "multi-dimensional analysis experiment above. This is despite after trying out various adaptations on\n",
      "the textual entailment approach proposed by Maynez et al. [81], our tested models have almost no\n",
      "discriminative ability and was thus not used. The robustness of the metrics used across the relevance,\n",
      "informativeness and semantic coherence dimensions should also be interpreted with care. Lastly,\n",
      "the experiment was only conducted using the arXiv benchmark dataset as it is the only dataset\n",
      "where all pre-trained weights are publicly available. To encourage similar analysis to be conducted\n",
      "across a wide range of benchmark dataset and model implementation, our evaluation metric toolkit\n",
      "for dataset and model is available at https://github.com/huankoh/long-doc-summarization.\n",
      "6\n",
      "METRICS\n",
      "As evaluating generated summary outputs using manual efforts are costly and impractical, the\n",
      "efficient ROUGE metric [70] has long been the standard way of comparing summarization model\n",
      "performance. It measures the lexical overlap between reference and candidate summary and the\n",
      "common n-gram measures are unigram (ROUGE-1), bigram (ROUGE-2) and longest common\n",
      "sub-sequence (ROUGE-L). However, as it is based on exact token matches and overlap between\n",
      "synonymous tokens or phrases will be ignored, the limitation of ROUGE score metrics have been\n",
      "widely explored [3, 11, 44, 61] and many have also attempted to propose more comprehensive\n",
      "content overlap metrics using soft semantic overlap [33, 130]. Further, while content overlap is\n",
      "the fundamental objective of summarization, the quality of a summary, as Gehrmann et al. [36]\n",
      "and Peyrard [92] suggested, should be measured in a multi-dimensional way including relevance,\n",
      "factual consistency, conciseness and semantic coherence. Relevance refers to whether the candidate\n",
      "summary contains the main ideas. Factual consistency metric measures whether a candidate\n",
      "summary is factually consistent with the source document. Conciseness measure whether important\n",
      "information is encapsulated in a short and brief manner. Semantic coherence relates to the collective\n",
      "quality and fluency of summary sentences. Based on these quality aspects, the following discusses\n",
      "the research efforts in the wider summarization domain with a focus on the long document\n",
      "summarization research settings at the end of this section.\n",
      "6.1\n",
      "Relevance\n",
      "A) Hard Lexical Overlap\n",
      "As mentioned above, ROUGE score is an efficient way to consider content overlap through hard\n",
      "lexical matching between candidate summary and the ground truth summary. However, as ROUGE\n",
      "only considers exact matching between reference summary and model output, it (a) will penalize\n",
      "models that coin novel wordings and phrases that do not match the wordings in the reference\n",
      "summary, (b) does not consider factual consistency between the model output and the source\n",
      "document and (c) does not directly consider fluency and conciseness of a summary. Finally, ROUGE\n",
      "score also goes against the human approach of clever paraphrasing and summarizing.\n",
      "B) Soft Content Overlap\n",
      "To solve the problem of exact matching of lexical units, Zhang et al. [130] proposes a model that\n",
      "measures soft overlap between the reference and candidate summary by comparing the contextual\n",
      "BERT embeddings of both summaries. Other variants of this idea include MoverScore [133], Word\n",
      "Mover Similarity and an extension of it, Sentence Mover Similarity [18, 65]. The soft content\n",
      "overlap metrics often rely substantially on the encoder used to vectorized the candidate and ground\n",
      "truth summary. BERTScore, for example, utilizes BERT as the fundamental pre-trained model to\n",
      "encode its representations. While BERT has been proven to perform amazingly well under many\n",
      "different benchmark settings, its performance under certain domains such as legal or scientific\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:24\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "research has not been thoroughly explored. For example, Beltagy et al. [1] fine-tuned BERT on large-\n",
      "scale scientific paper datasets and have found its performance to improve in scientific domains as\n",
      "compared to the BERT-base model. Evidenced by Tejaswin et al. [111]’s experimental result where\n",
      "BERTScore is found not to discriminate summaries with and without errors well, this questions the\n",
      "use of BERT-base model as the \"independent evaluator\" of candidate summaries across all domains.\n",
      "C) Reference-free Approach\n",
      "Rather than measuring the quality of candidate summary based on a ground truth summary,\n",
      "reference-free metrics for relevance measure the quality of candidate summary based on pseudo-\n",
      "reference summaries that are generated from source documents. Wu et al. [117]’s proposed metric\n",
      "requires training samples of high-quality summaries for model supervision while other reference-\n",
      "free metrics can generate metric scores without the use of high-quality summaries as supervisory\n",
      "signals [14, 35]. In section 3 of benchmark datasets, we see that the information covered by a\n",
      "reference summary depends on the data annotation approach as well as the intent of the original\n",
      "authors. We further observe that reference summaries of certain benchmark datasets contain\n",
      "significant noises. If supervised summarization models were to train on datasets with similar issues,\n",
      "they may fit on target summaries that are inconsistent with the expectation and needs of summary\n",
      "readers. A reference-free approach that can bypass the requirement of ground truth summaries\n",
      "would thus be beneficial to the development of models in cases where there is high heterogeneity\n",
      "in summary reader expectation and/or lack of ground truth summary labels. Nevertheless, the use\n",
      "case of reference-free metrics is often limited by the fact that they still require pseudo-reference\n",
      "summaries to be generated by an \"independent model\". Last but not least, the reference-free\n",
      "approach can also be used to augment the reference-based metrics [46].\n",
      "6.2\n",
      "Factual Consistency\n",
      "Widespread factual inconsistency in abstractive summarization model outputs greatly limit the\n",
      "potentiality of these abstractive models to be applied in most commercial settings. To this end,\n",
      "automated metrics on factual consistency have been proposed by others [27, 62, 81, 114], which can\n",
      "be categorized into two different approaches: Entailment Classification and Question Answering.\n",
      "A) Entailment Classification Approach\n",
      "The entailment classification approach evaluates the factual inconsistency of a candidate summary\n",
      "by breaking down the summary into smaller units (e.g., phrases/sentences) to be verified against\n",
      "the original document. For example, FactCC [62] implements a BERT-based factual consistency\n",
      "classifier that is trained on synthetic data, where the positive data labels are non-paraphrased and\n",
      "paraphrased sentences from the original source document, and the negative labels are artificially\n",
      "corrupted sentences from the source document. At the inference stage, the faithfulness score\n",
      "for a candidate summary is the number of consistent sentences divided by the total number of\n",
      "summary sentences. Similarly, other proposed models implement factual consistency classifiers by\n",
      "incorporating structured knowledge such as OpenIE triples [39] or dependency arc [40]. For the\n",
      "classifiers to be effective in discriminating the factual consistency of a candidate summary, they\n",
      "often require supervisory signals from factually consistent and inconsistent data [89].\n",
      "B) Question-Answering Approach\n",
      "The Question-Answering (QA) approach employs a question-generation model to generate questions\n",
      "from a given summary output [27, 114]. The generated questions are then answered in two different\n",
      "ways: i) answering the question conditioning on the source text and ii) answering the question\n",
      "conditioning on the summary output. If the answers match between the source text and candidate\n",
      "summary, the answer is then considered consistent, otherwise, it is inconsistent. The final score will\n",
      "be based on discrepancies between the answers generated conditional on the candidate summary\n",
      "and the answers generated conditional on the souce document. Recently, QAGen [85] proposes to\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:25\n",
      "generate questions and answers from a given text concurrently within a single model to evaluate\n",
      "factual consistency to improve the efficiency of this approach.\n",
      "Other Important Studies:\n",
      "It is important to note that the aforementioned works consider factual consistency as a binary\n",
      "outcome. In contrast, FRANK [89] advocates for a multi-dimensional approach to evaluate factual\n",
      "consistency based on semantic error, discourse error and content verifiability error. Through\n",
      "substantial human annotation, the study further found that the effectiveness of metrics is found to\n",
      "be extremely dependent on the types of architecture measured and the benchmark dataset used.\n",
      "Similarly, human evaluation experiments from previous works have shown conflicting and varying\n",
      "results in the desired approach of developing factuality metrics [81, 85]. In response, Gabriel et al.\n",
      "[31] proposed five conditions for the development of an effective factuality metric to encourage\n",
      "better standardization in the factual consistency metric research.\n",
      "6.3\n",
      "Conciseness and Semantic Coherence\n",
      "Metrics to measure other aspects of summarization such as conciseness and semantic coherence\n",
      "were also introduced. As they are not as crucial as relevance and factual consistency, these metrics\n",
      "often complement the others to allow a metric or a model to be more holistic and practical. For\n",
      "example, Bommasani and Cardie [4] considers semantic coherence of reference summaries when\n",
      "evaluating single document benchmark datasets while Ju et al. [55]’s unsupervised model generates\n",
      "fluent summary by utilizing the next sentence prediction task in BERT. Metrics for conciseness are\n",
      "also introduced to measure the quality of summaries [4, 14].\n",
      "6.4\n",
      "Research Efforts on Metrics in the Long Document Domain\n",
      "Many recently proposed metrics incorporate pre-trained architectures to achieve better perfor-\n",
      "mances. However, as argued in the model discussion above, these pre-trained architectures cannot\n",
      "be easily extended to long documents. As an illustration, our experiment has attempted various\n",
      "adaptations9 on a BERT textual entailment model to evaluate arXiv candidate summaries but has\n",
      "found it not effective in discriminating a summary’s factual consistency with the source. This is\n",
      "despite Maynez et al. [81]’s finding that this model best correlates with the human judgment of\n",
      "factual consistency on the XSUM short document dataset. Furthermore, other than the difficulty of\n",
      "adapting these models on long documents, Nan et al. [85] has also identified the issue of resource\n",
      "efficiency, where a competing model would take approximately 4 days to evaluate a CNN-DM\n",
      "test set with an NVIDIA V100 Tensor Core GPU and would likely take significantly longer under\n",
      "any long document benchmark datasets. Consequently, the need of re-designing the proposed\n",
      "evaluation models and the requirement for costly computation resources have likely discouraged\n",
      "the adoption of factual consistency assessment models in the long document summarization domain.\n",
      "Looking at the broader research on evaluation metrics of summarization as a whole, for 17 different\n",
      "research papers related to evaluation metrics published in ACL main conferences10 from 2015 to\n",
      "September 2021, there were no discussion on the evaluation metrics in the context of long document\n",
      "summarization datasets. This is important as Pagnoni et al. [89] has found that the effectiveness of\n",
      "proposed metrics to vary based on the dataset characteristics. In sum, unlike the quick adoption\n",
      "of short document practices in the model architectures space, research in exploring evaluation\n",
      "metrics within the context of long document summarization is lacking and may potentially hold\n",
      "back the future progression of long document summarization.\n",
      "9Result details are in the Supplementary Materials.\n",
      "10ACL main conferences are ACL, NAACL, EACL, EMNLP, CoNLL, and AACL. Papers are listed in the Supplementary\n",
      "Materials.\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:26\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "7\n",
      "APPLICATIONS\n",
      "As the quality of long document summaries generated by state-of-the-art models continues to\n",
      "improve, past works have explored their feasibility in the research and industrial domains. A natural\n",
      "extension for models that were implemented on the scientific paper benchmark, arXiv/PubMed, is to\n",
      "employ it for research purposes. These include writing section-structured [82], user-specific [45] or\n",
      "presentation-based [107] summaries for scientific papers, automating scientific reviewing [126], and\n",
      "even generating literature survey based on multiple biomedical long scientific papers [24]. When\n",
      "it comes to the general industrial applications of long document summarization, the knowledge\n",
      "and techniques learned from the research domain can address numerous commercial tasks. On the\n",
      "surface level, any information that would be expressed in a textual format would benefit from the\n",
      "advancement in this field, which encompasses summarizing any forms of long textual documents\n",
      "[75, 104], extracting content as feature snippets for search engines11, writing reviews for long media\n",
      "content [63] and summarizing long dialogues [16, 71, 136, 138] and multi-modal content [125].\n",
      "With the development becomes increasingly mature in the real-world settings, summarization\n",
      "models are now commercialized as a Software-as-a-Service (Saas) product in the news12, business13\n",
      "and consulting14 domains. Furthermore, as the long document summarization task can be generally\n",
      "understood as identification of important aspects from long sequences, the positive spillover from\n",
      "successful model implementation in this domain can affect a wide range of domains. Long document\n",
      "summarization models, for example, can be utilized for auxiliary tasks such as video captioning [72],\n",
      "long document question-answering [76] or multi-modal tasks [68, 86]. Liu et al. [73] also identified\n",
      "the \"unexpected side-effect\" of language model reliably learned how to transliterate names between\n",
      "languages, despite the fact that the model was trained to summarize long Wikipedia articles, while\n",
      "BigBird [127] applies Transformer-based models designed for long sequences not only to long\n",
      "document summarization but also to DNA promoter region and chromatin profile prediction tasks\n",
      "in the genomics research domain.\n",
      "8\n",
      "GENERAL CHALLENGES AND FUTURE DIRECTIONS\n",
      "This section discusses the general challenges of long document summarization that have yet to\n",
      "be solved and pinpoints potential future research directions to attract practitioners’ attention\n",
      "and improve our understanding and techniques in the long document summarization domain.\n",
      "Advancement in the long document summarization domain should also give rise to beneficial\n",
      "spillover to closely-related NLP sub-domains such as multi-hop QA, information retrieval and\n",
      "reading comprehension.\n",
      "8.1\n",
      "Neural Models and Long Sequence Reasoning\n",
      "While there have been significant efforts in solving the time and memory complexity of a neural\n",
      "architecture such as Transformers to enhance model efficiencies, the understanding of a model’s\n",
      "effectiveness in solving different NLP tasks or domains is limited. As shown by our model experiment\n",
      "and result findings of others [2, 49, 127], fine-tuning pre-trained models using efficient Transformers\n",
      "that can attend to larger input size of tokens can improve the model performances across a wide\n",
      "range of NLP tasks. However, the underlying reasons of the performance improvement is not well\n",
      "understood. For example, while Transformer models have found to outperform RNN models as\n",
      "RNN lacks the ability to reason over long sequences, Pagnoni et al. [89] have found that pre-trained\n",
      "11https://developers.google.com/search/docs/advanced/appearance/featured-snippets\n",
      "12https://ai.baidu.com/tech/nlp_apply/news_summary\n",
      "13https://quillbot.com/\n",
      "14https://www.datagrand.com/about-us/\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:27\n",
      "Transformer summarization models still make a similar amount of discourse-related errors as the\n",
      "RNN models. Furthermore, research in the effectiveness of various efficient attention mechanisms\n",
      "used by a Transformer to summarize long documents also showed varying results. On the one\n",
      "hand, Huang et al. [49] showed that efficient attention with learnable patterns to significantly\n",
      "outperform the the efficient attention with fixed patterns such as local-only attention mechanism.\n",
      "On the other hand, Manakul and Gales [77] have found that extending window size of efficient\n",
      "Transformers to increase number of attended tokens per token do not affect the average distance of\n",
      "attended neighbor, suggesting that local attention to neighboring tokens will be sufficient for the\n",
      "long document summarization task. Altogether, these results highlight the limited understanding\n",
      "on the strategies employed by current neural models to summarize long document and the need of\n",
      "further research to enhance our understanding on this issue.\n",
      "8.2\n",
      "Summarizer with Automatic Discourse Parsers/Annotator\n",
      "Our experimental result has demonstrated that the simple unsupervised graph architecture outper-\n",
      "forms the other unsupervised models when discourse bias of arXiv section information is included.\n",
      "Nonetheless, information regarding sections of a document may not always be of high quality\n",
      "or available for a summarization model. This limits the implementation of many long doument\n",
      "summarization models that require explicit section-based discourse information. Similar issue has\n",
      "been faced by researchers in the dialogue summarization domain where discourse level information\n",
      "is not provided and past work in this domain have achieved state-of-the-art results by incorpo-\n",
      "rating effective automatic discourse annotators [30, 71, 116]. An architecture that can effectively\n",
      "incorporate automated discourse parsers or annotators would thus be a fruitful direction for long\n",
      "document summarization researchers to explore.\n",
      "8.3\n",
      "End-to-end Neural Summarizer with Content Selection Mechanism\n",
      "In the medium term, in spite of the expected progress in computing efficiencies, there exist a\n",
      "significant amount of long documents such as business reports and books that have tokens that\n",
      "exceed hundreds of thousand [63, 75]. Thus, it is not possible to summarize the entire document\n",
      "using a powerful state-of-the-art model without any long document adaptation, as it will truncates\n",
      "most of the long document source text given the current input length limit. A more practical\n",
      "direction is to explore architectures with a content selection mechanism that has shown to be\n",
      "effective in long document summarization [77, 134]. Zhao et al. [134] has proposed an end-to-end\n",
      "long document summarization framework using transformers but did not incorporate powerful\n",
      "pre-trained models and performed slightly worse than other state-of-the-art models. LoBART [77],\n",
      "on the other hand, did not design the content selection and abstractive summarizer in an end-to-end\n",
      "manner. Experimental result in this survey has shown that LoBART’s disconnection between the\n",
      "retriever and summarizer resulted in less semantically coherent summaries. In the open-domain\n",
      "QA domain, RAG [67] achieves state-of-the-art by successfully incorporating content selection\n",
      "mechanism and pre-trained models in an end-to-end manner [91], pointing a promising direction\n",
      "for practitioners in the long document summarization domain to explore.\n",
      "8.4\n",
      "Quality and Diversity of Benchmark Dataset\n",
      "In section 3 of benchmark datasets, human annotation efforts have been done to measure the\n",
      "quality of the most commonly used long document summarization dataset, arXiv. It was found\n",
      "that 60% of reference summaries contain some form of errors and 15% of them have significant\n",
      "errors where at least half of the summary contains errors. This calls for a benchmark dataset with\n",
      "significantly better quality with fewer errors through robust heuristic rules and scraping strategies.\n",
      "Moreover, the long document summarization benchmark datasets are often in the legislative and\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:28\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "scientific domain. While these domains are extremely important, many other domains such as\n",
      "financial reports with significant numerical complexity or long-form dialogues of daily conversation\n",
      "in business settings are equally important. Development across different domains could attract\n",
      "even greater attention from a wide range of partners to incentivize greater research efforts in the\n",
      "summarization field. Last but not least, to achieve the original objectives of benchmark datasets,\n",
      "proposed model architectures for long document summarization should also be tested across a\n",
      "diverse set of long document benchmark datasets rather than focusing merely on arXiv/PubMed.\n",
      "8.5\n",
      "Practicality of Summarization Metrics\n",
      "The limitation of ROUGE metric that has been widely explored [33, 61, 88, 137] and significant efforts\n",
      "have been made to improve the way we measure candidate summaries from various different aspects.\n",
      "Nonetheless, the proposed methods lack practicality in terms of wide availability for all parties\n",
      "in the research communities. For example, Nan et al. [85] found that using a single NVIDIA V100\n",
      "Tensor Core GPU, a factual consistency metric proposed requires longer than four days to evaluate a\n",
      "single set of candidate summaries in the CNN-DM test dataset. Many metrics proposed also require\n",
      "substantial computing resources to re-train across different benchmark settings [27, 62, 114]. These\n",
      "issues will be exacerbated when it comes to the long document summarization domain. Moreover,\n",
      "most summarization metrics are only tested in the CNN-DM and XSum datasets but not others.\n",
      "This significantly limits its applicability as Pagnoni et al. [89] have found most metrics to lack\n",
      "robustness across different benchmark settings. To ensure effective metrics have wider application,\n",
      "efficiencies and practicality of metrics should be paid with great attention to ensure that sufficient\n",
      "incentive is provided for practitioners to explore the practicality of metrics rather than a mere\n",
      "focus on state-of-the-art metric performances.\n",
      "9\n",
      "CONCLUSION\n",
      "In this survey, we conduct a comprehensive overview of long document summarization and\n",
      "systematically analyze the three key components of its research settings: benchmark datasets,\n",
      "summarization models and evaluation metrics. We first highlight the intrinsic differences of short\n",
      "and long document datasets and show that summarizing long documents requires extra compression\n",
      "of the source text through the identification of key narratives that are more uniformly scattered\n",
      "across the source documents. Nevertheless, long documents are often more extractive in nature and\n",
      "often have explicit discourse structures to take advantage of. For summarization models, we provide\n",
      "a thorough review, comparison and summarization of the model architectures and mechanisms\n",
      "used to generate long document summaries. Through ad-hoc experiment, we also systematically\n",
      "investigate the architectures and mechanisms that are widely applied across various works. We\n",
      "further discuss the current research in evaluation metrics and call attention to the lack of research\n",
      "on metrics that can be easily applied to the long document summarization domain. Finally, we\n",
      "explore the applications of long document summarization models and suggest five future directions\n",
      "for long document summarization research.\n",
      "REFERENCES\n",
      "[1] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language Model for Scientific Text. In Proceedings\n",
      "of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference\n",
      "on Natural Language Processing (EMNLP-IJCNLP). 3615–3620.\n",
      "[2] Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint\n",
      "arXiv:2004.05150 (2020).\n",
      "[3] Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Re-evaluating evaluation in\n",
      "text summarization. arXiv preprint arXiv:2010.07100 (2020).\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:29\n",
      "[4] Rishi Bommasani and Claire Cardie. 2020. Intrinsic Evaluation of Summarization Datasets. In Proceedings of the 2020\n",
      "Conference on Empirical Methods in Natural Language Processing (EMNLP). 8075–8096.\n",
      "[5] Ravali Boorugu and G Ramesh. 2020. A Survey on NLP based Text Summarization for Summarizing Product Reviews.\n",
      "In 2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA). IEEE, 352–356.\n",
      "[6] Florian Boudin and Emmanuel Morin. 2013. Keyphrase Extraction for N-best Reranking in Multi-Sentence Com-\n",
      "pression. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies. Association for Computational Linguistics, Atlanta, Georgia, 298–305.\n",
      "[7] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\n",
      "Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint\n",
      "arXiv:2005.14165 (2020).\n",
      "[8] Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive\n",
      "summarization. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.\n",
      "[9] Jean Carletta, Simone Ashby, Sebastien Bourban, Mike Flynn, Mael Guillemot, Thomas Hain, Jaroslav Kadlec, Vasilis\n",
      "Karaiskos, Wessel Kraaij, Melissa Kronenthal, et al. 2005. The AMI meeting corpus: A pre-announcement. In\n",
      "International workshop on machine learning for multimodal interaction. Springer, 28–39.\n",
      "[10] Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and Yejin Choi. 2018. Deep Communicating Agents for Abstractive\n",
      "Summarization. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies, Volume 1 (Long Papers). 1662–1675.\n",
      "[11] Arun Chaganty, Stephen Mussmann, and Percy Liang. 2018. The price of debiasing automatic metrics in natural\n",
      "language evalaution. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume\n",
      "1: Long Papers). Association for Computational Linguistics, Melbourne, Australia, 643–653.\n",
      "[12] Yllias Chali, Sadid A Hasan, and Shafiq R Joty. 2009. A SVM-based ensemble approach to multi-document summariza-\n",
      "tion. In Canadian Conference on Artificial Intelligence. Springer, 199–202.\n",
      "[13] Danqi Chen, Jason Bolton, and Christopher D Manning. 2016. A Thorough Examination of the CNN/Daily Mail\n",
      "Reading Comprehension Task. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers). 2358–2367.\n",
      "[14] Wang Chen, Piji Li, and Irwin King. 2021. A Training-free and Reference-free Summarization Evaluation Metric via\n",
      "Centrality-weighted Relevance and Self-referenced Redundancy. In Proceedings of the 59th Annual Meeting of the\n",
      "Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing\n",
      "(Volume 1: Long Papers). Association for Computational Linguistics, Online, 404–414.\n",
      "[15] Jianpeng Cheng and Mirella Lapata. 2016. Neural summarization by extracting sentences and words. arXiv preprint\n",
      "arXiv:1603.07252 (2016).\n",
      "[16] Bharath Chintagunta, Namit Katariya, Xavier Amatriain, and Anitha Kannan. 2021. Medically Aware GPT-3 as a\n",
      "Data Generator for Medical Dialogue Summarization. In Proceedings of the Second Workshop on Natural Language\n",
      "Processing for Medical Conversations. 66–76.\n",
      "[17] Janara Christensen, Stephen Soderland, Oren Etzioni, et al. 2013. Towards coherent multi-document summarization.\n",
      "In Proceedings of the 2013 conference of the North American chapter of the association for computational linguistics:\n",
      "Human language technologies. 1163–1173.\n",
      "[18] Elizabeth Clark, Asli Celikyilmaz, and Noah A Smith. 2019. Sentence mover’s similarity: Automatic evaluation\n",
      "for multi-sentence texts. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.\n",
      "2748–2760.\n",
      "[19] Ann Clifton, Sravana Reddy, Yongze Yu, Aasish Pappu, Rezvaneh Rezapour, Hamed Bonab, Maria Eskevich, Gareth\n",
      "Jones, Jussi Karlgren, Ben Carterette, and Rosie Jones. 2020. 100,000 Podcasts: A Spoken English Document Cor-\n",
      "pus. In Proceedings of the 28th International Conference on Computational Linguistics. International Committee on\n",
      "Computational Linguistics, Barcelona, Spain (Online), 5903–5917.\n",
      "[20] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian.\n",
      "2018. A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents. In Proceedings of the\n",
      "2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\n",
      "Technologies, Volume 2 (Short Papers). 615–621.\n",
      "[21] Peng Cui and Le Hu. 2021. Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long\n",
      "Documents. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies. 5881–5891.\n",
      "[22] Peng Cui, Le Hu, and Yuanchao Liu. 2020. Enhancing Extractive Text Summarization with Topic-Aware Graph Neural\n",
      "Networks. In Proceedings of the 28th International Conference on Computational Linguistics. 5360–5371.\n",
      "[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\n",
      "Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter\n",
      "of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:30\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "4171–4186.\n",
      "[24] Jay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, and Lucy Wang. 2021. MSˆ2: Multi-Document\n",
      "Summarization of Medical Studies. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\n",
      "Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 7494–7513.\n",
      "[25] Yue Dong, Andrei Mircea Romascanu, and Jackie Chi Kit Cheung. 2021. Discourse-Aware Unsupervised Summarization\n",
      "for Long Scientific Documents. In Proceedings of the 16th Conference of the European Chapter of the Association for\n",
      "Computational Linguistics: Main Volume. 1089–1102.\n",
      "[26] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. 2021. GSum: A General Framework\n",
      "for Guided Neural Abstractive Summarization. In Proceedings of the 2021 Conference of the North American Chapter of\n",
      "the Association for Computational Linguistics: Human Language Technologies. 4830–4842.\n",
      "[27] Esin Durmus, He He, and Mona Diab. 2020. FEQA: A Question Answering Evaluation Framework for Faithfulness As-\n",
      "sessment in Abstractive Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational\n",
      "Linguistics. 5055–5070.\n",
      "[28] Wafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea, and Hoda K Mohamed. 2021. Automatic text summarization: A\n",
      "comprehensive survey. Expert Systems with Applications 165 (2021), 113679.\n",
      "[29] Günes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization.\n",
      "Journal of artificial intelligence research 22 (2004), 457–479.\n",
      "[30] Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, and Ting Liu. 2021. Language Model as an Annotator: Exploring\n",
      "DialoGPT for Dialogue Summarization. In Proceedings of the 59th Annual Meeting of the Association for Computational\n",
      "Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).\n",
      "Association for Computational Linguistics, Online, 1479–1491.\n",
      "[31] Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, and Jianfeng Gao. 2021. GO FIGURE: A Meta Evaluation of\n",
      "Factuality in Summarization. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association\n",
      "for Computational Linguistics, Online, 478–487.\n",
      "[32] Mahak Gambhir and Vishal Gupta. 2017. Recent automatic text summarization techniques: a survey. Artificial\n",
      "Intelligence Review 47, 1 (2017), 1–66.\n",
      "[33] Kavita Ganesan. 2018. ROUGE 2.0: Updated and Improved Measures for Evaluation of Summarization Tasks. arXiv\n",
      "preprint arXiv:1803.01937 (2018).\n",
      "[34] Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making Pre-trained Language Models Better Few-shot Learners.\n",
      "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International\n",
      "Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics,\n",
      "Online, 3816–3830.\n",
      "[35] Yang Gao, Wei Zhao, and Steffen Eger. 2020. SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics\n",
      "for Multi-Document Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational\n",
      "Linguistics. Association for Computational Linguistics, Online, 1347–1354.\n",
      "[36] Sebastian Gehrmann, Yuntian Deng, and Alexander M Rush. 2018. Bottom-Up Abstractive Summarization. In\n",
      "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 4098–4109.\n",
      "[37] Alexios Gidiotis and Grigorios Tsoumakas. 2020. A divide-and-conquer approach to the summarization of long\n",
      "documents. IEEE/ACM Transactions on Audio, Speech, and Language Processing 28 (2020), 3029–3040.\n",
      "[38] Yihong Gong and Xin Liu. 2001. Generic text summarization using relevance measure and latent semantic analysis.\n",
      "In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information\n",
      "retrieval. 19–25.\n",
      "[39] Ben Goodrich, Vinay Rao, Peter J Liu, and Mohammad Saleh. 2019. Assessing the factual accuracy of generated text.\n",
      "In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 166–175.\n",
      "[40] Tanya Goyal and Greg Durrett. 2020. Evaluating Factuality in Generation with Dependency-level Entailment. In\n",
      "Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics,\n",
      "Online, 3592–3603.\n",
      "[41] Yvette Graham. 2015. Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE. In Proceedings\n",
      "of the 2015 conference on empirical methods in natural language processing. 128–137.\n",
      "[42] Matt Grenander, Yue Dong, Jackie Chi Kit Cheung, and Annie Louis. 2019. Countering the Effects of Lead Bias in\n",
      "News Summarization via Multi-Stage Training and Auxiliary Losses. In Proceedings of the 2019 Conference on Empirical\n",
      "Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing\n",
      "(EMNLP-IJCNLP). 6019–6024.\n",
      "[43] Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse\n",
      "extractive strategies. arXiv preprint arXiv:1804.11283 (2018).\n",
      "[44] Tatsunori B. Hashimoto, Hugh Zhang, and Percy Liang. 2019. Unifying Human and Statistical Evaluation for\n",
      "Natural Language Generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:31\n",
      "for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for\n",
      "Computational Linguistics, Minneapolis, Minnesota, 1689–1701.\n",
      "[45] Junxian He, Wojciech Kryściński, Bryan McCann, Nazneen Rajani, and Caiming Xiong. 2020. Ctrlsum: Towards\n",
      "generic controllable text summarization. arXiv preprint arXiv:2012.04281 (2020).\n",
      "[46] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: A Reference-free\n",
      "Evaluation Metric for Image Captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural\n",
      "Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic,\n",
      "7514–7528.\n",
      "[47] Wan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui Min, Jing Tang, and Min Sun. 2018. A Unified Model for\n",
      "Extractive and Abstractive Summarization using Inconsistency Loss. In Proceedings of the 56th Annual Meeting of the\n",
      "Association for Computational Linguistics (Volume 1: Long Papers). 132–141.\n",
      "[48] Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, and Yue Zhang. 2020. What Have We\n",
      "Achieved on Text Summarization?. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\n",
      "Processing (EMNLP). 446–469.\n",
      "[49] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient Attentions for Long Document\n",
      "Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies. 1419–1436.\n",
      "[50] Kokil Jaidka, Muthu Kumar Chandrasekaran, Sajal Rustagi, and Min-Yen Kan. 2016. Overview of the CL-SciSumm\n",
      "2016 Shared Task. In Proceedings of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural\n",
      "Language Processing for Digital Libraries (BIRNDL). 93–102.\n",
      "[51] Adam Janin, Don Baron, Jane Edwards, Dan Ellis, David Gelbart, Nelson Morgan, Barbara Peskin, Thilo Pfau, Elizabeth\n",
      "Shriberg, Andreas Stolcke, et al. 2003. The ICSI meeting corpus. In 2003 IEEE International Conference on Acoustics,\n",
      "Speech, and Signal Processing, 2003. Proceedings.(ICASSP’03)., Vol. 1. IEEE, I–I.\n",
      "[52] Yangfeng Ji and Jacob Eisenstein. 2014. Representation learning for text-level discourse parsing. In Proceedings of the\n",
      "52nd annual meeting of the association for computational linguistics (volume 1: Long papers). 13–24.\n",
      "[53] Karen Sparck Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of\n",
      "documentation (1972).\n",
      "[54] Jiaxin Ju, Ming Liu, Longxiang Gao, and Shirui Pan. 2020. Monash-Summ@ LongSumm 20 SciSummPip: An\n",
      "Unsupervised Scientific Paper Summarization Pipeline. In Proceedings of the First Workshop on Scholarly Document\n",
      "Processing. 318–327.\n",
      "[55] Jiaxin Ju, Ming Liu, Huan Yee Koh, Yuan Jin, Lan Du, and Shirui Pan. 2021. Leveraging Information Bottleneck\n",
      "for Scientific Document Summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021.\n",
      "4091–4098.\n",
      "[56] Byeongchang Kim, Hyunwoo Kim, and Gunhee Kim. 2019. Abstractive Summarization of Reddit Posts with Multi-\n",
      "level Memory Networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2519–2531.\n",
      "[57] Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to\n",
      "sentence compression. Artificial Intelligence 139, 1 (2002), 91–107.\n",
      "[58] Hayato Kobayashi, Masaki Noguchi, and Taichi Yatsuka. 2015. Summarization based on embedding distributions. In\n",
      "Proceedings of the 2015 conference on empirical methods in natural language processing. 1984–1989.\n",
      "[59] Anastassia Kornilova and Vlad Eidelman. 2019. BillSum: A Corpus for Automatic Summarization of US Legislation.\n",
      "EMNLP-IJCNLP 2019 (2019), 48.\n",
      "[60] Mahnaz Koupaee and William Yang Wang. 2018. WikiHow: A Large Scale Text Summarization Dataset. ArXiv\n",
      "abs/1810.09305 (2018).\n",
      "[61] Wojciech Kryściński, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural Text\n",
      "Summarization: A Critical Evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\n",
      "Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 540–551.\n",
      "[62] Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the Factual Consistency\n",
      "of Abstractive Text Summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\n",
      "Processing (EMNLP). Association for Computational Linguistics, Online, 9332–9346.\n",
      "[63] Wojciech Kryściński, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2021. BookSum: A\n",
      "Collection of Datasets for Long-form Narrative Summarization. arXiv preprint arXiv:2105.08209 (2021).\n",
      "[64] Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of the 18th\n",
      "annual international ACM SIGIR conference on Research and development in information retrieval. 68–73.\n",
      "[65] Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word embeddings to document distances.\n",
      "In International conference on machine learning. PMLR, 957–966.\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:32\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "[66] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov,\n",
      "and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,\n",
      "Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational\n",
      "Linguistics. 7871–7880.\n",
      "[67] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler,\n",
      "Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp\n",
      "tasks. In NeurIPS.\n",
      "[68] Jiazheng Li, Linyi Yang, Barry Smyth, and Ruihai Dong. 2020. MAEC: A multimodal aligned earnings conference\n",
      "call dataset for financial risk prediction. In Proceedings of the 29th ACM International Conference on Information &\n",
      "Knowledge Management. 3063–3070.\n",
      "[69] Xinnian Liang, Shuangzhi Wu, Mu Li, and Zhoujun Li. 2021. Improving unsupervised extractive summarization with\n",
      "facet-aware modeling. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 1685–1697.\n",
      "[70] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out.\n",
      "74–81.\n",
      "[71] Chunyi Liu, Peng Wang, Jiang Xu, Zang Li, and Jieping Ye. 2019. Automatic dialogue summary generation for\n",
      "customer service. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data\n",
      "Mining. 1957–1965.\n",
      "[72] Hui Liu and Xiaojun Wan. 2021. Video Paragraph Captioning as a Text Summarization Task. In Proceedings of the\n",
      "59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on\n",
      "Natural Language Processing (Volume 2: Short Papers). 55–60.\n",
      "[73] Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018.\n",
      "Generating Wikipedia by Summarizing Long Sequences. In International Conference on Learning Representations.\n",
      "[74] Yang Liu and Mirella Lapata. 2019. Text Summarization with Pretrained Encoders. In Proceedings of the 2019 Conference\n",
      "on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language\n",
      "Processing (EMNLP-IJCNLP). 3730–3740.\n",
      "[75] Lefteris Loukas, Manos Fergadiotis, Ion Androutsopoulos, and Prodromos Malakasiotis. 2021. EDGAR-CORPUS:\n",
      "Billions of Tokens Make The World Go Round. arXiv:2109.14394 [cs.CL]\n",
      "[76] Chenyang Lyu, Lifeng Shang, Yvette Graham, Jennifer Foster, Xin Jiang, and Qun Liu. 2021. Improving Unsupervised\n",
      "Question Answering via Summarization-Informed Question Generation. In Proceedings of the 2021 Conference on\n",
      "Empirical Methods in Natural Language Processing. 4134–4148.\n",
      "[77] Potsawee Manakul and Mark Gales. 2021. Long-span summarization via local attention and content selection. In\n",
      "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint\n",
      "Conference on Natural Language Processing (Volume 1: Long Papers). 6026–6041.\n",
      "[78] Inderjeet Mani and Eric Bloedorn. 1998. Machine learning of generic and user-focused summarization. In AAAI/IAAI.\n",
      "821–826.\n",
      "[79] William C Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text\n",
      "organization. Text-interdisciplinary Journal for the Study of Discourse 8, 3 (1988), 243–281.\n",
      "[80] Yuning Mao, Liyuan Liu, Qi Zhu, Xiang Ren, and Jiawei Han. 2020. Facet-Aware Evaluation for Extractive Summa-\n",
      "rization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 4941–4957.\n",
      "[81] Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On Faithfulness and Factuality in\n",
      "Abstractive Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\n",
      "1906–1919.\n",
      "[82] Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, and Daqing He. 2021. Bringing\n",
      "Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents. In Proceedings of the 59th\n",
      "Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural\n",
      "Language Processing (Volume 2: Short Papers). Association for Computational Linguistics, Online, 1080–1089.\n",
      "[83] Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into text. In Proceedings of the 2004 conference on\n",
      "empirical methods in natural language processing. 404–411.\n",
      "[84] Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Ça glar Gulçehre, and Bing Xiang. 2016. Abstractive Text\n",
      "Summarization using Sequence-to-sequence RNNs and Beyond. In Proceedings of The 20th SIGNLL Conference on\n",
      "Computational Natural Language Learning. 280–290.\n",
      "[85] Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen McKeown, Ramesh Nallapati, Dejiao\n",
      "Zhang, Zhiguo Wang, Andrew O. Arnold, and Bing Xiang. 2021. Improving Factual Consistency of Abstractive\n",
      "Summarization via Question Answering. In Proceedings of the 2021 Conference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics,\n",
      "Online.\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:33\n",
      "[86] Medhini Narasimhan, Anna Rohrbach, and Trevor Darrell. 2021. CLIP-It! language-guided video summarization. In\n",
      "Thirty-Fifth Conference on Neural Information Processing Systems.\n",
      "[87] Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018. Don’t Give Me the Details, Just the Summary! Topic-Aware\n",
      "Convolutional Neural Networks for Extreme Summarization. In Proceedings of the 2018 Conference on Empirical\n",
      "Methods in Natural Language Processing. 1797–1807.\n",
      "[88] Jun Ping Ng and Viktoria Abrecht. 2015. Better Summarization Evaluation with Word Embeddings for ROUGE. In\n",
      "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 1925–1930.\n",
      "[89] Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding Factuality in Abstractive\n",
      "Summarization with FRANK: A Benchmark for Factuality Metrics. In Proceedings of the 2021 Conference of the North\n",
      "American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for\n",
      "Computational Linguistics, Online, 4812–4829.\n",
      "[90] Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A Deep Reinforced Model for Abstractive Summarization.\n",
      "In International Conference on Learning Representations.\n",
      "[91] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine\n",
      "Jernite, Vladimir Karpukhin, Jean Maillard, et al. 2021. KILT: a Benchmark for Knowledge Intensive Language Tasks.\n",
      "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:\n",
      "Human Language Technologies. 2523–2544.\n",
      "[92] Maxime Peyrard. 2019. A Simple Theoretical Model of Importance for Summarization. In Proceedings of the 57th\n",
      "Annual Meeting of the Association for Computational Linguistics. 1059–1073.\n",
      "[93] Maxime Peyrard. 2019. Studying summarization evaluation metrics in the appropriate scoring range. In Proceedings\n",
      "of the 57th Annual Meeting of the Association for Computational Linguistics. 5093–5100.\n",
      "[94] Jonathan Pilault, Raymond Li, Sandeep Subramanian, and Christopher Pal. 2020. On extractive and abstractive neural\n",
      "document summarization with transformer language models. In Proceedings of the 2020 Conference on Empirical\n",
      "Methods in Natural Language Processing (EMNLP). 9308–9319.\n",
      "[95] Vahed Qazvinian and Dragomir Radev. 2008. Scientific Paper Summarization Using Citation Summary Networks. In\n",
      "Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008). 689–696.\n",
      "[96] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\n",
      "Peter J Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of\n",
      "Machine Learning Research 21 (2020), 1–67.\n",
      "[97] Revanth Rameshkumar and Peter Bailey. 2020. Storytelling with Dialogue: A Critical Role Dungeons and Dragons\n",
      "Dataset. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for\n",
      "Computational Linguistics, Online, 5121–5134.\n",
      "[98] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In\n",
      "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint\n",
      "Conference on Natural Language Processing (EMNLP-IJCNLP). 3982–3992.\n",
      "[99] Tobias Rohde, Xiaoxia Wu, and Yinhan Liu. 2021. Hierarchical learning for generation with long source sequences.\n",
      "arXiv preprint arXiv:2104.07545 (2021).\n",
      "[100] Sascha Rothe, Joshua Maynez, and Shashi Narayan. 2021. A Thorough Evaluation of Task-Specific Pretraining for\n",
      "Summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association\n",
      "for Computational Linguistics, Online and Punta Cana, Dominican Republic, 140–145.\n",
      "[101] Alexander M Rush, Sumit Chopra, and Jason Weston. 2015. A Neural Attention Model for Abstractive Sentence\n",
      "Summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 379–389.\n",
      "[102] Evan Sandhaus. 2008. The new york times annotated corpus. Linguistic Data Consortium, Philadelphia 6, 12 (2008),\n",
      "e26752.\n",
      "[103] Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get To The Point: Summarization with Pointer-Generator\n",
      "Networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\n",
      "Papers). 1073–1083.\n",
      "[104] Eva Sharma, Chen Li, and Lu Wang. 2019. BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent\n",
      "Summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2204–2213.\n",
      "[105] Tian Shi, Yaser Keneshloo, Naren Ramakrishnan, and Chandan K Reddy. 2021. Neural abstractive text summarization\n",
      "with sequence-to-sequence models. ACM Transactions on Data Science 2, 1 (2021), 1–37.\n",
      "[106] K Shivakumar and Rab Soumya. 2015. Text summarization using clustering technique and SVM technique. International\n",
      "Journal of Applied Engineering Research 10, 12 (2015), 28873–28881.\n",
      "[107] Edward Sun, Yufang Hou, Dakuo Wang, Yunfeng Zhang, and Nancy XR Wang. 2021. D2S: Document-to-Slide\n",
      "Generation Via Query-Based Text Summarization. In Proceedings of the 2021 Conference of the North American Chapter\n",
      "of the Association for Computational Linguistics: Human Language Technologies. 1405–1418.\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:34\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "[108] Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. oLMpics-On What Language Model Pre-training\n",
      "Captures. Transactions of the Association for Computational Linguistics 8 (2020), 743–758.\n",
      "[109] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian\n",
      "Ruder, and Donald Metzler. 2020. Long Range Arena: A Benchmark for Efficient Transformers. In International\n",
      "Conference on Learning Representations.\n",
      "[110] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2022. Efficient Transformers: A Survey. ACM Comput.\n",
      "Surv. (apr 2022). Just Accepted.\n",
      "[111] Priyam Tejaswin, Dhruv Naik, and Pengfei Liu. 2021. How well do you know your summarization datasets? arXiv\n",
      "preprint arXiv:2106.11388 (2021).\n",
      "[112] Lucy Vanderwende, Hisami Suzuki, Chris Brockett, and Ani Nenkova. 2007. Beyond SumBasic: Task-focused\n",
      "summarization with sentence simplification and lexical expansion. Information Processing & Management 43, 6 (2007),\n",
      "1606–1618.\n",
      "[113] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\n",
      "Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information\n",
      "Processing Systems. 6000–6010.\n",
      "[114] Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and Answering Questions to Evaluate the Factual\n",
      "Consistency of Summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\n",
      "5008–5020.\n",
      "[115] Matt Wilber, William Timkey, and Marten van Schijndel. 2021. To Point or Not to Point: Understanding How\n",
      "Abstractive Summarizers Paraphrase Text. In Findings of the Association for Computational Linguistics: ACL-IJCNLP\n",
      "2021. Association for Computational Linguistics, Online, 3362–3376.\n",
      "[116] Chien-Sheng Wu, Linqing Liu, Wenhao Liu, Pontus Stenetorp, and Caiming Xiong. 2021. Controllable Abstractive\n",
      "Dialogue Summarization with Sketch Supervision. In Findings of the Association for Computational Linguistics:\n",
      "ACL-IJCNLP 2021. Association for Computational Linguistics, Online, 5108–5122.\n",
      "[117] Hanlu Wu, Tengfei Ma, Lingfei Wu, Tariro Manyumwa, and Shouling Ji. 2020. Unsupervised Reference-Free Summary\n",
      "Quality Evaluation via Contrastive Learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural\n",
      "Language Processing (EMNLP). Association for Computational Linguistics, Online, 3612–3621.\n",
      "[118] Wen Xiao and Giuseppe Carenini. 2019. Extractive Summarization of Long Documents by Combining Global and\n",
      "Local Context. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\n",
      "International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 3011–3021.\n",
      "[119] Jiacheng Xu, Shrey Desai, and Greg Durrett. 2020. Understanding Neural Abstractive Summarization Models via\n",
      "Uncertainty. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).\n",
      "6275–6281.\n",
      "[120] Jiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Discourse-Aware Neural Extractive Text Summarization. In\n",
      "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 5021–5031.\n",
      "[121] Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R Fabbri, Irene Li, Dan Friedman, and Dragomir R Radev.\n",
      "2019. Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with\n",
      "citation networks. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 7386–7393.\n",
      "[122] Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek, Krishnan Srinivasan, and Dragomir Radev. 2017.\n",
      "Graph-based Neural Multi-Document Summarization. In Proceedings of the 21st Conference on Computational Natural\n",
      "Language Learning (CoNLL 2017). 452–462.\n",
      "[123] Mark Yatskar. 2019. A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC. In Proceedings of the 2019 Conference\n",
      "of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume\n",
      "1 (Long and Short Papers). 2318–2323.\n",
      "[124] Dani Yogatama, Fei Liu, and Noah A Smith. 2015. Extractive summarization by maximizing semantic volume. In\n",
      "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 1961–1966.\n",
      "[125] Tiezheng Yu, Wenliang Dai, Zihan Liu, and Pascale Fung. 2021. Vision Guided Generative Pre-trained Language\n",
      "Models for Multimodal Abstractive Summarization. In Proceedings of the 2021 Conference on Empirical Methods in\n",
      "Natural Language Processing. 3995–4007.\n",
      "[126] Weizhe Yuan, Pengfei Liu, and Graham Neubig. 2021. Can We Automate Scientific Reviewing? arXiv preprint\n",
      "arXiv:2102.00176 (2021).\n",
      "[127] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip\n",
      "Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big Bird: Transformers for Longer Sequences.. In NeurIPS.\n",
      "[128] Fang-Fang Zhang, Jin-ge Yao, and Rui Yan. 2018. On the abstractiveness of neural document summarization. In\n",
      "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 785–790.\n",
      "[129] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. Pegasus: Pre-training with extracted gap-sentences\n",
      "for abstractive summarization. In International Conference on Machine Learning. PMLR, 11328–11339.\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:35\n",
      "[130] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. BERTScore: Evaluating Text\n",
      "Generation with BERT. In International Conference on Learning Representations.\n",
      "[131] Yusen Zhang, Ansong Ni, Tao Yu, Rui Zhang, Chenguang Zhu, Budhaditya Deb, Asli Celikyilmaz, Ahmed Hassan\n",
      "Awadallah, and Dragomir Radev. 2021. An Exploratory Study on Long Dialogue Summarization: What Works and\n",
      "What’s Next. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Findings.\n",
      "(2021).\n",
      "[132] Jinming Zhao, Ming Liu, Longxiang Gao, Yuan Jin, Lan Du, He Zhao, He Zhang, and Gholamreza Haffari. 2020.\n",
      "SummPip: Unsupervised Multi-Document Summarization with Sentence Graph Compression. In Proceedings of the\n",
      "43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. 1949–1952.\n",
      "[133] Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Steffen Eger. 2019. MoverScore: Text Generation\n",
      "Evaluating with Contextualized Embeddings and Earth Mover Distance. In Proceedings of the 2019 Conference on\n",
      "Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language\n",
      "Processing (EMNLP-IJCNLP). 563–578.\n",
      "[134] Yao Zhao, Mohammad Saleh, and Peter J Liu. 2020.\n",
      "Seal: Segment-wise extractive-abstractive long-form text\n",
      "summarization. arXiv preprint arXiv:2006.10213 (2020).\n",
      "[135] Hao Zheng and Mirella Lapata. 2019. Sentence Centrality Revisited for Unsupervised Summarization. In Proceedings\n",
      "of the 57th Annual Meeting of the Association for Computational Linguistics. 6236–6247.\n",
      "[136] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz,\n",
      "Yang Liu, Xipeng Qiu, and Dragomir Radev. 2021. QMSum: A New Benchmark for Query-based Multi-domain\n",
      "Meeting Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association\n",
      "for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Online,\n",
      "5905–5921.\n",
      "[137] Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu, and Eduard Hovy. 2006. ParaEval: Using Paraphrases to\n",
      "Evaluate Summaries Automatically. In Proceedings of the Human Language Technology Conference of the NAACL, Main\n",
      "Conference. Association for Computational Linguistics, New York City, USA, 447–454.\n",
      "[138] Chenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng. 2021. MediaSum: A Large-scale Media Interview Dataset for\n",
      "Dialogue Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Technologies. 5927–5934.\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:36\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "10\n",
      "SUPPLEMENTARY MATERIALS\n",
      "10.1\n",
      "Long Document Summarization Systems\n",
      "Table 4 details the summary of long document baseline and state-of-the-art summarization systems\n",
      "proposed by previous works in this domain. The \"Prior\" column illustrates whether inductive bias\n",
      "such as discourse structure information of the original long document is used and the \"Trunc\"\n",
      "column reflects the total percentage of significant truncation across the five long document bench-\n",
      "mark.\n",
      "Model\n",
      "Description\n",
      "Prior\n",
      "Trunc\n",
      "Unsupervised\n",
      "Baseline\n",
      "Extractive\n",
      "LSA [38]\n",
      "Singular Value Decomposition + Term Frequency of Sentence Matrix\n",
      "-\n",
      "-\n",
      "TextRank [83]\n",
      "Sentence Graph Centrality (PageRank) + Similarity(common words)\n",
      "-\n",
      "-\n",
      "LexRank [29]\n",
      "Sentence Graph Centrality (PageRank) + Similarity(Tf-Idf)\n",
      "-\n",
      "-\n",
      "SumBasic [112]\n",
      "Average Word-Occurrence Probability of Sentence\n",
      "-\n",
      "-\n",
      "Unsupervised\n",
      "Neural\n",
      "Extractive\n",
      "PacSum [135]\n",
      "Sentence Graph Centrality + Similarity(BERT)\n",
      "-\n",
      "-\n",
      "HipoRank [25]\n",
      "Hierarchical Section-Sentence Graph Centrality + Similarity(BERT)\n",
      "Section\n",
      "-\n",
      "FAR [69]\n",
      "Sentence Graph Centrality + Similarity(BERT) + Facet Aware Candidate Search\n",
      "-\n",
      "-\n",
      "Supervised\n",
      "Neural\n",
      "Extractive\n",
      "Sent-CLF/PTR [94]\n",
      "Bi-LSTM (word & sentence level) w/o pretraining\n",
      "-\n",
      "-\n",
      "GlobalLocal [118]\n",
      "GloVe (word/sentence level) + Bi-LSTM (section & document level)\n",
      "Section\n",
      "-\n",
      "Topic-GraphSum [22]\n",
      "BERT (sentence level) + Bipartite GAT (NTM topic-sentence)\n",
      "-\n",
      "UNK\n",
      "SSM-DM [21]\n",
      "BERT (sentence level) + Sliding Selector Network (Memory Network & GAT)\n",
      "-\n",
      "-\n",
      "Supervised\n",
      "Neural\n",
      "Abstractive\n",
      "Discourse-Aware [20]\n",
      "BiLSTM (word- & section- level) w/o pretraining\n",
      "Section\n",
      "-\n",
      "Pegasus [129]\n",
      "Pretraining Summarization Task (GSG) on C4/HugeNews + Data-specific fine-tuning\n",
      "-\n",
      "85%\n",
      "CRTLSum [45]\n",
      "Fine-tuned BART with keywords prompt-engineering\n",
      "-\n",
      "85%\n",
      "BigBird [127]\n",
      "Pretrained Pegasus with Sparse Attention (Local + Global + Random)\n",
      "-\n",
      "20%\n",
      "Longformer [2]\n",
      "Fine-tuned BART with SparseAttention (Local + Global)\n",
      "-\n",
      "-\n",
      "HEPOS [49]\n",
      "Fine-tuned BART with Efficient Attentions (LSH/Sinkhorn + Hepos)\n",
      "-\n",
      "2%\n",
      "Supervised\n",
      "Neural\n",
      "Hybrid\n",
      "TLM+Ext [94]\n",
      "Bi-LSTM for ContentSelection + Transformer-based Decoder w/o pretraining\n",
      "-\n",
      "-\n",
      "DANCER [37]\n",
      "Pretrained Pegasus Section-by-Section Summarization\n",
      "Section\n",
      "-\n",
      "SEAL [134]\n",
      "End-to-end Transformer-based Content Selector + Decoder w/o pretraining\n",
      "-\n",
      "-\n",
      "LoBART [77]\n",
      "Multi-task RNN for ContentSelection + Fine-tuned BART with SparseAttention (Local)\n",
      "-\n",
      "-\n",
      "Table 4. Summary of Long Document Baseline and State-of-the-Art Summarization Systems.\n",
      "10.2\n",
      "Graph-based Ranking Algorithm in Experimental Section\n",
      "In general form, given a set of sentences in the original source document, 𝐷= {𝑠1,𝑠2, ...,𝑠𝑚}\n",
      "with the inter-sentential similarity relations represented as 𝑒𝑖𝑗= (𝑠𝑖,𝑠𝑗) ∈𝐸where 𝑖≠𝑗, the\n",
      "following equation illustrates the graph-based ranking architecture in computing the scoring for\n",
      "each sentence:\n",
      "𝑐𝑒𝑛𝑡𝑟𝑎𝑙𝑖𝑡𝑦(𝑠𝑖) =\n",
      "∑︁\n",
      "𝑗∈{1,...,𝑚},𝑖≠𝑗\n",
      "𝑒𝑖𝑗∗𝐵𝑖𝑎𝑠(𝑒𝑖𝑗)\n",
      "The similarity between each sentence is computed using similarity measures such as cosine\n",
      "similarity after being encoded using a sentence encoder. The graph architecture we implemented\n",
      "is a basic directed graph where the centrality score of each sentence is computed based on the\n",
      "summation of bias-adjusted cosine similarity between other sentences and/or sections. The section\n",
      "node will still be represented by sentences where it is the average of the representations for\n",
      "sentences within the section of interest. In other words, a sentence that has the highest sum of\n",
      "similarity against all the other sentences after adjusting for bias will be ranked as the top sentence.\n",
      "Tf-Idf Only. For Tf-Idf encoding, we use scikitlearn Tf-Idf vectorizer to train the encoder using\n",
      "source documents in the arXiv test set (note: this does not include the reference summaries). The\n",
      "preprocessing follows [77] to ensure consistency and the minimum document frequency is set to\n",
      "be 0.01. As the vector dimension based on original Tf-Idf will be huge, we reduce the dimension to\n",
      "768 using TruncatedSVD.\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:37\n",
      "BERT-only. For BERT encoding, we utilize SentenceTransformer package (https://www.sbert.net/)\n",
      "where the model used is \"bert-base-nli-mean-tokens\".\n",
      "Tf-Idf + Long Document Discourse Bias. Following the implementation of [25], the bias is calculated\n",
      "using intra-section bias (or position-level bias within each section) and inter-section bias (or section-\n",
      "level bias). For sentences within the same section, the cosine-similarity between sentences adjusted\n",
      "by intra-section bias, 𝑒𝑖𝑛𝑡𝑟𝑎𝐵\n",
      "𝑖𝑗\n",
      ", is computed by adjusting the lambda biases, 𝜆1 and 𝜆2 based on\n",
      "sentence boundary function, 𝑑𝑏:\n",
      "𝑒𝑖𝑛𝑡𝑟𝑎𝐵\n",
      "𝑖𝑗\n",
      "=\n",
      "\u001a 𝑠𝑖𝑚(𝑠𝐼\n",
      "𝑗,𝑠𝐼\n",
      "𝑖) ∗𝜆1,𝑖𝑓𝑑𝑏(𝑠𝐼\n",
      "𝑖) ≥𝑑𝑏(𝑠𝐼\n",
      "𝑗)\n",
      "𝑠𝑖𝑚(𝑠𝐼\n",
      "𝑗,𝑠𝐼\n",
      "𝑖) ∗𝜆2,𝑖𝑓𝑑𝑏(𝑠𝐼\n",
      "𝑖) < 𝑑𝑏(𝑠𝐼\n",
      "𝑗)\n",
      "where the sentence boundary function, 𝑑𝑏, will determine sentences 𝑠𝐼\n",
      "𝑖that are closer to the section\n",
      "𝐼boundaries to be more important. This is computed by:\n",
      "𝑑𝑏(𝑠𝐼\n",
      "𝑖) = 𝑚𝑖𝑛(𝑥𝐼\n",
      "𝑖, 𝛼(𝑛𝐼−𝑥𝐼\n",
      "𝑖))\n",
      "𝑛𝐼is the number of sentences in section 𝐼and 𝑥𝐼\n",
      "𝑖represents sentence i’s position in section I. Cosine\n",
      "similarity between sentence and section adjusted by inter-section bias is calculated similarly except\n",
      "that the bias is computed based on the section’s position in the document. Finally, the resulting\n",
      "adjusted centrality score for each sentence is:\n",
      "𝑐(𝑠𝐼\n",
      "𝑖) = 𝜇1 · 𝑐𝑖𝑛𝑡𝑒𝑟(𝑠𝐼\n",
      "𝑖) + 𝑐𝑖𝑛𝑡𝑟𝑎(𝑠𝐼\n",
      "𝑖)\n",
      "where 𝜇1 is a weighting factor for inter-section centrality.\n",
      "For hyperparameter tuning, we set 𝜆1 = 0.5 and 𝜆2 = 1. Then, using arXiv validation samples, we\n",
      "adjust 𝛼∈{0, 0.5, 0.8, 1.0, 1.2} to control the relative importance of the start and end of a section of a\n",
      "source document and 𝜇1 ∈{0.5, 1.0, 1.5} to control the weights of intra-section sentence importance\n",
      "versus inter-section sectional importance. Importantly, the original paper uses 𝜆1 = 0 where less\n",
      "important sentences are pruned, while we set 𝜆1 = 0.5 to down weight rather than prune the less\n",
      "important sentences. For more details, we refer our reader to the original paper [25] with their\n",
      "codes available on https://github.com/mirandrom/HipoRank.\n",
      "BERT + Long Document Discourse Bias. Same as Tf-Idf except that the Tf-Idf sentence encoder is\n",
      "replaced by BERT.\n",
      "Lastly, for the implementation of Transformer-based abstractive summarization models, the links\n",
      "to original author’s pre-trained weights, codes and implementations are provided in the footnote\n",
      "of our main article.\n",
      "10.3\n",
      "BERT NSP - Assessing Semantic Coherence of Candidate Summaries\n",
      "Bommasani and Cardie [4] suggested using the general pre-trained BERT model to evaluate the\n",
      "semantic coherence or fluency of a summary without any fine-tuning. We find the general pre-\n",
      "trained BERT model that was not trained on academic papers to have little discriminative ability\n",
      "for the semantic coherence of arXiv-related summaries. Thus, we fine-tune the pre-trained BERT\n",
      "model using positive and negative sentence pairs. Positive sentences are extracted by taking any\n",
      "sentence together with its following sentence in the reference summary and source text of the arXiv\n",
      "dataset. Negative sentences are created by replacing the following sentences with any randomly\n",
      "extracted sentences from either the same or other documents in the arXiv benchmark dataset. We\n",
      "also ensured that the total samples of positive and negative sentence pairs are balanced and an\n",
      "equal amount of sentences are obtained from the reference summary and the source text. The codes\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "1:38\n",
      "Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan\n",
      "of all the metrics used in the main paper, including BERT NSP and intrinsic characteristics of the\n",
      "dataset, are made publicly available on: https://github.com/huankoh/long-doc-summarization.\n",
      "10.4\n",
      "Textual Entailment as Factual Consistency Metric\n",
      "Adaptation\n",
      "Good\n",
      "Bad\n",
      "Ordered R1\n",
      "0.03 /0.10 /0.86\n",
      "0.04 / 0.11 / 0.85\n",
      "Ordered R2\n",
      "0.05 / 0.11 / 0.84\n",
      "0.04 / 0.13 / 0.83\n",
      "Ordered RL\n",
      "0.04 / 0.12 / 0.84\n",
      "0.04 / 0.13 / 0.83\n",
      "Randomised R1\n",
      "0.04 / 0.25 / 0.71\n",
      "0.04 / 0.30 / 0.66\n",
      "Randomised R2\n",
      "0.06 / 0.21 / 0.73\n",
      "0.07 / 0.19 / 0.74\n",
      "Randomised RL\n",
      "0.05 / 0.25 / 0.70\n",
      "0.05 / 0.24 / 0.71\n",
      "Table 5. Textual Entailment Result (Entail/Contradict/Neutral) on annotated arXiv reference summaries\n",
      "after Long Document Adaptation. The Good column represents the results of reference summaries that are\n",
      "annotated to be high quality and the Bad column for low-quality data.\n",
      "Conditional on the source text, the entailment task classifies summary sentences as entails,\n",
      "neutral or contradicts. Ideally, a candidate summary should entail or be neutral to the source text,\n",
      "but never contradict the source text. As BERT textual entailment model without long document\n",
      "adaptation have a token limit of 512, this will not be directly applicable for long document datasets\n",
      "with a token length of at least in the thousands. To use this in our experiment, we attempted to adapt\n",
      "Maynez et al. [81]’s textual entailment BERT model by implementing a content selection mechanism\n",
      "to reduce the input size of the source document. The selected subset is constructed using gold\n",
      "label sequences by greedily optimizing the ROUGE score on the ground truth reference summaries,\n",
      "following the algorithm provided by Xiao and Carenini [118]. As we have different variants of the\n",
      "ROUGE score, we experimented with a greedy selection of Rouge-1, Rouge-2 and Rouge-L in the\n",
      "originally ordered sentences and in the randomized ordered sentences. The aim of randomized\n",
      "ordered sentences is to ensure that the salient contents are extracted more uniformly from the\n",
      "source text. To evaluate the discriminative ability of our adapted model, we use the annotated test\n",
      "data in section 3 to evaluate the discriminative ability of our adapted BERT textual entailment\n",
      "model. As the annotated data has 15% of the randomly sampled data to be extremely low quality\n",
      "and 30% to have zero errors in the sentences. A good BERT textual entailment model should then\n",
      "evaluate the 30% high-quality samples as low contradiction and high entailment or neutrality while\n",
      "evaluating the 15% low-quality samples with higher contraction and lower entailment or neutrality.\n",
      "From Table 5, despite trying out various adaptations, our models have almost no discriminative\n",
      "ability and were thus not used in our experimental section in the main article. This is an important\n",
      "limitation of our experiment in section 5 of our main article.\n",
      "10.5\n",
      "Metric-related ACL main conference research papers\n",
      "Table 6 next page details the 17 papers published from 2015 to September 2021 in ACL main\n",
      "conferences, including ACL, NAACL, EACL, EMNLP, CoNLL, and AACL.\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics\n",
      "1:39\n",
      "Paper Title\n",
      "Year\n",
      "I. ACL\n",
      "Studying Summarization Evaluation Metrics in the Appropriate Scoring Range\n",
      "2019\n",
      "Facet-Aware Evaluation for Extractive Summarization\n",
      "2020\n",
      "FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization\n",
      "2020\n",
      "On Faithfulness and Factuality in Abstractive Summarization\n",
      "2020\n",
      "Improving Factual Consistency of Abstractive Summarization via Question Answering\n",
      "2021\n",
      "A Training-free and Reference-free Summarization Evaluation Metric via Centrality-weighted Relevance and Self-referenced Redundancy\n",
      "2021\n",
      "GO FIGURE: A Meta Evaluation of Factuality in Summarization\n",
      "2021\n",
      "Focus Attention: Promoting Faithfulness and Diversity in Summarization\n",
      "2021\n",
      "Evaluating the Efficacy of Summarization Evaluation across Languages\n",
      "2021\n",
      "II. NAACL\n",
      "Question Answering as an Automatic Evaluation Metric for News Article Summarization\n",
      "2019\n",
      "Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics\n",
      "2021\n",
      "III. EMNLP\n",
      "Re-evaluating Automatic Summarization with BLEU and 192 Shades of ROUGE\n",
      "2015\n",
      "Better Summarization Evaluation with Word Embeddings for ROUGE\n",
      "2015\n",
      "Answers Unite! Unsupervised Metrics for Reinforced Summarization Models\n",
      "2019\n",
      "What Have We Achieved on Text Summarization?\n",
      "2020\n",
      "Evaluating the Factual Consistency of Abstractive Text Summarization\n",
      "2020\n",
      "Re-evaluating Evaluation in Text Summarization\n",
      "2020\n",
      "Table 6. List of Metric-related ACL Main Conferences Papers. EACL, CoNLL, and AACL do not have metric-\n",
      "related summarization research papers.\n",
      "ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.\n",
      "\n",
      "Member-only story\n",
      "Summarize Large Documents or\n",
      "Text Using LLMs and LangChain\n",
      "Ranjeet Tiwari | Senior Architect - AI | IITJ · Follow\n",
      "4 min read · Jul 17, 2024\n",
      "100\n",
      "Summarizing long texts can be quite a challenge, but with LangChain and\n",
      "Language Learning Model (LLM), it’s made simple. Imagine you’re reading a\n",
      "lengthy book or a detailed report, and you need to condense it into a short,\n",
      "easy-to-read summary.\n",
      "LangChain(with LLM) provides several strategies to help you do just that.\n",
      "Let’s dive into these strategies using real-world examples to make things\n",
      "clearer.\n",
      "LangChain\n",
      "The “Stuff” Strategy\n",
      "The simplest method is called the “stuff” strategy. If the entire text fits within the\n",
      "LLM’s context window, you can directly input the raw text and get a summary.\n",
      "For example, suppose you have a short article about climate change:\n",
      "Input Text:\n",
      "“Climate change refers to long-term shifts and alterations in temperature\n",
      "and weather patterns, primarily due to human activities like burning fossil\n",
      "fuels, deforestation, and industrial processes. These activities increase levels\n",
      "of greenhouse gases in the atmosphere, leading to global warming and its\n",
      "This member-only story is on us. Upgrade to access all of Medium.\n",
      "Open in app\n",
      "Search\n",
      "Write\n",
      "28/1/25, 1:43 p.m.\n",
      "Summarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium\n",
      "https://medium.com/@AI-Simplified/summarize-large-documents-or-text-using-llms-and-langchain-3116543df4ba\n",
      "1/6\n",
      "\n",
      "associated impacts, such as more frequent extreme weather events, rising\n",
      "sea levels, and changes in wildlife habitats.”\n",
      "Using the “stuff” strategy, you input the entire paragraph, and the LLM\n",
      "provides a summary:\n",
      "Summary received from LLMs of your choice:\n",
      "“Climate change is caused by human activities that increase greenhouse\n",
      "gases, leading to global warming and extreme weather.”\n",
      "The “Map-Reduce” Strategy\n",
      "Often, texts are too long to fit into the context window. In such cases, LangChain’s\n",
      "“map-reduce” strategy is useful. This strategy involves breaking the text into\n",
      "chunks, summarizing each chunk, and then summarizing those summaries.\n",
      "Step-by-Step Example:\n",
      "1. Creating Chunks of text extracted from documents/blogs/news:\n",
      "“Climate change refers to long-term shifts and alterations in temperature\n",
      "and weather patterns, primarily due to human activities like burning\n",
      "fossil fuels, deforestation, and industrial processes.”\n",
      "“These activities increase levels of greenhouse gases in the atmosphere,\n",
      "leading to global warming and its associated impacts, such as more\n",
      "frequent extreme weather events, rising sea levels, and changes in\n",
      "wildlife habitats.”\n",
      "2. Summarize each chunks created based on given length and allowed\n",
      "limits of LLMs system:\n",
      "“Climate change is caused by human activities altering weather patterns.”\n",
      "“Increased greenhouse gases lead to global warming and extreme\n",
      "weather.”\n",
      "3. Combine Summaries provided by LLMs:\n",
      "“Human activities cause climate change by altering weather patterns and\n",
      "increasing greenhouse gases, leading to global warming and extreme\n",
      "weather.”\n",
      "The “Refine” Strategy\n",
      "The “refine” strategy involves starting with an initial summary of the first chunk\n",
      "of text and gradually refining it with subsequent chunks. This approach allows for\n",
      "a more integrated and cohesive summary.\n",
      "Step-by-Step Example:\n",
      "1. Initial Summary:\n",
      "28/1/25, 1:43 p.m.\n",
      "Summarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium\n",
      "https://medium.com/@AI-Simplified/summarize-large-documents-or-text-using-llms-and-langchain-3116543df4ba\n",
      "2/6\n",
      "\n",
      "“Climate change is caused by human activities.”\n",
      "2. Refine with Additional Chunks:\n",
      "After adding the second chunk: “Climate change is caused by human\n",
      "activities that increase greenhouse gases, leading to global warming.”\n",
      "After adding the third chunk: “Climate change, driven by human\n",
      "activities, increases greenhouse gases and leads to global warming and\n",
      "extreme weather.”\n",
      "Summarizing Multiple Documents\n",
      "Suppose you have a set of documents (PDFs, blogs , customer questions, etc.)\n",
      "and you want to summarize the content.\n",
      "LLMs are a great tool for this given their proficiency in understanding and\n",
      "synthesizing text. In the context of retrieval-augmented generation,\n",
      "summarizing text can help distill the information in a large number of\n",
      "retrieved documents to provide context for a LLM.\n",
      "Using LangChain for Multi-Document Summarization\n",
      "LangChain simplifies summarizing content from multiple documents with a\n",
      "few straightforward steps. Here’s a quick guide:\n",
      "1. Set Up Your Environment:\n",
      "Use a Jupyter Notebook for an interactive learning experience.\n",
      "Install LangChain and its dependencies.\n",
      "pip install langchain\n",
      "2. Load Your Documents:\n",
      "Use document loaders, like the WebBaseLoader, to load content from\n",
      "various sources.\n",
      "from langchain_community.document_loaders import WebBaseLoader \n",
      "loader = WebBaseLoader(\"https://example.com/blog-post\") \n",
      "docs = loader.load()\n",
      "3. Choose Your Summarization Strategy:\n",
      "Stuff: Concatenate documents into a single prompt.\n",
      "Map-Reduce: Split documents into batches, summarize those, and then\n",
      "summarize the summaries.\n",
      "28/1/25, 1:43 p.m.\n",
      "Summarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium\n",
      "https://medium.com/@AI-Simplified/summarize-large-documents-or-text-using-llms-and-langchain-3116543df4ba\n",
      "3/6\n",
      "\n",
      "Refine: Update a rolling summary by iterating over the documents in\n",
      "sequence.\n",
      "from langchain.chains.summarize import load_summarize_chain\n",
      "from langchain_openai import ChatOpenAI\n",
      "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-1106\")\n",
      "# Using Stuff\n",
      "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
      "result = chain.invoke(docs)\n",
      "print(result[\"output_text\"])\n",
      "# Using Map-Reduce\n",
      "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
      "result = chain.invoke(docs)\n",
      "print(result[\"output_text\"])\n",
      "# Using Refine\n",
      "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
      "result = chain.invoke(docs)\n",
      "print(result[\"output_text\"])\n",
      "To summarize\n",
      "LangChain offers versatile strategies for summarizing text using LLMs,\n",
      "making it easier to handle texts of any length. Whether you use the “stuff,”\n",
      "“map-reduce,” or “refine” strategy depends on the text’s length and\n",
      "complexity. By breaking down the text and refining summaries, you can\n",
      "achieve clear and concise summaries suitable for any purpose.\n",
      "Written by Ranjeet Tiwari | Senior Architect - AI | IITJ\n",
      "71 Followers · 8 Following\n",
      "M.Tech from IIT Jodhpur, Senior Architect - AI in a IT Company, Follow me on\n",
      "medium and LinkedIn at https://www.linkedin.com/in/ranjeet-tiwari-9606a946/\n",
      "Follow\n",
      "No responses yet\n",
      "Generative Ai Solution\n",
      "Langchain\n",
      "Large Language Models\n",
      "Deep Learning\n",
      "Artificial Intelligence\n",
      "What are your thoughts?\n",
      "Respond\n",
      "28/1/25, 1:43 p.m.\n",
      "Summarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium\n",
      "https://medium.com/@AI-Simplified/summarize-large-documents-or-text-using-llms-and-langchain-3116543df4ba\n",
      "4/6\n",
      "\n",
      "More from Ranjeet Tiwari | Senior Architect - AI | IITJ\n",
      "See all from Ranjeet Tiwari | Senior Architect - AI | IITJ\n",
      "Recommended from Medium\n",
      "Building Large Language Models\n",
      "from Scratch: Initial Guide\n",
      "Oct 1, 2024\n",
      "Prepare Instruction Dataset to\n",
      "Fine-Tune Large Language Model…\n",
      "Fine-tuning a Large Language Model (LLM)\n",
      "can seem daunting, but with a clear…\n",
      "Jul 15, 2024\n",
      "Small LLMs: Building a Multi-\n",
      "Agentic RAG System\n",
      "Ever wondered how powerful a “small”\n",
      "language model can be? 🤔 Imagine buildin…\n",
      "Jan 5\n",
      "Leveraging Celery and Kafka for\n",
      "Efficient Distributed Processing i…\n",
      "In the realm of distributed processing and\n",
      "task management, the combination of Celer…\n",
      "Jul 9, 2023\n",
      "AI’nt That Easy #20: Evaluating\n",
      "Text Summarization with LLM…\n",
      "Use HuggingFace\n",
      "apply_chat_template when…\n",
      "Ranjeet Tiwari | Senior Architect - AI | IITJ\n",
      "5\n",
      "Ranjeet Tiwari | Senior Architect - AI | IITJ\n",
      "39\n",
      "Ranjeet Tiwari | Senior Architect - AI | IITJ\n",
      "13\n",
      "Ranjeet Tiwari | Senior Architect - AI | IITJ\n",
      "72\n",
      "4\n",
      "Aakriti Aggarwal\n",
      "Manyi\n",
      "28/1/25, 1:43 p.m.\n",
      "Summarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium\n",
      "https://medium.com/@AI-Simplified/summarize-large-documents-or-text-using-llms-and-langchain-3116543df4ba\n",
      "5/6\n",
      "\n",
      "Lists\n",
      "AI Regulation\n",
      "6 stories · 678 saves\n",
      "Natural Language Processing\n",
      "1903 stories · 1556 saves\n",
      "ChatGPT\n",
      "21 stories · 954 saves\n",
      "Generative AI Recommended\n",
      "Reading\n",
      "52 stories · 1620 saves\n",
      "See more recommendations\n",
      "Large Language Models (LLMs) have\n",
      "revolutionized natural language processing,…\n",
      "Oct 20, 2024\n",
      "To build a chatbot, it is common practice to\n",
      "use an instruction-tuned model. An…\n",
      "Nov 27, 2024\n",
      "Introduction to Building\n",
      "Applications with the DeepSeek…\n",
      "In today’s rapidly evolving world of artificial\n",
      "intelligence (AI), large language models…\n",
      "1d ago\n",
      "Part 2A: Implementing a Graph\n",
      "Builder to Extract a Basic…\n",
      "In this post, I’ll show you how to construct a\n",
      "basic document-centric knowledge graph…\n",
      "6d ago\n",
      "6 AI Agents That Are So Good,\n",
      "They Feel Illegal\n",
      "AI agents are the future because they can\n",
      "replace all the manual work with automation…\n",
      "Jan 11\n",
      "In\n",
      "by\n",
      "I am among the first people to gain\n",
      "access to OpenAI’s “Operator”…\n",
      "“An AI completing multiple tasks at the same\n",
      "time” — DALL-E\n",
      "4d ago\n",
      "5\n",
      "1\n",
      "Kamal Dhungana\n",
      "55\n",
      "Ngoc\n",
      "27\n",
      "Mohit Vaswani\n",
      "1.3K\n",
      "40\n",
      "Artificial Intelligence in Plain En…\n",
      "Austin Sta…\n",
      "860\n",
      "37\n",
      "28/1/25, 1:43 p.m.\n",
      "Summarize Large Documents or Text Using LLMs and LangChain | by Ranjeet Tiwari | Senior Architect - AI | IITJ | Medium\n",
      "https://medium.com/@AI-Simplified/summarize-large-documents-or-text-using-llms-and-langchain-3116543df4ba\n",
      "6/6\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([542, 111291, 14571], [81, 31847, 80, 5276])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.encode(\"congratulations\"), encoding.encode(\"rqsqeft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "813"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoding.encode(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39876"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoding.encode(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62016"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "969*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question Answering using LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from langchain_core.prompts import (SystemMessagePromptTemplate, HumanMessagePromptTemplate,\n",
    "                                    ChatPromptTemplate)\n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:3b'\n",
    "model = 'llama3.1:8b'\n",
    "model = 'phi4:latest'\n",
    "#model = 'deepseek-r1:8b'\n",
    "#model = 'deepseek-r1:14b'\n",
    "\n",
    "llm = ChatOllama(base_url=base_url, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = SystemMessagePromptTemplate.from_template(\"\"\"You are helpful AI assistant who answer user question based on the provided context. \n",
    "                                                    Do not answer in more than {words} words\"\"\")\n",
    "\n",
    "prompt = \"\"\"Answer user question based on the provided context ONLY! If you do not know the answer, just say \"I don't know\".\n",
    "            ### Context:\n",
    "            {context}\n",
    "\n",
    "            ### Question:\n",
    "            {question}\n",
    "\n",
    "            ### Answer:\"\"\"\n",
    "\n",
    "prompt = HumanMessagePromptTemplate.from_template(prompt)\n",
    "\n",
    "messages = [system, prompt]\n",
    "template = ChatPromptTemplate(messages)\n",
    "\n",
    "# template\n",
    "# template.invoke({'context': context, 'question': \"How to gain muscle mass?\", 'words': 50})\n",
    "\n",
    "qna_chain = template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question', 'words'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['words'], input_types={}, partial_variables={}, template='You are helpful AI assistant who answer user question based on the provided context. \\n                                                    Do not answer in more than {words} words'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer user question based on the provided context ONLY! If you do not know the answer, just say \"I don\\'t know\".\\n            ### Context:\\n            {context}\\n\\n            ### Question:\\n            {question}\\n\\n            ### Answer:'), additional_kwargs={})])\n",
       "| ChatOllama(model='phi4:latest', base_url='http://localhost:11434')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para resumir documentos largos, puedes utilizar técnicas de procesamiento de lenguaje natural (NLP) con el apoyo de modelos de lenguaje grandes (LLMs). A continuación se presentan algunas estrategias y herramientas para lograrlo:\n",
      "\n",
      "1. **Estrategia \"Stuff\"**: \n",
      "   - Combina todos los fragmentos del documento en un solo texto largo.\n",
      "   - Aplica una instrucción al modelo indicando que resuma el contenido, ideal para documentos de longitud media.\n",
      "\n",
      "2. **Estrategia \"Map-Reduce\"**:\n",
      "   - Divide el documento grande en partes más pequeñas (chunks).\n",
      "   - Resumir cada parte por separado utilizando un modelo LLM.\n",
      "   - Combinar los resultados parciales en un resumen final.\n",
      "   - Esta técnica es útil para documentos muy extensos, ya que divide la carga y mejora el manejo del contexto.\n",
      "\n",
      "3. **Estrategia \"Refine\"**:\n",
      "   - Resumir primero el documento completo con una instrucción de resumen general.\n",
      "   - Luego, refinar cada sección del resumen agregando más detalle o aclaraciones según sea necesario.\n",
      "   - Útil para mejorar la precisión y claridad del resumen final.\n",
      "\n",
      "### Herramientas y Técnicas\n",
      "\n",
      "- **LangChain**: Una biblioteca que facilita el uso de modelos LLM en Python, proporcionando herramientas como:\n",
      "  - `map_reduce` para aplicar funciones (como resumir) a cada parte de un texto.\n",
      "  - `StuffDocumentsTemplate` para combinar documentos antes del procesamiento.\n",
      "\n",
      "- **Prompt Engineering**: Diseñar instrucciones claras para el modelo sobre cómo debe realizar la tarea. Esto es crucial para obtener resultados precisos y relevantes.\n",
      "\n",
      "### Ejemplo de Implementación en Python\n",
      "\n",
      "```python\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
      "from langchain.document_loaders import PyPDFLoader\n",
      "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
      "from langchain.chains.map_reduce import create_map_reduce_chain\n",
      "\n",
      "# Cargar y dividir documento\n",
      "loader = PyPDFLoader(\"example.pdf\")\n",
      "docs = loader.load_and_split()\n",
      "splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
      "chunks = splitter.split_documents(docs)\n",
      "\n",
      "# Configurar el prompt para resumir\n",
      "system_message_prompt = SystemMessagePromptTemplate.from_template(\"Resumen del siguiente texto:\")\n",
      "human_message_prompt = HumanMessagePromptTemplate.from_template(\"{text}\")\n",
      "prompt = ChatPromptTemplate(\n",
      "    messages=[\n",
      "        system_message_prompt,\n",
      "        human_message_prompt,\n",
      "    ]\n",
      ")\n",
      "\n",
      "# Crear y ejecutar la cadena map-reduce\n",
      "chain = create_map_reduce_chain(llm, prompt=prompt)\n",
      "summary = chain.run(chunks)\n",
      "print(summary)\n",
      "```\n",
      "\n",
      "Esta implementación utiliza `langchain` para dividir un documento PDF en partes manejables y resumirlas usando técnicas de procesamiento basadas en modelos LLM.\n"
     ]
    }
   ],
   "source": [
    "response = qna_chain.invoke({'context': context, 'question': \"¿como resumir documentos largos?\", 'words': 50})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las estrategias para resumir documentos mencionadas en el texto incluyen:\n",
      "\n",
      "1. **Estrategia \"Stuff\"**: Esta técnica implica combinar todo el contenido de un documento o múltiples documentos en una sola solicitud a un modelo de lenguaje grande (LLM). Es adecuada para textos cortos donde la longitud no supere los límites del modelo.\n",
      "\n",
      "2. **Estrategia \"Map-Reduce\"**: Consiste en dividir el texto original en partes más pequeñas, resumir cada parte por separado y luego integrar estos resúmenes parciales en un único documento completo. Esta estrategia es útil para manejar documentos largos que exceden los límites de longitud del modelo.\n",
      "\n",
      "3. **Estrategia \"Refine\"**: Después de obtener un primer borrador del resumen, esta técnica aplica correcciones o mejoras iterativas al texto inicial para mejorar la calidad y precisión del resumen final.\n",
      "\n",
      "Estas estrategias se aplican dependiendo de la longitud y complejidad del texto a resumir.\n"
     ]
    }
   ],
   "source": [
    "response = qna_chain.invoke({'context': context, 'question': \"¿cuales son las estrategias de resumir documentos?\", 'words': 50})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, RL does not require labeled input/output pairs and differs from unsupervised learning because it focuses on finding a balance between exploration (of uncharted territory) and exploitation (refining known strategies).\n",
      "\n",
      "Here's how reinforcement learning typically works:\n",
      "\n",
      "1. **Agent**: The learner or decision-maker.\n",
      "2. **Environment**: Everything the agent interacts with.\n",
      "3. **Action**: A set of choices that an agent can make.\n",
      "4. **State**: The current situation returned by the environment.\n",
      "5. **Reward**: Feedback from the environment based on the action taken.\n",
      "\n",
      "The process involves:\n",
      "\n",
      "- The agent observes the state of the environment.\n",
      "- It selects and performs an action.\n",
      "- The environment responds with a new state and provides a reward (or penalty).\n",
      "- The goal is to learn a policy that maps states to actions in a way that maximizes cumulative rewards over time.\n",
      "\n",
      "Reinforcement learning has been applied successfully in various domains, including robotics, game playing (e.g., AlphaGo), autonomous vehicles, and recommendation systems. Key concepts within RL include exploration vs. exploitation, Markov Decision Processes (MDPs), value functions, policy gradients, Q-learning, and more advanced methods like deep reinforcement learning that integrate neural networks to handle complex tasks.\n",
      "\n",
      "Overall, reinforcement learning is powerful for situations where an agent must learn from interacting with a dynamic environment and optimize its actions based on feedback over time.\n"
     ]
    }
   ],
   "source": [
    "response = qna_chain.invoke({'context': context, 'question': \"Que es reinforcement learning?\", 'words': 50})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 2: PDF Document Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = SystemMessagePromptTemplate.from_template(\"\"\"You are helpful AI assistant who works as document summarizer. \n",
    "                                                   You must not hallucinate or provide any false information.\"\"\")\n",
    "\n",
    "prompt = \"\"\"Summarize the given context in {words}.\n",
    "            ### Context:\n",
    "            {context}\n",
    "\n",
    "            ### Summary:\"\"\"\n",
    "\n",
    "prompt = HumanMessagePromptTemplate.from_template(prompt)\n",
    "\n",
    "messages = [system, prompt]\n",
    "template = ChatPromptTemplate(messages)\n",
    "\n",
    "summary_chain = template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'words'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are helpful AI assistant who works as document summarizer. \\n                                                   You must not hallucinate or provide any false information.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'words'], input_types={}, partial_variables={}, template='Summarize the given context in {words}.\\n            ### Context:\\n            {context}\\n\\n            ### Summary:'), additional_kwargs={})])\n",
       "| ChatOllama(model='phi4:latest', base_url='http://localhost:11434')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article, written by Ranjeet Tiwari, discusses how LangChain can be used to summarize large documents with Large Language Models (LLMs), highlighting three strategies: \"stuff,\" \"map-reduce,\" and \"refine.\"\n",
      "\n",
      "1. **\"Stuff\" Strategy**: This approach involves inputting the entire text into a language model in one go. It's effective for shorter texts but may not work well for longer documents due to token limits.\n",
      "\n",
      "2. **\"Map-Reduce\" Strategy**: Longer documents are divided into smaller parts, each of which is processed individually by a language model (the map phase). The resulting summaries from these parts are then combined to form a comprehensive summary (the reduce phase).\n",
      "\n",
      "3. **\"Refine\" Strategy**: This method involves iteratively refining the text or its summary through multiple passes with the language model until a desired outcome is achieved.\n",
      "\n",
      "Tiwari emphasizes that choosing between these strategies depends on the document's length and complexity, allowing for clear and concise summaries suited to various purposes. The article also includes instructions for setting up LangChain, installing necessary packages via pip, and executing code snippets in Python.\n"
     ]
    }
   ],
   "source": [
    "response = summary_chain.invoke({'context': context, 'words': 50})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article by Ranjeet Tiwari discusses techniques for summarizing large documents using Large Language Models (LLMs) and LangChain. It outlines three main strategies: \"stuff,\" \"map-reduce,\" and \"refine.\"\n",
      "\n",
      "1. **Stuff Strategy**: This method involves feeding the entire text into an LLM, which is suitable for smaller texts that can be processed in one go due to token limitations.\n",
      "\n",
      "2. **Map-Reduce Strategy**: For longer texts, this approach splits the document into manageable chunks using a tool like LangChain's \"DocumentChunker.\" Each chunk is summarized individually with an LLM, and these summaries are then combined to form a comprehensive summary of the entire text.\n",
      "\n",
      "3. **Refine Strategy**: This method involves iteratively summarizing a large text by first breaking it down into smaller parts. Each part is summarized and the summaries themselves are further condensed until a concise final summary is achieved.\n",
      "\n",
      "The article emphasizes choosing an appropriate strategy based on the length and complexity of the document to achieve clear and effective summaries using LLMs.\n"
     ]
    }
   ],
   "source": [
    "response = summary_chain.invoke({'context': context, 'words': 500})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 3: Report Generation from PDF Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streamlit Tutorial: https://www.youtube.com/watch?v=hff2tHUzxJM&list=PLc2rvfiptPSSpZ99EnJbH5LjTJ_nOoSWW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Detailed Report on Summarizing Large Documents Using LLMs and LangChain\n",
      "\n",
      "## Overview\n",
      "\n",
      "This report provides an overview of techniques for summarizing large documents using Large Language Models (LLMs) and LangChain, as detailed by Ranjeet Tiwari. The focus is on three primary strategies: \"stuff,\" \"map-reduce,\" and \"refine.\" Each strategy caters to different needs based on document length and complexity.\n",
      "\n",
      "## Strategies for Summarization\n",
      "\n",
      "### 1. Stuff Strategy\n",
      "- **Description**: This approach involves feeding the entire text into an LLM without segmenting it.\n",
      "- **Use Case**: Best suited for short documents where context loss is minimal.\n",
      "\n",
      "### 2. Map-Reduce Strategy\n",
      "- **Process**:\n",
      "  - **Mapping Phase**: The document is divided into smaller chunks that fit within the model's token limit (e.g., ~300 tokens).\n",
      "  - **Reduction Phase**: Each chunk is summarized individually, and these summaries are then combined to form a comprehensive summary.\n",
      "- **Application**: Ideal for medium-length documents where context needs careful preservation across segments.\n",
      "\n",
      "### 3. Refine Strategy\n",
      "- **Process**:\n",
      "  - The document is divided into parts that align with the model's token limitations.\n",
      "  - Each part is summarized separately.\n",
      "  - A final refinement step combines these summaries to ensure cohesiveness and completeness.\n",
      "- **Application**: Suitable for long documents requiring detailed attention across multiple sections.\n",
      "\n",
      "## Practical Implementation\n",
      "\n",
      "### Tools and Technologies\n",
      "- **LangChain**: Used for breaking down large texts into manageable chunks.\n",
      "- **LLMs (e.g., ChatGPT)**: Employed for generating summaries of each chunk or part.\n",
      "\n",
      "### Example Implementation\n",
      "1. **Chunking with LangChain**:\n",
      "   - Divide the document based on token limits.\n",
      "2. **Summarization with LLMs**:\n",
      "   - Use models like ChatGPT to summarize each chunk.\n",
      "3. **Combining Summaries**:\n",
      "   - Merge individual summaries into a final, cohesive summary.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The choice between \"stuff,\" \"map-reduce,\" and \"refine\" strategies depends on the document's length and complexity. By leveraging LangChain for chunking and LLMs for summarization, users can achieve clear and concise summaries tailored to their specific needs. This approach ensures that even lengthy documents are summarized effectively without losing critical context.\n",
      "\n",
      "---\n",
      "\n",
      "This report encapsulates the methodologies discussed by Ranjeet Tiwari for efficiently summarizing large texts using advanced AI tools, providing a practical guide for those looking to implement these strategies in various contexts.\n"
     ]
    }
   ],
   "source": [
    "response = qna_chain.invoke({'context': context, \n",
    "                             'question': \"Provide a detailed report from the provided context. Write answer in Markdown.\", \n",
    "                             'words': 2000})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
