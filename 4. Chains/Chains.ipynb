{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Expression Language Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  LangChain Expression Language is that any two runnables can be \"chained\" together into sequences. \n",
    "- The output of the previous runnable's .invoke() call is passed as input to the next runnable.\n",
    "- This can be done using the pipe operator (|), or the more explicit .pipe() method, which does the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Type of LCEL Chains\n",
    "    - SequentialChain\n",
    "    - Parallel Chain\n",
    "    - Router Chain\n",
    "    - Chain Runnables\n",
    "    - Custom Chain (Runnable Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('./../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "                                        SystemMessagePromptTemplate,\n",
    "                                        HumanMessagePromptTemplate,\n",
    "                                        ChatPromptTemplate\n",
    "                                        )\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "\n",
    "llm = ChatOllama(base_url=base_url, model=model)\n",
    "\n",
    "system = SystemMessagePromptTemplate.from_template('You are {school} teacher. You answer in short sentences.')\n",
    "\n",
    "question = HumanMessagePromptTemplate.from_template('tell me about the {topics} in {points} points')\n",
    "\n",
    "messages = [system, question]\n",
    "\n",
    "template = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chain\n",
    "# input | model | output (str output)\n",
    "chain = template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's what I know about the sun:\n",
      "\n",
      "• The sun is a huge ball of hot, glowing gas.\n",
      "• It's about 93 million miles away from Earth.\n",
      "• The sun makes all the light and heat that we need for life on our planet.\n",
      "• Without the sun, we would be frozen in place, unable to grow or move around.\n",
      "• We can see only one side of the sun because it reflects sunlight back towards us, making it look bright and round.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'school': 'elementary', 'topics': 'sun', 'points': 5})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Runnables (Chain Multiple Runnables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can even combine this chain with more runnables to create another chain.\n",
    "- Let's see how easy our generated output is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fact is relatively simple and easy to comprehend with basic knowledge.\n"
     ]
    }
   ],
   "source": [
    "analysis_prompt = ChatPromptTemplate.from_template('analyze the following fact: {fact}\\n\\nHow easy is this fact to understand? Tell me in 10 words.')\n",
    "analysis_prompt\n",
    "\n",
    "fact_check_chain = analysis_prompt | llm | StrOutputParser()\n",
    "output = fact_check_chain.invoke({'fact': response})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The concept of a massive star's immense size and nuclear reaction complexity is hard to grasp.\n"
     ]
    }
   ],
   "source": [
    "composed_chain = {\"fact\": chain} | analysis_prompt | llm | StrOutputParser()\n",
    "\n",
    "output = composed_chain.invoke({'school': 'ph.d', 'topics': 'sun', 'points': 5})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel LCEL Chain\n",
    "- Parallel chains are used to run multiple runnables in parallel.\n",
    "- The final return value is a dict with the results of each value under its appropriate key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(base_url=base_url, model=model)\n",
    "\n",
    "system = SystemMessagePromptTemplate.from_template('You are {school} teacher. You answer in short sentences.')\n",
    "\n",
    "question = HumanMessagePromptTemplate.from_template('tell me about the {topics} in {points} points')\n",
    "\n",
    "messages = [system, question]\n",
    "\n",
    "template = ChatPromptTemplate(messages)\n",
    "\n",
    "fact_chain = template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = HumanMessagePromptTemplate.from_template('write a poem about the {topics} in {sentences} sentences')\n",
    "\n",
    "messages = [system, question]\n",
    "\n",
    "template = ChatPromptTemplate(messages)\n",
    "\n",
    "poem_chain = template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fact': \"Here's what I know about the sun:\\n\\n1. **The Sun is a Star**: It's a massive ball of hot, glowing gas that lives at the center of our solar system.\\n\\n2. **It's Really Hot**: The surface temperature of the sun is about 5,500 degrees Celsius. That's really, really hot!\\n\\n3. **The Sun is Huge**: Our sun is about 109 times bigger than Earth and has a diameter of about 1.4 million kilometers.\\n\\n4. **It Makes Us Warm**: The sun's rays help give us warmth on Earth by heating up the planet.\\n\\n5. **We're Not Alone**: There are billions of other stars in our galaxy, and some of them might be similar to the sun in size and temperature.\", 'poem': 'The sun rises high in the sky,\\nBringing light to the world outside.\\nIts warm rays shine down on our face,\\nWarming us up with a gentle pace,\\nA beautiful sight, a lovely hue.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = RunnableParallel(fact = fact_chain, poem=poem_chain)\n",
    "\n",
    "response = chain.invoke({'school': 'elementary', 'topics': 'sun', 'points': 5, 'sentences': 5})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's what I know about the sun:\n",
      "\n",
      "1. **The Sun is a Star**: It's a massive ball of hot, glowing gas that lives at the center of our solar system.\n",
      "\n",
      "2. **It's Really Hot**: The surface temperature of the sun is about 5,500 degrees Celsius. That's really, really hot!\n",
      "\n",
      "3. **The Sun is Huge**: Our sun is about 109 times bigger than Earth and has a diameter of about 1.4 million kilometers.\n",
      "\n",
      "4. **It Makes Us Warm**: The sun's rays help give us warmth on Earth by heating up the planet.\n",
      "\n",
      "5. **We're Not Alone**: There are billions of other stars in our galaxy, and some of them might be similar to the sun in size and temperature.\n",
      "\n",
      "\n",
      "\n",
      "The sun rises high in the sky,\n",
      "Bringing light to the world outside.\n",
      "Its warm rays shine down on our face,\n",
      "Warming us up with a gentle pace,\n",
      "A beautiful sight, a lovely hue.\n"
     ]
    }
   ],
   "source": [
    "response.keys()\n",
    "\n",
    "print(response['fact'])\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(response['poem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Router\n",
    "- The router chain is used to route the output of a previous runnable to the next runnable based on the output of the previous runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Given the user review below, classify it as either being about `Positive` or `Negative`.\n",
    "            Do not respond with more than one word.\n",
    "\n",
    "            Question: {question}\n",
    "            Classification:\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "chain = template | llm | StrOutputParser()\n",
    "\n",
    "question = \"Thank you so much for providing such a great platform for learning.\"\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_prompt = \"\"\"\n",
    "                You are expert in writing reply for positive reviews.\n",
    "                You need to encourage the user to share their experience on social media.\n",
    "                Question: {question}\n",
    "                Answer:\"\"\"\n",
    "positive_template = ChatPromptTemplate.from_template(positive_prompt)\n",
    "positive_chain = positive_template | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "negative_prompt = \"\"\"\n",
    "                You are expert in writing reply for negative reviews.\n",
    "                You need first to apologize for the inconvenience caused to the user.\n",
    "                You need to encourage the user to share their concern on following Email:'udemy@kgptalkie.com'.\n",
    "                Question: {question}\n",
    "                Answer:\"\"\"\n",
    "negative_template = ChatPromptTemplate.from_template(negative_prompt)\n",
    "negative_chain = negative_template | llm | StrOutputParser()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    if \"positive\" in info[\"topic\"].lower():\n",
    "        return positive_chain\n",
    "    else:\n",
    "        return negative_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am not happy with the course content. I want my refund.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a sample reply:\n",
      "\n",
      "Dear valued customer,\n",
      "\n",
      "We apologize for any inconvenience or frustration caused by the course content that didn't meet your expectations. We understand that our courses are designed to provide valuable learning experiences, and it's disappointing when this doesn't align with your goals.\n",
      "\n",
      "We want to assure you that we take all concerns seriously and would like to help resolve this matter. Unfortunately, as a gesture of goodwill, we cannot offer a refund at this time. However, we value your feedback and would appreciate any additional information that might help us improve our course content in the future.\n",
      "\n",
      "If you're willing, please could you contact us at udemy@kgptalkie.com so we can discuss further how we can better address your concerns? Your input will play a crucial role in shaping the next version of our courses. We're committed to providing high-quality educational resources and appreciate your patience and understanding during this time.\n",
      "\n",
      "Thank you for choosing Udemy, and we look forward to serving you better in the future.\n",
      "\n",
      "Best regards,\n",
      "The Udemy Team\n"
     ]
    }
   ],
   "source": [
    "response = full_chain.invoke({\"question\": question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a potential response:\n",
      "\n",
      "\"Dear [User's Name],\n",
      "\n",
      "I'm so sorry to hear that you're not satisfied with the course content and are looking for a refund. I understand how frustrating it can be when we don't meet our expectations.\n",
      "\n",
      "As per our policies, refunds will be processed within 7-10 business days after receiving your request. However, please note that there might be some delay in processing due to various reasons such as administrative tasks or external factors.\n",
      "\n",
      "I'd like to suggest an alternative solution. We're committed to providing you with a high-quality learning experience. I'd love to discuss this further with you and see if there's anything we can do to rectify the situation. Please feel free to share your concerns and suggestions via our email address: udyetalkie@kgptalkie.com.\n",
      "\n",
      "Your feedback is invaluable in helping us improve, and I'm confident that together, we can find a resolution that meets your expectations.\n",
      "\n",
      "Thank you for choosing Udemy and talkie, and I look forward to hearing from you soon.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\"\n"
     ]
    }
   ],
   "source": [
    "question = \"I am not happy with the course content. I want my refund.\"\n",
    "response = full_chain.invoke({\"question\": question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Custom Chain Runnables with RunnablePassthrough and RunnableLambda\n",
    "- This is useful for formatting or when you need functionality not provided by other LangChain components, and custom functions used as Runnables are called RunnableLambdas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char_counts': 899,\n",
       " 'word_counts': 137,\n",
       " 'output': 'Bar + Gaze are a New York-based indie folk duo consisting of Alex Hall and Gabe Nichols. They create music that blends elements of folk, rock, and electronic music with introspective and emotionally charged lyrics.\\n\\nTheir sound is characterized by Hall\\'s soaring vocals, Nichols\\' atmospheric instrumentation, and the way they weave together storytelling and poetic imagery to convey complex emotions and ideas. The duo\\'s music often explores themes of love, loss, identity, and social commentary, all set to a backdrop of haunting melodies and textures.\\n\\nBar + Gaze have released several EPs and albums throughout their career, including \"Bar\" (2014) and \"Gaze\" (2015). Their music has been praised for its originality, sensitivity, and emotional depth. If you\\'re looking for artists who blend folk and electronic elements with introspective songwriting, Bar + Gaze is definitely worth checking out.'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "\n",
    "def char_counts(text):\n",
    "    return len(text)\n",
    "\n",
    "def word_counts(text):\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {in1} + {in2}\")\n",
    "\n",
    "chain1 = prompt | llm\n",
    "\n",
    "chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    |{\n",
    "        \"char_counts\": RunnableLambda(char_counts),\n",
    "        \"word_counts\": RunnableLambda(word_counts),\n",
    "        \"output\": RunnablePassthrough()\n",
    "    }\n",
    ")\n",
    "\n",
    "chain.invoke({\"in1\": \"bar\", \"in2\": \"gah\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Chain using `@chain` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'facts': \"Here's what you need to know about the sun:\\n\\n1. The sun is a star that gives us light and warmth.\\n2. It's about 93 million miles away from Earth.\\n3. The sun makes up 99% of our solar system's mass.\\n4. It takes about 8 minutes and 20 seconds for sunlight to reach Earth.\\n5. The surface temperature of the sun is about 5,500 degrees Celsius.\",\n",
       " 'poem': 'The sun shines bright in the morning sky,\\nBringing light to the world, as it passes by.\\nIt rises high and sets low with a grin,\\nWarming our skin and making everything win.\\nA beautiful sight that we all can see.'}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "@chain\n",
    "def custom_chain(params):\n",
    "    return {\n",
    "        \"facts\": fact_chain.invoke(params),\n",
    "        \"poem\": poem_chain.invoke(params)\n",
    "    }\n",
    "\n",
    "\n",
    "params = {'school': 'elementary', 'topics': 'sun', 'points': 5, 'sentences': 5}\n",
    "custom_chain.invoke(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['points', 'school', 'topics'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['school'], input_types={}, partial_variables={}, template='You are {school} teacher. You answer in short sentences.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['points', 'topics'], input_types={}, partial_variables={}, template='tell me about the {topics} in {points} points'), additional_kwargs={})])\n",
       "| ChatOllama(model='llama3.2:1b', base_url='http://localhost:11434')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['school', 'sentences', 'topics'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['school'], input_types={}, partial_variables={}, template='You are {school} teacher. You answer in short sentences.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['sentences', 'topics'], input_types={}, partial_variables={}, template='write a poem about the {topics} in {sentences} sentences'), additional_kwargs={})])\n",
       "| ChatOllama(model='llama3.2:1b', base_url='http://localhost:11434')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
